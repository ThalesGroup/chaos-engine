{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Chaos Engine is an application for creating random Chaos Events in cloud applications to test resiliency. It follows the Principles of Chaos to create random faults ( experiments ) that could reasonably occur in a real application deployment. Chaos Engine makes intelligent decisions in how and when to create experiments. When properly configured, experiments can be restricted to occur only during normal business hours (i.e., no pager alerts). Chaos Engine currently supports Amazon Web Services, Pivotal Cloud Foundry, and Kubernetes. We have future plans to add support for Google Cloud Platform. Warning Running chaos experiments in a non-resilient system can result in significant faults. We highly recommend you use a graduated approach to chaos implementation, and build confidence in your development and staging environments before attempting the same in your production environment. Supported Experiments Amazon Web Services EC2 Instances Experiment Target Description Stop Instance All EC2 Instances Stops selected instance Restart Instance All EC2 Instances Restarts selected instance Remove Security Groups All EC2 Instances Removes all assigned security groups Instance Termination EC2 Instance in ASG only Terminates an instance when it is running in ASG Deploy Shell Experiment EC2 Instance in ASG only Deploys an experiment from shell experiment suite described below RDS Instances Experiment Target Description Restart Instance All RDS Instances Restarts selected instance Remove Security Groups All RDS Instances Removes all assigned security groups Take Snapshot All RDS Instances Takes a snapshot of the DB Restart Subset of Nodes RDS Cluster only Randomly selects set of nodes and restarts them Initiate failover RDS Cluster only Initialize failover between nodes Kubernetes Experiment Target Description Delete POD POD Deletes randomly selected pod Deploy Shell Experiment Container Deploys an experiment from shell experiment suite described below Pivotal Cloud Foundry Experiment Target Description Rescale Application Application Rescales an application to random number of instances Restage Application Application Redeploys an application Restart Application Application Restarts all application containers Restart Instance Container Restarts selected container Deploy Shell Experiment Container Deploys an experiment from shell experiment suite described below Shell Experiments Suite Experiment Target Description BurnIO EC2, PCF or Kubernetes resource supporting remote command execution Utilize system disk to maximum CPU Burn EC2, PCF or Kubernetes resource supporting RCE Simulates high CPU usage on all available processing units DNS Block EC2, PCF or Kubernetes resource supporting RCE Removes all DNS servers from system configuration Fill Disk EC2, PCF or Kubernetes resource supporting RCE Creates large file on the system root partition Fork Bomb EC2, PCF or Kubernetes resource supporting RCE Runs endless recursion that corrupts system memory Memory Consumer EC2, PCF or Kubernetes resource supporting RCE Consumes all free memory Null Route EC2, PCF or Kubernetes resource supporting RCE Adds an IP table rule that will forward traffic from specific subnet to black hole Random Generator Starvation EC2, PCF or Kubernetes resource supporting RCE Simulates entropy starvation Process Termination EC2, PCF or Kubernetes resource supporting RCE Terminates random process","title":"Home"},{"location":"#supported-experiments","text":"","title":"Supported Experiments"},{"location":"#amazon-web-services","text":"","title":"Amazon Web Services"},{"location":"#ec2-instances","text":"Experiment Target Description Stop Instance All EC2 Instances Stops selected instance Restart Instance All EC2 Instances Restarts selected instance Remove Security Groups All EC2 Instances Removes all assigned security groups Instance Termination EC2 Instance in ASG only Terminates an instance when it is running in ASG Deploy Shell Experiment EC2 Instance in ASG only Deploys an experiment from shell experiment suite described below","title":"EC2 Instances"},{"location":"#rds-instances","text":"Experiment Target Description Restart Instance All RDS Instances Restarts selected instance Remove Security Groups All RDS Instances Removes all assigned security groups Take Snapshot All RDS Instances Takes a snapshot of the DB Restart Subset of Nodes RDS Cluster only Randomly selects set of nodes and restarts them Initiate failover RDS Cluster only Initialize failover between nodes","title":"RDS Instances"},{"location":"#kubernetes","text":"Experiment Target Description Delete POD POD Deletes randomly selected pod Deploy Shell Experiment Container Deploys an experiment from shell experiment suite described below","title":"Kubernetes"},{"location":"#pivotal-cloud-foundry","text":"Experiment Target Description Rescale Application Application Rescales an application to random number of instances Restage Application Application Redeploys an application Restart Application Application Restarts all application containers Restart Instance Container Restarts selected container Deploy Shell Experiment Container Deploys an experiment from shell experiment suite described below","title":"Pivotal Cloud Foundry"},{"location":"#shell-experiments-suite","text":"Experiment Target Description BurnIO EC2, PCF or Kubernetes resource supporting remote command execution Utilize system disk to maximum CPU Burn EC2, PCF or Kubernetes resource supporting RCE Simulates high CPU usage on all available processing units DNS Block EC2, PCF or Kubernetes resource supporting RCE Removes all DNS servers from system configuration Fill Disk EC2, PCF or Kubernetes resource supporting RCE Creates large file on the system root partition Fork Bomb EC2, PCF or Kubernetes resource supporting RCE Runs endless recursion that corrupts system memory Memory Consumer EC2, PCF or Kubernetes resource supporting RCE Consumes all free memory Null Route EC2, PCF or Kubernetes resource supporting RCE Adds an IP table rule that will forward traffic from specific subnet to black hole Random Generator Starvation EC2, PCF or Kubernetes resource supporting RCE Simulates entropy starvation Process Termination EC2, PCF or Kubernetes resource supporting RCE Terminates random process","title":"Shell Experiments Suite"},{"location":"architecture/","text":"Architecture Application Information Language Java 11 Operating System Cross Platform Platform Java Virtual Machine Frameworks Spring Framework Spring Boot 2.2.3.RELEASE Port Requirements API Interface: 8080 Runtime Dependencies Docker Minimum Version 18 Recommended Version 18.09.7+ Link Docker HashiCorp Vault (Optional) Minimum Version 0.9.x Recommended Version 0.10.x Link Vault by HashiCorp Docker Image Vault - Official Docker Image DataDog Agent (Optional) Docker Image DataDog Agent for Docker Development Dependencies Java Minimum Version 11 Runtime Environment Java SE Runtime Environment Development Kit JDK 11 from OpenJDK Maven Minimum Version 3.6.0 Recommended Version 3.6.1 Link Apache Maven Project MkDocs Minimum Version 1.0.4 Link MkDocs Docker Compose Minimum Version 1.22.0 Link Install Docker Compose Tip The Linux Installation steps for docker-compose instruct you to extract the file into /usr/local/bin . In some flavours of Linux, a copy of docker-compose may already exist in another path. If you seem to still have an old version after running the installation, try running which docker-compose to verify you are executing the version you just downloaded.","title":"Architecture"},{"location":"architecture/#architecture","text":"Application Information Language Java 11 Operating System Cross Platform Platform Java Virtual Machine Frameworks Spring Framework Spring Boot 2.2.3.RELEASE Port Requirements API Interface: 8080","title":"Architecture"},{"location":"architecture/#runtime-dependencies","text":"Docker Minimum Version 18 Recommended Version 18.09.7+ Link Docker HashiCorp Vault (Optional) Minimum Version 0.9.x Recommended Version 0.10.x Link Vault by HashiCorp Docker Image Vault - Official Docker Image DataDog Agent (Optional) Docker Image DataDog Agent for Docker","title":"Runtime Dependencies"},{"location":"architecture/#development-dependencies","text":"Java Minimum Version 11 Runtime Environment Java SE Runtime Environment Development Kit JDK 11 from OpenJDK Maven Minimum Version 3.6.0 Recommended Version 3.6.1 Link Apache Maven Project MkDocs Minimum Version 1.0.4 Link MkDocs Docker Compose Minimum Version 1.22.0 Link Install Docker Compose Tip The Linux Installation steps for docker-compose instruct you to extract the file into /usr/local/bin . In some flavours of Linux, a copy of docker-compose may already exist in another path. If you seem to still have an old version after running the installation, try running which docker-compose to verify you are executing the version you just downloaded.","title":"Development Dependencies"},{"location":"configuration/","text":"Configuration Chaos Engine reads configuration from multiple locations, and in multiple naming conventions. application.properties file As a Spring Boot application, the application will look for a file named application.properties in the ./config folder. This file should be formatted with one variable per line, in the format of key=value . Line breaks can be made by adding a backslash ( \\ ) at the end of the line. aws.ec2.routableCidrBlocks=10.0.0.0/8,192.168.100.0/24 aws.ec2.filter.tag.chaosTestTag=chaosTesting aws.ec2.filter.keyName=chaosTestKey aws.ec2.privateSshKeys.chaosTestKey=-----BEGIN RSA PRIVATE KEY-----\\ MIIEpgIBAAKCAQEAuENqLqKYT7vld6EvSK1myOH29dX2lb3sLEXcHybgKGr1kjjU\\ ... cxsbp/4QHL+kwxzKqF6w3s6ZQ5sOh8vBoUf3RhdjM7NY7dQOHhUltfN6\\ -----END RSA PRIVATE KEY----- Environment Variables Environment variables can be read as configuration properties. Given their naming restrictions, Spring Framework remaps them intelligently such that an underscore can either represent a period or a new word with camelCaps. Environment Variable Parsed as AWS_EC2_ROUTABLE_CIDR_BLOCKS aws.ec2.routableCidrBlocks AWS_EC2_FILTER_TAG_CHAOS_TEST_TAG aws.ec2.filter.tag.chaosTestTag AWS_EC2_PRIVATE_SSH_KEYS_TEST_KEY aws.ec2.privateSshKeys.testKey Vault Vault needs to be configured using one of the other methods. Once configured, properties from Vault are loaded in. Secrets need to be named in the same manner as in the application.properties file configuration .","title":"Configuration"},{"location":"configuration/#configuration","text":"Chaos Engine reads configuration from multiple locations, and in multiple naming conventions.","title":"Configuration"},{"location":"configuration/#applicationproperties-file","text":"As a Spring Boot application, the application will look for a file named application.properties in the ./config folder. This file should be formatted with one variable per line, in the format of key=value . Line breaks can be made by adding a backslash ( \\ ) at the end of the line. aws.ec2.routableCidrBlocks=10.0.0.0/8,192.168.100.0/24 aws.ec2.filter.tag.chaosTestTag=chaosTesting aws.ec2.filter.keyName=chaosTestKey aws.ec2.privateSshKeys.chaosTestKey=-----BEGIN RSA PRIVATE KEY-----\\ MIIEpgIBAAKCAQEAuENqLqKYT7vld6EvSK1myOH29dX2lb3sLEXcHybgKGr1kjjU\\ ... cxsbp/4QHL+kwxzKqF6w3s6ZQ5sOh8vBoUf3RhdjM7NY7dQOHhUltfN6\\ -----END RSA PRIVATE KEY-----","title":"application.properties file"},{"location":"configuration/#environment-variables","text":"Environment variables can be read as configuration properties. Given their naming restrictions, Spring Framework remaps them intelligently such that an underscore can either represent a period or a new word with camelCaps. Environment Variable Parsed as AWS_EC2_ROUTABLE_CIDR_BLOCKS aws.ec2.routableCidrBlocks AWS_EC2_FILTER_TAG_CHAOS_TEST_TAG aws.ec2.filter.tag.chaosTestTag AWS_EC2_PRIVATE_SSH_KEYS_TEST_KEY aws.ec2.privateSshKeys.testKey","title":"Environment Variables"},{"location":"configuration/#vault","text":"Vault needs to be configured using one of the other methods. Once configured, properties from Vault are loaded in. Secrets need to be named in the same manner as in the application.properties file configuration .","title":"Vault"},{"location":"hardening/","text":"Hardening Guidelines Chaos Engine Framework Chaos Engine Source Code If you build your own Chaos Engine images make sure that your source code was downloaded from official Gemalto repository in GitHub . Docker Images Docker registry Official public repository is in DockerHub. There are two important tags: stable \u2013 containing latest stable release of the framework latest \u2013 representing latest development snapshot It's recommended to run master branch based images in production environments Remove development configuration options Activate production profile, make sure that following environment variables are set: SPRING_PROFILES_ACTIVE=PRODUCTION DEPLOYMENT_ENVIRONMENT=PROD Verify that the HTTPS scheme is used for communication with Vault. Environment variable VAULT_SCHEME must be set to HTTPS or left empty. Secure Chaos Engine REST API Follow REST Security documentation Vault Apply following rules: Avoid usage of the dev Vault token ( 00000000-0000-0000-0000-000000000000 ). Generate new Vault token. Do not use root tokens Enable SSL Provision Vault securely Advance Hardening Recommendations How to enable SSL Securing Vault How to create new Vault token Token Creation How to securely preload Vault with secrets and config The Engine started using docker-compose If you start the Chaos Engine using docker-compose.yml script located in the root of the git repo, the Vault server will be automatically provision with secrets. Those secrets are loaded from vault-secrets.json located in ./developer-tools/vault-loader. Delete the vault-secrets.json when the Engine start up is completed. The Vault Running as a stand alone service Download Vault binary Run following commands VAULT_TOKEN = $( cat /path/to/token ) export VAULT_TOKEN export VAULT_ADDR = 'https://$VAULT_HOST:$VAULT_PORT' ; ./vault auth $VAULT_TOKEN ; vault kv put secret/chaosengine - < vault-secrets.json Vault alternatives If you deploy the Chaos Engine to K8S the Vault can be replaced by Kubernetes Secrets. DataDog Generate a new API key dedicated to your Chaos Engine instance and provision the DataDog agent with that new key. Slack Create a new Slack channel that will be used as a dumping group for Chaos Engine notifications. Create a new Slack token and link the token with the channel created in previous step.","title":"Hardening Guidelines"},{"location":"hardening/#hardening-guidelines","text":"","title":"Hardening Guidelines"},{"location":"hardening/#chaos-engine-framework","text":"","title":"Chaos Engine Framework"},{"location":"hardening/#chaos-engine-source-code","text":"If you build your own Chaos Engine images make sure that your source code was downloaded from official Gemalto repository in GitHub .","title":"Chaos Engine Source Code"},{"location":"hardening/#docker-images","text":"","title":"Docker Images"},{"location":"hardening/#docker-registry","text":"Official public repository is in DockerHub. There are two important tags: stable \u2013 containing latest stable release of the framework latest \u2013 representing latest development snapshot It's recommended to run master branch based images in production environments","title":"Docker registry"},{"location":"hardening/#remove-development-configuration-options","text":"Activate production profile, make sure that following environment variables are set: SPRING_PROFILES_ACTIVE=PRODUCTION DEPLOYMENT_ENVIRONMENT=PROD Verify that the HTTPS scheme is used for communication with Vault. Environment variable VAULT_SCHEME must be set to HTTPS or left empty.","title":"Remove development configuration options"},{"location":"hardening/#secure-chaos-engine-rest-api","text":"Follow REST Security documentation","title":"Secure Chaos Engine REST API"},{"location":"hardening/#vault","text":"Apply following rules: Avoid usage of the dev Vault token ( 00000000-0000-0000-0000-000000000000 ). Generate new Vault token. Do not use root tokens Enable SSL Provision Vault securely Advance Hardening Recommendations","title":"Vault"},{"location":"hardening/#how-to-enable-ssl","text":"Securing Vault","title":"How to enable SSL"},{"location":"hardening/#how-to-create-new-vault-token","text":"Token Creation","title":"How to create new Vault token"},{"location":"hardening/#how-to-securely-preload-vault-with-secrets-and-config","text":"","title":"How to securely preload Vault with secrets and config"},{"location":"hardening/#the-engine-started-using-docker-compose","text":"If you start the Chaos Engine using docker-compose.yml script located in the root of the git repo, the Vault server will be automatically provision with secrets. Those secrets are loaded from vault-secrets.json located in ./developer-tools/vault-loader. Delete the vault-secrets.json when the Engine start up is completed.","title":"The Engine started using docker-compose"},{"location":"hardening/#the-vault-running-as-a-stand-alone-service","text":"Download Vault binary Run following commands VAULT_TOKEN = $( cat /path/to/token ) export VAULT_TOKEN export VAULT_ADDR = 'https://$VAULT_HOST:$VAULT_PORT' ; ./vault auth $VAULT_TOKEN ; vault kv put secret/chaosengine - < vault-secrets.json","title":"The Vault Running as a stand alone service"},{"location":"hardening/#vault-alternatives","text":"If you deploy the Chaos Engine to K8S the Vault can be replaced by Kubernetes Secrets.","title":"Vault alternatives"},{"location":"hardening/#datadog","text":"Generate a new API key dedicated to your Chaos Engine instance and provision the DataDog agent with that new key.","title":"DataDog"},{"location":"hardening/#slack","text":"Create a new Slack channel that will be used as a dumping group for Chaos Engine notifications. Create a new Slack token and link the token with the channel created in previous step.","title":"Slack"},{"location":"Configuration/","text":"Chaos Engine Configuration Chaos Engine can be configured from multiple sources. In addition, the naming conventions take advantage of Spring Relaxed Bindings, meaning a property can be identified with multiple naming conventions. Each modules own documentation will indicate what specific configuration it requires. Configuration Sources application.properties File Spring Boot looks for a file named application.properties under the ./config directory to load properties from. This file should be formatted in key=value format. Use backslashes to preserve line breaks. application.properties aws.ec2.routableCidrBlocks = 10 .0.0.0/8,192.168.100.0/24 aws.ec2.filter.tag.chaosTestTag = chaosTesting aws.ec2.filter.keyName = chaosTestKey aws.ec2.privateSshKeys.chaosTestKey = -----BEGIN RSA PRIVATE KEY----- \\ MIIEpgIBAAKCAQEAuENqLqKYT7vld6EvSK1myOH29dX2lb3sLEXcHybgKGr1kjjU \\ ... cxsbp/4QHL+kwxzKqF6w3s6ZQ5sOh8vBoUf3RhdjM7NY7dQOHhUltfN6 \\ -----END RSA PRIVATE KEY----- Environment Variables Spring will read all environment variables to populate variables. Due to restricted characters in environment variables, you will need to refer to Relaxed Bindings to create equivalent variable names for use in shell. Vault See Vault Integration for more information. Relaxed Bindings Reference: https://github.com/spring-projects/spring-boot/wiki/relaxed-binding-2.0 Spring Framework allows for multiple naming conventions to be used to bind a configuration into a single variable in the underlying Java code. Depending on where you are configuring the property from, certain characters may be unavailable, forcing you to use another naming convention. Examples Multi-word property name You can configure the AWS Module using the properties aws.secret-access-key and aws.access-key-id . In addition, you could use camelCaps ( aws.secretAccessKey and aws.accessKeyId ) or CAPS_AND_UNDERSCORES ( AWS_SECRET_ACCESS_KEY and AWS_ACCESS_KEY_ID ) to configure these values. Arrays If you are configuring users for HTTP Authentication, you would add them under chaos.security.users[0].username et al . To use CAPS_AND_UNDERSCORES, simply treat the array index as its own word (i.e., CHAOS_SECURITY_USERS_0_USERNAME ).","title":"Chaos Engine Configuration"},{"location":"Configuration/#chaos-engine-configuration","text":"Chaos Engine can be configured from multiple sources. In addition, the naming conventions take advantage of Spring Relaxed Bindings, meaning a property can be identified with multiple naming conventions. Each modules own documentation will indicate what specific configuration it requires.","title":"Chaos Engine Configuration"},{"location":"Configuration/#configuration-sources","text":"","title":"Configuration Sources"},{"location":"Configuration/#applicationproperties-file","text":"Spring Boot looks for a file named application.properties under the ./config directory to load properties from. This file should be formatted in key=value format. Use backslashes to preserve line breaks. application.properties aws.ec2.routableCidrBlocks = 10 .0.0.0/8,192.168.100.0/24 aws.ec2.filter.tag.chaosTestTag = chaosTesting aws.ec2.filter.keyName = chaosTestKey aws.ec2.privateSshKeys.chaosTestKey = -----BEGIN RSA PRIVATE KEY----- \\ MIIEpgIBAAKCAQEAuENqLqKYT7vld6EvSK1myOH29dX2lb3sLEXcHybgKGr1kjjU \\ ... cxsbp/4QHL+kwxzKqF6w3s6ZQ5sOh8vBoUf3RhdjM7NY7dQOHhUltfN6 \\ -----END RSA PRIVATE KEY-----","title":"application.properties File"},{"location":"Configuration/#environment-variables","text":"Spring will read all environment variables to populate variables. Due to restricted characters in environment variables, you will need to refer to Relaxed Bindings to create equivalent variable names for use in shell.","title":"Environment Variables"},{"location":"Configuration/#vault","text":"See Vault Integration for more information.","title":"Vault"},{"location":"Configuration/#relaxed-bindings","text":"Reference: https://github.com/spring-projects/spring-boot/wiki/relaxed-binding-2.0 Spring Framework allows for multiple naming conventions to be used to bind a configuration into a single variable in the underlying Java code. Depending on where you are configuring the property from, certain characters may be unavailable, forcing you to use another naming convention.","title":"Relaxed Bindings"},{"location":"Configuration/#examples","text":"","title":"Examples"},{"location":"Configuration/#multi-word-property-name","text":"You can configure the AWS Module using the properties aws.secret-access-key and aws.access-key-id . In addition, you could use camelCaps ( aws.secretAccessKey and aws.accessKeyId ) or CAPS_AND_UNDERSCORES ( AWS_SECRET_ACCESS_KEY and AWS_ACCESS_KEY_ID ) to configure these values.","title":"Multi-word property name"},{"location":"Configuration/#arrays","text":"If you are configuring users for HTTP Authentication, you would add them under chaos.security.users[0].username et al . To use CAPS_AND_UNDERSCORES, simply treat the array index as its own word (i.e., CHAOS_SECURITY_USERS_0_USERNAME ).","title":"Arrays"},{"location":"Configuration/security/","text":"REST Security The Chaos Engine REST API is secured using a cookie-based Authentication System. Users can log in using preconfigured username and password combinations to grant access roles. The roles they are granted will determine which API Endpoints they are allowed to access. Authentication Credentials Credentials are preconfigured in Spring Properties under an array of chaos.security.users . Each user object will have a username , a password , and a comma separated list of roles . They can be configured in any location that Chaos Engine looks for properties, and follows the same relaxed-binding mechanism that other variables use. Vault { \"chaos.security.users[0].username\" : \"admin\" , \"chaos.security.users[0].password\" : \"admin_P@ssw0rd\" , \"chaos.security.users[0].roles\" : \"ADMIN\" , \"chaos.security.users[1].username\" : \"user\" , \"chaos.security.users[1].password\" : \"user_P@ssw0rd\" , \"chaos.security.users[1].roles\" : \"USER\" } ENV Vars CHAOS_SECURITY_USERS_0_USERNAME = admin CHAOS_SECURITY_USERS_0_PASSWORD = admin_P@ssw0rd CHAOS_SECURITY_USERS_0_ROLES = ADMIN CHAOS_SECURITY_USERS_1_USERNAME = user CHAOS_SECURITY_USERS_1_PASSWORD = user_P@ssw0rd CHAOS_SECURITY_USERS_1_ROLES = USER Authentication Endpoint Authentication can be done by sending a POST request to the /login endpoint of the Chaos Engine, and specifying a username and password in the data fields. Be sure to capture the cookies returned by this request. Request curl -vvv -s localhost:8080/login -X POST --data username = admin --data password = admin -c /dev/stdout * Trying 127 .0.0.1... * TCP_NODELAY set * Connected to localhost ( 127 .0.0.1 ) port 8080 ( #0) > POST /login HTTP/1.1 > Host: localhost:8080 > User-Agent: curl/7.58.0 > Accept: */* > Content-Length: 29 > Content-Type: application/x-www-form-urlencoded > * upload completely sent off: 29 out of 29 bytes Response < HTTP/1.1 200 * cookie size: name/val 10 + 32 bytes * cookie size: name/val 4 + 1 bytes * cookie size: name/val 8 + 0 bytes * Added cookie JSESSIONID = \"2DED401442B2AC62DD15DC0B60A62BA5\" for domain localhost, path /, expire 0 < Set-Cookie: JSESSIONID = 2DED401442B2AC62DD15DC0B60A62BA5 ; Path = / ; HttpOnly < X-Content-Type-Options: nosniff < X-XSS-Protection: 1 ; mode = block < Cache-Control: no-cache, no-store, max-age = 0 , must-revalidate < Pragma: no-cache < Expires: 0 < X-Frame-Options: DENY < Content-Length: 0 < Date: Fri, 01 Nov 2019 18 :05:06 GMT < * Connection #0 to host localhost left intact Cookies # Netscape HTTP Cookie File # https://curl.haxx.se/docs/http-cookies.html # This file was generated by libcurl! Edit at your own risk. #HttpOnly_localhost FALSE / FALSE 0 JSESSIONID 2DED401442B2AC62DD15DC0B60A62BA5 Sending Authenticated Requests Sending the JSESSIONID cookie that was returned by the login request along with any REST call in order to authenticate it. Ending a session Sending the JSESSIONID cookie to the /logout endpoint in a POST request will terminate the session. The session will also end after 15 minutes of inactivity. Permission Levels There are three distinct levels of permissions programmed into the system. Unauthenticated Unauthenticated users can access the GET /health endpoint, in order for container orchestrators (i.e., Kubernetes) to run a health check of the system. Generic Authentication Authenticated users with no specific roles can access any GET based endpoint. Admin Authentication Users with the ADMIN role can access all endpoints. Disabling Security Requirements It is possible that in some environments, you may want to disable the security layer (for example, in a CICD Pipeline). This can be accomplished by setting the property chaos.security.enabled with a value of false .","title":"REST Security"},{"location":"Configuration/security/#rest-security","text":"The Chaos Engine REST API is secured using a cookie-based Authentication System. Users can log in using preconfigured username and password combinations to grant access roles. The roles they are granted will determine which API Endpoints they are allowed to access.","title":"REST Security"},{"location":"Configuration/security/#authentication-credentials","text":"Credentials are preconfigured in Spring Properties under an array of chaos.security.users . Each user object will have a username , a password , and a comma separated list of roles . They can be configured in any location that Chaos Engine looks for properties, and follows the same relaxed-binding mechanism that other variables use. Vault { \"chaos.security.users[0].username\" : \"admin\" , \"chaos.security.users[0].password\" : \"admin_P@ssw0rd\" , \"chaos.security.users[0].roles\" : \"ADMIN\" , \"chaos.security.users[1].username\" : \"user\" , \"chaos.security.users[1].password\" : \"user_P@ssw0rd\" , \"chaos.security.users[1].roles\" : \"USER\" } ENV Vars CHAOS_SECURITY_USERS_0_USERNAME = admin CHAOS_SECURITY_USERS_0_PASSWORD = admin_P@ssw0rd CHAOS_SECURITY_USERS_0_ROLES = ADMIN CHAOS_SECURITY_USERS_1_USERNAME = user CHAOS_SECURITY_USERS_1_PASSWORD = user_P@ssw0rd CHAOS_SECURITY_USERS_1_ROLES = USER","title":"Authentication Credentials"},{"location":"Configuration/security/#authentication-endpoint","text":"Authentication can be done by sending a POST request to the /login endpoint of the Chaos Engine, and specifying a username and password in the data fields. Be sure to capture the cookies returned by this request. Request curl -vvv -s localhost:8080/login -X POST --data username = admin --data password = admin -c /dev/stdout * Trying 127 .0.0.1... * TCP_NODELAY set * Connected to localhost ( 127 .0.0.1 ) port 8080 ( #0) > POST /login HTTP/1.1 > Host: localhost:8080 > User-Agent: curl/7.58.0 > Accept: */* > Content-Length: 29 > Content-Type: application/x-www-form-urlencoded > * upload completely sent off: 29 out of 29 bytes Response < HTTP/1.1 200 * cookie size: name/val 10 + 32 bytes * cookie size: name/val 4 + 1 bytes * cookie size: name/val 8 + 0 bytes * Added cookie JSESSIONID = \"2DED401442B2AC62DD15DC0B60A62BA5\" for domain localhost, path /, expire 0 < Set-Cookie: JSESSIONID = 2DED401442B2AC62DD15DC0B60A62BA5 ; Path = / ; HttpOnly < X-Content-Type-Options: nosniff < X-XSS-Protection: 1 ; mode = block < Cache-Control: no-cache, no-store, max-age = 0 , must-revalidate < Pragma: no-cache < Expires: 0 < X-Frame-Options: DENY < Content-Length: 0 < Date: Fri, 01 Nov 2019 18 :05:06 GMT < * Connection #0 to host localhost left intact Cookies # Netscape HTTP Cookie File # https://curl.haxx.se/docs/http-cookies.html # This file was generated by libcurl! Edit at your own risk. #HttpOnly_localhost FALSE / FALSE 0 JSESSIONID 2DED401442B2AC62DD15DC0B60A62BA5","title":"Authentication Endpoint"},{"location":"Configuration/security/#sending-authenticated-requests","text":"Sending the JSESSIONID cookie that was returned by the login request along with any REST call in order to authenticate it.","title":"Sending Authenticated Requests"},{"location":"Configuration/security/#ending-a-session","text":"Sending the JSESSIONID cookie to the /logout endpoint in a POST request will terminate the session. The session will also end after 15 minutes of inactivity.","title":"Ending a session"},{"location":"Configuration/security/#permission-levels","text":"There are three distinct levels of permissions programmed into the system.","title":"Permission Levels"},{"location":"Configuration/security/#unauthenticated","text":"Unauthenticated users can access the GET /health endpoint, in order for container orchestrators (i.e., Kubernetes) to run a health check of the system.","title":"Unauthenticated"},{"location":"Configuration/security/#generic-authentication","text":"Authenticated users with no specific roles can access any GET based endpoint.","title":"Generic Authentication"},{"location":"Configuration/security/#admin-authentication","text":"Users with the ADMIN role can access all endpoints.","title":"Admin Authentication"},{"location":"Configuration/security/#disabling-security-requirements","text":"It is possible that in some environments, you may want to disable the security layer (for example, in a CICD Pipeline). This can be accomplished by setting the property chaos.security.enabled with a value of false .","title":"Disabling Security Requirements"},{"location":"Configuration/vault_integration/","text":"Vault Integration Chaos Engine makes use of Vault, both to securely store information. This information can be rotated in Vault and automatically refresh in runtime in Chaos Engine. Secret Location Secrets should be configured under the path /secrets/chaosengine . If using Spring Profiles, it will first try to look under secrets/chaosengine/{profile-name} . Tip If overriding the value for spring.application.name , then that application name will be used for the secrets path instead of chaosengine . SDK Given the already large use of Spring Framework, Spring Cloud Vault is used to seamlessly integrate with Vault. Configuration The vault endpoint is configured through environment variables. Variable Description Default Value VAULT_TOKEN Token for access into the Vault 00000000-0000-0000-0000-000000000000 VAULT_SCHEME HTTP/HTTPS scheme for access into Vault http VAULT_HOST FQDN of the Vault Host localhost VAULT_PORT Port of the Vault Host 8200 VAULT_10 Used to enable support for HashiCorp Vault v 0.10.x KV Version 2 false SDK Native Variables Variable Mapped Variable Description Default spring.cloud.vault.token VAULT_TOKEN Token used for authentication when the authentication mechanism is TOKEN 00000000-0000-0000-0000-000000000000 spring.cloud.vault.port VAULT_PORT Port of the Vault Host 8200 spring.cloud.vault.uri N/A The full URI of {scheme}://{host}:{port}/ Derived from host/port/scheme. spring.cloud.vault.fail-fast N/A If set, application startup will fail if Vault is required and cannot be accessed. false spring.cloud.vault.kv.enabled VAULT_10 Enables support for Key/Version Version 2, introduced in HashiCorp Vault 0.10 false spring.cloud.vault.scheme VAULT_SCHEME HTTP/HTTPS scheme for access into Vault https spring.cloud.vault.host VAULT_HOST FQDN of the Vault Host localhost spring.cloud.vault.authentication N/A The Authentication Mechanism used to authenticate against Vault TOKEN spring.cloud.vault.enabled N/A Flag to enable/disable Vault Integration true Feeding Vault With Data Example export VAULT_ADDR='http://$VAULT_HOST:8200' export VAULT_DEV_ROOT_TOKEN_ID=00000000-0000-0000-0000-000000000000 vault auth $VAULT_DEV_ROOT_TOKEN_ID vault kv put secret/chaosengine aws.accessKeyId=$AWS_ACCESS_KEY_ID aws.secretAccessKey=$AWS_SECRET_ACCESS_KEY aws.region=eu-west-1 AWS_FILTER_KEYS='Chaos Victim' AWS_FILTER_VALUES=true holidays=DUM aws.ec2=true vault kv get secret/chaosengine Triggering a Refresh The Java Objects are not automatically refreshed when values are changed in vault. Instead, an API must be called to trigger a reload. This should be done after ALL values have changed, as calling it with only some values changed may have unpredictable errors (i.e., Access Key ID changed but Secret Key is unchanged may cause errors). In order to trigger the request, a HTTP POST request needs to be made against http://{chaosengine}/refresh . Spring Profiles The Spring Cloud Vault integration makes use of the Spring Profile option to allow for different variables to be accessible from different environments. By default, Chaos Engine will look in /secret/chaosengine for the appropriate resource name. However, if run with a specified Spring Profile Name, it will look in /secret/chaosengine/{Profile Name}/ first, allowing for distinct secrets to be given in different environments. Development Notes ConfigurationProperties Classes that need to use many values should use the \\@ConfigurationProperties annotation. This can be specified with a prefix. When done, any variables named {prefix}.{variable} will automatically be assigned using a Setter method for that variable. Example of \\@ConfigurationProperties @Component @ConfigurationProperties ( prefix = \"chaos\" ) public class MyClass { private String myString ; // This will inherit chaos.myString's value private Integer myInteger ; // This will inherit chaos.myInteger's value /* Setters are needed for the variables in order for ConfigurationProperties to inject them. */ public void setMyString ( String myString ) { this . myString = myString ; } public void setMyInteger ( Integer myInteger ) { this . myInteger = myInteger ; } /* Beans need a RefreshScope annotation in order to be reinitialized when changed. */ @Bean @RefreshScope MyBean myBean () { return new MyBean ( myString , myInteger ); } } RefreshScope Spring Beans that use values from ConfigurationProperties can be flagged for reinitializing with a \\@RefreshScope annotation. Spring does this by adding an interpretation layer between where the bean is Autowired and where it is created, extending the class of the bean to do so. This does mean that it will not work with beans of a final class. See above for an example.","title":"Vault Integration"},{"location":"Configuration/vault_integration/#vault-integration","text":"Chaos Engine makes use of Vault, both to securely store information. This information can be rotated in Vault and automatically refresh in runtime in Chaos Engine.","title":"Vault Integration"},{"location":"Configuration/vault_integration/#secret-location","text":"Secrets should be configured under the path /secrets/chaosengine . If using Spring Profiles, it will first try to look under secrets/chaosengine/{profile-name} . Tip If overriding the value for spring.application.name , then that application name will be used for the secrets path instead of chaosengine .","title":"Secret Location"},{"location":"Configuration/vault_integration/#sdk","text":"Given the already large use of Spring Framework, Spring Cloud Vault is used to seamlessly integrate with Vault.","title":"SDK"},{"location":"Configuration/vault_integration/#configuration","text":"The vault endpoint is configured through environment variables. Variable Description Default Value VAULT_TOKEN Token for access into the Vault 00000000-0000-0000-0000-000000000000 VAULT_SCHEME HTTP/HTTPS scheme for access into Vault http VAULT_HOST FQDN of the Vault Host localhost VAULT_PORT Port of the Vault Host 8200 VAULT_10 Used to enable support for HashiCorp Vault v 0.10.x KV Version 2 false SDK Native Variables Variable Mapped Variable Description Default spring.cloud.vault.token VAULT_TOKEN Token used for authentication when the authentication mechanism is TOKEN 00000000-0000-0000-0000-000000000000 spring.cloud.vault.port VAULT_PORT Port of the Vault Host 8200 spring.cloud.vault.uri N/A The full URI of {scheme}://{host}:{port}/ Derived from host/port/scheme. spring.cloud.vault.fail-fast N/A If set, application startup will fail if Vault is required and cannot be accessed. false spring.cloud.vault.kv.enabled VAULT_10 Enables support for Key/Version Version 2, introduced in HashiCorp Vault 0.10 false spring.cloud.vault.scheme VAULT_SCHEME HTTP/HTTPS scheme for access into Vault https spring.cloud.vault.host VAULT_HOST FQDN of the Vault Host localhost spring.cloud.vault.authentication N/A The Authentication Mechanism used to authenticate against Vault TOKEN spring.cloud.vault.enabled N/A Flag to enable/disable Vault Integration true Feeding Vault With Data Example export VAULT_ADDR='http://$VAULT_HOST:8200' export VAULT_DEV_ROOT_TOKEN_ID=00000000-0000-0000-0000-000000000000 vault auth $VAULT_DEV_ROOT_TOKEN_ID vault kv put secret/chaosengine aws.accessKeyId=$AWS_ACCESS_KEY_ID aws.secretAccessKey=$AWS_SECRET_ACCESS_KEY aws.region=eu-west-1 AWS_FILTER_KEYS='Chaos Victim' AWS_FILTER_VALUES=true holidays=DUM aws.ec2=true vault kv get secret/chaosengine","title":"Configuration"},{"location":"Configuration/vault_integration/#triggering-a-refresh","text":"The Java Objects are not automatically refreshed when values are changed in vault. Instead, an API must be called to trigger a reload. This should be done after ALL values have changed, as calling it with only some values changed may have unpredictable errors (i.e., Access Key ID changed but Secret Key is unchanged may cause errors). In order to trigger the request, a HTTP POST request needs to be made against http://{chaosengine}/refresh .","title":"Triggering a Refresh"},{"location":"Configuration/vault_integration/#spring-profiles","text":"The Spring Cloud Vault integration makes use of the Spring Profile option to allow for different variables to be accessible from different environments. By default, Chaos Engine will look in /secret/chaosengine for the appropriate resource name. However, if run with a specified Spring Profile Name, it will look in /secret/chaosengine/{Profile Name}/ first, allowing for distinct secrets to be given in different environments.","title":"Spring Profiles"},{"location":"Configuration/vault_integration/#development-notes","text":"","title":"Development Notes"},{"location":"Configuration/vault_integration/#configurationproperties","text":"Classes that need to use many values should use the \\@ConfigurationProperties annotation. This can be specified with a prefix. When done, any variables named {prefix}.{variable} will automatically be assigned using a Setter method for that variable. Example of \\@ConfigurationProperties @Component @ConfigurationProperties ( prefix = \"chaos\" ) public class MyClass { private String myString ; // This will inherit chaos.myString's value private Integer myInteger ; // This will inherit chaos.myInteger's value /* Setters are needed for the variables in order for ConfigurationProperties to inject them. */ public void setMyString ( String myString ) { this . myString = myString ; } public void setMyInteger ( Integer myInteger ) { this . myInteger = myInteger ; } /* Beans need a RefreshScope annotation in order to be reinitialized when changed. */ @Bean @RefreshScope MyBean myBean () { return new MyBean ( myString , myInteger ); } }","title":"ConfigurationProperties"},{"location":"Configuration/vault_integration/#refreshscope","text":"Spring Beans that use values from ConfigurationProperties can be flagged for reinitializing with a \\@RefreshScope annotation. Spring does this by adding an interpretation layer between where the bean is Autowired and where it is created, extending the class of the bean to do so. This does mean that it will not work with beans of a final class. See above for an example.","title":"RefreshScope"},{"location":"Core_Modules/administrative_state/","text":"Administrative State The Administrative State of Chaos Engine controls the workflow of Experiments. The Administrative State can be controlled using the REST API . Available States State Description STARTING The Chaos Engine has not completed starting yet. STARTED Chaos Engine is up and running. Experiments are in full lifecycle. PAUSED Experiments are paused. No external actions are performed (no new experiments, no self-healing, no finalization) DRAIN New experiments are paused, but existing experiments are allowed to complete, including self-healing and finalization ABORT New experiments are paused. Existing experiments are immediately advanced to their self-healing phase. For more information on Experiment Lifecycles, see the Experiment Manager documentation.","title":"Administrative State"},{"location":"Core_Modules/administrative_state/#administrative-state","text":"The Administrative State of Chaos Engine controls the workflow of Experiments. The Administrative State can be controlled using the REST API .","title":"Administrative State"},{"location":"Core_Modules/administrative_state/#available-states","text":"State Description STARTING The Chaos Engine has not completed starting yet. STARTED Chaos Engine is up and running. Experiments are in full lifecycle. PAUSED Experiments are paused. No external actions are performed (no new experiments, no self-healing, no finalization) DRAIN New experiments are paused, but existing experiments are allowed to complete, including self-healing and finalization ABORT New experiments are paused. Existing experiments are immediately advanced to their self-healing phase. For more information on Experiment Lifecycles, see the Experiment Manager documentation.","title":"Available States"},{"location":"Core_Modules/container_manager/","text":"Container Manager Description The Container Manager in Chaos Engine is designed to cache information about containers and prevent the need for recreating them repeatedly. Since we expect most containers to have a relatively long life cycle, we should prevent recreating objects every time we scan the platforms. Development Notes Usage See this simple code example on how the Container Manager should be used in the process of creating a container. ContainerManager Usage package com.thales.chaos ; import com.thales.chaos.container.Container ; import com.thales.chaos.container.ContainerManager ; import com.thales.chaos.container.enums.ContainerHealth ; import com.thales.chaos.experiment.enums.ExperimentType ; import com.thales.chaos.notification.datadog.DataDogIdentifier ; import com.thales.chaos.platform.Platform ; import org.springframework.beans.factory.annotation.Autowired ; import org.springframework.stereotype.Service ; import javax.validation.constraints.NotNull ; @Service public class Example { @Autowired private ContainerManager containerManager ; private ExampleContainer getContainer ( Object ... args ) { ExampleContainer exampleContainer = containerManager . getMatchingContainer ( ExampleContainer . class , args . getIdentifier ()); if ( exampleContainer == null ) { exampleContainer = actuallyCreateContainer (); containerManager . offer ( exampleContainer ); } return exampleContainer ; } private ExampleContainer actuallyCreateContainer () { return new ExampleContainer () {}; } private abstract class ExampleContainer extends Container { } } In this example class, a Spring Service is attempting to look for an existing container if it already exists in the Container Manager cache (Line 20) . If it does not exist (21) , it will create the container using another call (22) , and then offer that container back into the Container Manager (23) , before finally returning the container (25) that was either retrieved from the cache or created.","title":"Container Manager"},{"location":"Core_Modules/container_manager/#container-manager","text":"","title":"Container Manager"},{"location":"Core_Modules/container_manager/#description","text":"The Container Manager in Chaos Engine is designed to cache information about containers and prevent the need for recreating them repeatedly. Since we expect most containers to have a relatively long life cycle, we should prevent recreating objects every time we scan the platforms.","title":"Description"},{"location":"Core_Modules/container_manager/#development-notes","text":"","title":"Development Notes"},{"location":"Core_Modules/container_manager/#usage","text":"See this simple code example on how the Container Manager should be used in the process of creating a container. ContainerManager Usage package com.thales.chaos ; import com.thales.chaos.container.Container ; import com.thales.chaos.container.ContainerManager ; import com.thales.chaos.container.enums.ContainerHealth ; import com.thales.chaos.experiment.enums.ExperimentType ; import com.thales.chaos.notification.datadog.DataDogIdentifier ; import com.thales.chaos.platform.Platform ; import org.springframework.beans.factory.annotation.Autowired ; import org.springframework.stereotype.Service ; import javax.validation.constraints.NotNull ; @Service public class Example { @Autowired private ContainerManager containerManager ; private ExampleContainer getContainer ( Object ... args ) { ExampleContainer exampleContainer = containerManager . getMatchingContainer ( ExampleContainer . class , args . getIdentifier ()); if ( exampleContainer == null ) { exampleContainer = actuallyCreateContainer (); containerManager . offer ( exampleContainer ); } return exampleContainer ; } private ExampleContainer actuallyCreateContainer () { return new ExampleContainer () {}; } private abstract class ExampleContainer extends Container { } } In this example class, a Spring Service is attempting to look for an existing container if it already exists in the Container Manager cache (Line 20) . If it does not exist (21) , it will create the container using another call (22) , and then offer that container back into the Container Manager (23) , before finally returning the container (25) that was either retrieved from the cache or created.","title":"Usage"},{"location":"Core_Modules/experiment_manager/","text":"Experiment Manager The experiment manager automatically chooses a platform to perform an experiment on, select some containers from that platform, and select experiments to run on those containers. It will regularly poll the platforms of these containers, checking if the issue has healed itself, and will attempt to self heal the experiment after a minimum time. Experiment Life Cycle Creating Experiments A scheduled method runs every 15 seconds and evaluates platforms for any eligible to run an experiment on, based on the Scheduler. If no platforms are eligible for experimentation, no operation is done. If multiple platforms are eligible for experimentation, one is chosen randomly. If there is a platform eligible for experimentation, the Roster is evaluated. If the platform has a specific experiment subset method, that is invoked to get a subset for experimentation. Otherwise, a random set of containers from all those available in the platform are taken. For each container under experimentation, a random Experiment Method is chosen from all the experiment methods in the container. That method may set a Self Healing method, a Health Check method, and a Finalizable method. Dev Tip Experiment Methods are specifically annotated methods in the Container classes. They are selected with the use of Java Reflections, and do not need to be manually called. Running Experiments Once the experiments have been created, a separate method evaluates them regularly to see if the experiment is complete. It does so using the Health Check method that is assigned as part of the Experiment Method. The Health Check starts evaluating after only 30 seconds, but this value can be configured as part of the specific experiment code. This is to prevent issues where experiments do not immediately result in the container leaving a steady state. After a predetermined amount of time, the experiment enters Self Healing mode. The Self Healing method is called to try and bring the container back into a healthy state. This method is called again regularly, although there is a limited backoff period between invocations. After the container returns to a healthy state, it may be necessary to perform other operations to get the entire application suite back to a healthy state. The finalizable method is called to achieve this. For example, if an application is scaled down to validate it will behave properly with less instances, we will scale it back up to its expected value, even if everything looks healthy at the lower level. Self Healing When an experiment has gone on too long, self healing will be invoked. Self healing tries to undo what has been done to the affected container in order to restore its state. Note that it does not take action on any other containers in the dependency chain that may have been affected by the experiment. Repeated Self Healing If an experiment is still not in a restored state, self healing will regularly retry. There is a minimum duration between retries, ensuring that previous self-healing attempts have had a chance to completely restore state of the container. User Defined Experiments User Defined Experiments create the ability for the user to create a set of Experiments with specific criteria. Mechanism Two API endpoints exist to support User Defined Experiments. The first endpoint provides the mechanism for actually creating a set of experiments given a criteria input. The second endpoint provides the necessary criteria to recreate any earlier experiment. When creating an experiment, containers are targeted using their Aggregation Identifier, as defined by each Platform specification. The Aggregation Identifier should be grouping containers that are operating together to offer the same service, and should be functionally identical to one another. User Defined Experiment JSON Structure The parameter for a User Defined Experiment is a single object with two variables. The platformType variable should be the name of the Platform you wish to experiment on. The experimentCriteria variable requires an object of containerIdentifier, experimentMethods[], and optional specificContainerTargets[], to identify the aggregate container group, the type of experiments to run, and any specific targets which may not be identical (i.e., a MongoDB Primary node). The Experiment Structure object can be sent as the data stream of a POST request to /experiment/build to create the experiments. The server expects a Content-Type of application/json. See the REST API for more information. Example JSON Structure { \"platformType\" : \"KubernetesPlatform\" , \"experimentCriteria\" : [ { \"containerIdentifier\" : \"ngnix\" , \"experimentMethods\" : [ \"terminateProcess.sh\" ], \"specificContainerTargets\" : [] }, { \"containerIdentifier\" : \"mongo\" , \"experimentMethods\" : [ \"cpuBurn.sh\" , \"starveRandomNumberGenerator.sh\" ], \"specificContainerTargets\" : [ \"mongo-qw93ut\" ] } ] } cURL Equivalent curl -X POST \\ http://localhost:8080/experiment/build \\ -H 'Accept: application/json;charset=utf-8' \\ -H 'Content-Type: application/json' \\ -d '{ \"platformType\": \"KubernetesPlatform\", \"experimentCriteria\": [ { \"containerIdentifier\": \"ngnix\", \"experimentMethods\": [ \"terminateProcess.sh\" ], \"specificContainerTargets\": [] }, { \"containerIdentifier\": \"mongo\", \"experimentMethods\": [ \"cpuBurn.sh\", \"starveRandomNumberGenerator.sh\" ], \"specificContainerTargets\": [ \"mongo-qw93ut\" ] } ] }' Potential Error Responses An HTTP 5xx error may be returned if the request is malformed, or if the experiments cannot be created for any number of reasons. A more specific error code will be returned as part of the response body. Experiment History Endpoint The Experiment History endpoint of /experiment/history , when accessed with a GET request, will return a map of Timestamps to Defined Experiment Structures. The data from this map can be extracted and used on the build endpoint to easily recreate experiments. Request curl -X GET \\ http://localhost:8080/experiment/history \\ -H 'Accept: application/json;charset=utf-8' Response { \"2019-08-29T12:55:31.139495Z\" : { \"platformType\" : \"KubernetesPlatform\" , \"experimentCriteria\" : [ { \"containerIdentifier\" : \"ngnix\" , \"experimentMethods\" : [ \"terminateProcess.sh\" ], \"specificContainerTargets\" : [] }, { \"containerIdentifier\" : \"application-67d549b976\" , \"experimentMethods\" : [ \"cpuBurn.sh\" ], \"specificContainerTargets\" : [] } ] } } Specific Container Targets By specifying specific instances in the specificContainerTargets[] field, it is possible to direct specific experiments to occur on specific instances of a group that are not necessarily equal. This is useful for any applications which may have a mechanism for electing a primary node, such as MongoDB. When crafting an experiment for these applications, it you need to specify a specific experiment occur on a specific instance, construct the array of specificContainerTargets with the unique name of the target, and put the experimentMethod for that container in the same position in its array. Any experimentMethods which are not associated with a specificContainerTargets are then randomly assigned to any leftover targets. Example { \"containerIdentifier\" : \"mongo\" , \"experimentMethods\" : [ \"cpuBurn.sh\" , \"starveRandomNumberGenerator.sh\" ], \"specificContainerTargets\" : [ \"mongo-qw93ut\" ] } Limitations There is not enough information recorded and stored to validate \"how\" identical two experiments with the same criteria will be. For example, if running an experiment on a Cloud Platform VM that operates as a node in a Kubernetes Cluster, the effect of the experiment may be dependent on what pods are currently being served by that node. For example, an experiment that starves /dev/random may create a visible effect if a specific pod heavily uses random number generator being located on that node at that time. If that pod is absent, or not being used for that function at that time, then the impact on the service as a whole will be different. In addition, autoscaling may have changed the number of containers in a platform at different times. For example, a service under heavy load at specific times may have 20 Kubernetes Pods for a given microservice at a given time. At lower peak times, it may be down to only 5. User Defined Experiments automatically ensure that at least one container per aggregation identifier will not be experimented upon. Finally, while the Specific Container Targets helps provide a way to ensure that experiments occur on specific containers that are less equal than others, the engine makes no attempt to hold a history of which specific containers it randomly generated an experiment on, as the specific containers are unlikely to exist after experiment ended. There is also no system by which the engine distinguishes how specific application instances are different from others. In order to make use of Specific Container Targets, the engineers operating Chaos Engine will need to discover on their own that a specific instance is different and needs to be specifically handled.","title":"Experiment Manager"},{"location":"Core_Modules/experiment_manager/#experiment-manager","text":"The experiment manager automatically chooses a platform to perform an experiment on, select some containers from that platform, and select experiments to run on those containers. It will regularly poll the platforms of these containers, checking if the issue has healed itself, and will attempt to self heal the experiment after a minimum time.","title":"Experiment Manager"},{"location":"Core_Modules/experiment_manager/#experiment-life-cycle","text":"","title":"Experiment Life Cycle"},{"location":"Core_Modules/experiment_manager/#creating-experiments","text":"A scheduled method runs every 15 seconds and evaluates platforms for any eligible to run an experiment on, based on the Scheduler. If no platforms are eligible for experimentation, no operation is done. If multiple platforms are eligible for experimentation, one is chosen randomly. If there is a platform eligible for experimentation, the Roster is evaluated. If the platform has a specific experiment subset method, that is invoked to get a subset for experimentation. Otherwise, a random set of containers from all those available in the platform are taken. For each container under experimentation, a random Experiment Method is chosen from all the experiment methods in the container. That method may set a Self Healing method, a Health Check method, and a Finalizable method. Dev Tip Experiment Methods are specifically annotated methods in the Container classes. They are selected with the use of Java Reflections, and do not need to be manually called.","title":"Creating Experiments"},{"location":"Core_Modules/experiment_manager/#running-experiments","text":"Once the experiments have been created, a separate method evaluates them regularly to see if the experiment is complete. It does so using the Health Check method that is assigned as part of the Experiment Method. The Health Check starts evaluating after only 30 seconds, but this value can be configured as part of the specific experiment code. This is to prevent issues where experiments do not immediately result in the container leaving a steady state. After a predetermined amount of time, the experiment enters Self Healing mode. The Self Healing method is called to try and bring the container back into a healthy state. This method is called again regularly, although there is a limited backoff period between invocations. After the container returns to a healthy state, it may be necessary to perform other operations to get the entire application suite back to a healthy state. The finalizable method is called to achieve this. For example, if an application is scaled down to validate it will behave properly with less instances, we will scale it back up to its expected value, even if everything looks healthy at the lower level.","title":"Running Experiments"},{"location":"Core_Modules/experiment_manager/#self-healing","text":"When an experiment has gone on too long, self healing will be invoked. Self healing tries to undo what has been done to the affected container in order to restore its state. Note that it does not take action on any other containers in the dependency chain that may have been affected by the experiment.","title":"Self Healing"},{"location":"Core_Modules/experiment_manager/#repeated-self-healing","text":"If an experiment is still not in a restored state, self healing will regularly retry. There is a minimum duration between retries, ensuring that previous self-healing attempts have had a chance to completely restore state of the container.","title":"Repeated Self Healing"},{"location":"Core_Modules/experiment_manager/#user-defined-experiments","text":"User Defined Experiments create the ability for the user to create a set of Experiments with specific criteria.","title":"User Defined Experiments"},{"location":"Core_Modules/experiment_manager/#mechanism","text":"Two API endpoints exist to support User Defined Experiments. The first endpoint provides the mechanism for actually creating a set of experiments given a criteria input. The second endpoint provides the necessary criteria to recreate any earlier experiment. When creating an experiment, containers are targeted using their Aggregation Identifier, as defined by each Platform specification. The Aggregation Identifier should be grouping containers that are operating together to offer the same service, and should be functionally identical to one another.","title":"Mechanism"},{"location":"Core_Modules/experiment_manager/#user-defined-experiment-json-structure","text":"The parameter for a User Defined Experiment is a single object with two variables. The platformType variable should be the name of the Platform you wish to experiment on. The experimentCriteria variable requires an object of containerIdentifier, experimentMethods[], and optional specificContainerTargets[], to identify the aggregate container group, the type of experiments to run, and any specific targets which may not be identical (i.e., a MongoDB Primary node). The Experiment Structure object can be sent as the data stream of a POST request to /experiment/build to create the experiments. The server expects a Content-Type of application/json. See the REST API for more information. Example JSON Structure { \"platformType\" : \"KubernetesPlatform\" , \"experimentCriteria\" : [ { \"containerIdentifier\" : \"ngnix\" , \"experimentMethods\" : [ \"terminateProcess.sh\" ], \"specificContainerTargets\" : [] }, { \"containerIdentifier\" : \"mongo\" , \"experimentMethods\" : [ \"cpuBurn.sh\" , \"starveRandomNumberGenerator.sh\" ], \"specificContainerTargets\" : [ \"mongo-qw93ut\" ] } ] } cURL Equivalent curl -X POST \\ http://localhost:8080/experiment/build \\ -H 'Accept: application/json;charset=utf-8' \\ -H 'Content-Type: application/json' \\ -d '{ \"platformType\": \"KubernetesPlatform\", \"experimentCriteria\": [ { \"containerIdentifier\": \"ngnix\", \"experimentMethods\": [ \"terminateProcess.sh\" ], \"specificContainerTargets\": [] }, { \"containerIdentifier\": \"mongo\", \"experimentMethods\": [ \"cpuBurn.sh\", \"starveRandomNumberGenerator.sh\" ], \"specificContainerTargets\": [ \"mongo-qw93ut\" ] } ] }'","title":"User Defined Experiment JSON Structure"},{"location":"Core_Modules/experiment_manager/#potential-error-responses","text":"An HTTP 5xx error may be returned if the request is malformed, or if the experiments cannot be created for any number of reasons. A more specific error code will be returned as part of the response body.","title":"Potential Error Responses"},{"location":"Core_Modules/experiment_manager/#experiment-history-endpoint","text":"The Experiment History endpoint of /experiment/history , when accessed with a GET request, will return a map of Timestamps to Defined Experiment Structures. The data from this map can be extracted and used on the build endpoint to easily recreate experiments. Request curl -X GET \\ http://localhost:8080/experiment/history \\ -H 'Accept: application/json;charset=utf-8' Response { \"2019-08-29T12:55:31.139495Z\" : { \"platformType\" : \"KubernetesPlatform\" , \"experimentCriteria\" : [ { \"containerIdentifier\" : \"ngnix\" , \"experimentMethods\" : [ \"terminateProcess.sh\" ], \"specificContainerTargets\" : [] }, { \"containerIdentifier\" : \"application-67d549b976\" , \"experimentMethods\" : [ \"cpuBurn.sh\" ], \"specificContainerTargets\" : [] } ] } }","title":"Experiment History Endpoint"},{"location":"Core_Modules/experiment_manager/#specific-container-targets","text":"By specifying specific instances in the specificContainerTargets[] field, it is possible to direct specific experiments to occur on specific instances of a group that are not necessarily equal. This is useful for any applications which may have a mechanism for electing a primary node, such as MongoDB. When crafting an experiment for these applications, it you need to specify a specific experiment occur on a specific instance, construct the array of specificContainerTargets with the unique name of the target, and put the experimentMethod for that container in the same position in its array. Any experimentMethods which are not associated with a specificContainerTargets are then randomly assigned to any leftover targets. Example { \"containerIdentifier\" : \"mongo\" , \"experimentMethods\" : [ \"cpuBurn.sh\" , \"starveRandomNumberGenerator.sh\" ], \"specificContainerTargets\" : [ \"mongo-qw93ut\" ] }","title":"Specific Container Targets"},{"location":"Core_Modules/experiment_manager/#limitations","text":"There is not enough information recorded and stored to validate \"how\" identical two experiments with the same criteria will be. For example, if running an experiment on a Cloud Platform VM that operates as a node in a Kubernetes Cluster, the effect of the experiment may be dependent on what pods are currently being served by that node. For example, an experiment that starves /dev/random may create a visible effect if a specific pod heavily uses random number generator being located on that node at that time. If that pod is absent, or not being used for that function at that time, then the impact on the service as a whole will be different. In addition, autoscaling may have changed the number of containers in a platform at different times. For example, a service under heavy load at specific times may have 20 Kubernetes Pods for a given microservice at a given time. At lower peak times, it may be down to only 5. User Defined Experiments automatically ensure that at least one container per aggregation identifier will not be experimented upon. Finally, while the Specific Container Targets helps provide a way to ensure that experiments occur on specific containers that are less equal than others, the engine makes no attempt to hold a history of which specific containers it randomly generated an experiment on, as the specific containers are unlikely to exist after experiment ended. There is also no system by which the engine distinguishes how specific application instances are different from others. In order to make use of Specific Container Targets, the engineers operating Chaos Engine will need to discover on their own that a specific instance is different and needs to be specifically handled.","title":"Limitations"},{"location":"Core_Modules/scheduler/","text":"Scheduler The Chaos Engine uses an advanced scheduler to ensure sufficiently random time between failures, while simultaneously ensuring that experiments only occur while there are resources available to fix any issues that are not automatically resolved. Configuration Key Names Description Default Notes holidays Configures which country's Holidays and Working Hours to follow. Available: CAN, CZE, FRA, USA CAN ISO-3166-Alpha-3 country codes are used automatedMode Configures if experiments should be run automatically ( true ), or only by API ( false ) true Behaviour The Chaos Engine should only run experiments when the appropriate resources are available to remedy any Chaos Incidents that may be found in the process of running Chaos experiments, for which the Engine is not automatically able to resolve. The country selection implements default working hours for which the Engine will schedule experiments on Monday to Friday. The chosen country will also calculate observed holidays. It is assumed that most or all employees will be unavailable to resolve Chaos Incidents on these days in a timely manner, so the Chaos Engine will not trigger automatic experiments on these days. This does not prevent starting explicit experiments using the Chaos Engine API. In addition to the specific observed holidays, the Chaos Engine will also isolate any single abandoned days. If, for example, a holiday occurs on a Tuesday, it is assumed that the accompanying Monday will have a higher than normal amount of employees on vacation to take advantage of the long weekend. The Monday will also be flagged as an observed holiday. The same is true for Friday given an observed holiday on Thursday. Finally, December 24th through January 1st are considered holidays due to the large amount of vacation taken during that periods. Random Experiment Timing Each Experiment Module has its own independent configuration for AverageMillisPerExperiment . This value controls how the Scheduler calculates when the next experiment will occur. An algorithm uses a random number with a Gaussian distribution to seed the percentile of time between experiments. This percentile is then fed through a function that, given infinite iterations, will tend to average with the configured value above. Because the distribution is only bounded on the lower end, it is possible for long times between experiments (up to ~16 x the configured average). As a result, the median time between experiments will be lower than the average time between experiments. The time between experiments only counts time within office hours. When the scheduler reaches the end of the work day, it stops counting until the beginning of the next day. Disabling Automated Mode By default, Chaos Engine generates automated tests. To disable this behaviour, you can use the automatedMode configuration flag. In addition, you can use a REST API to toggle this flag at runtime.","title":"Scheduler"},{"location":"Core_Modules/scheduler/#scheduler","text":"The Chaos Engine uses an advanced scheduler to ensure sufficiently random time between failures, while simultaneously ensuring that experiments only occur while there are resources available to fix any issues that are not automatically resolved.","title":"Scheduler"},{"location":"Core_Modules/scheduler/#configuration","text":"Key Names Description Default Notes holidays Configures which country's Holidays and Working Hours to follow. Available: CAN, CZE, FRA, USA CAN ISO-3166-Alpha-3 country codes are used automatedMode Configures if experiments should be run automatically ( true ), or only by API ( false ) true","title":"Configuration"},{"location":"Core_Modules/scheduler/#behaviour","text":"The Chaos Engine should only run experiments when the appropriate resources are available to remedy any Chaos Incidents that may be found in the process of running Chaos experiments, for which the Engine is not automatically able to resolve. The country selection implements default working hours for which the Engine will schedule experiments on Monday to Friday. The chosen country will also calculate observed holidays. It is assumed that most or all employees will be unavailable to resolve Chaos Incidents on these days in a timely manner, so the Chaos Engine will not trigger automatic experiments on these days. This does not prevent starting explicit experiments using the Chaos Engine API. In addition to the specific observed holidays, the Chaos Engine will also isolate any single abandoned days. If, for example, a holiday occurs on a Tuesday, it is assumed that the accompanying Monday will have a higher than normal amount of employees on vacation to take advantage of the long weekend. The Monday will also be flagged as an observed holiday. The same is true for Friday given an observed holiday on Thursday. Finally, December 24th through January 1st are considered holidays due to the large amount of vacation taken during that periods.","title":"Behaviour"},{"location":"Core_Modules/scheduler/#random-experiment-timing","text":"Each Experiment Module has its own independent configuration for AverageMillisPerExperiment . This value controls how the Scheduler calculates when the next experiment will occur. An algorithm uses a random number with a Gaussian distribution to seed the percentile of time between experiments. This percentile is then fed through a function that, given infinite iterations, will tend to average with the configured value above. Because the distribution is only bounded on the lower end, it is possible for long times between experiments (up to ~16 x the configured average). As a result, the median time between experiments will be lower than the average time between experiments. The time between experiments only counts time within office hours. When the scheduler reaches the end of the work day, it stops counting until the beginning of the next day.","title":"Random Experiment Timing"},{"location":"Core_Modules/scheduler/#disabling-automated-mode","text":"By default, Chaos Engine generates automated tests. To disable this behaviour, you can use the automatedMode configuration flag. In addition, you can use a REST API to toggle this flag at runtime.","title":"Disabling Automated Mode"},{"location":"Experiment_Modules/","text":"Experiment Modules Each Experiment Module is responsible for interacting with the API endpoint of its appropriate Cloud Platform or Orchestration tool. The Experiment Modules discover nodes that can be experimented upon, ensuring experiments keep a minimum blast radius, and performing the API calls necessary to create an experiment. Common Configuration Some properties are common to all Experiment Modules. These are all configured with the same prefix as the main experiment component variables (i.e., aws.ec2.averageMillisPerExperiment ). Key Name Description Default ${prefix}.averageMillisPerExperiment The average number of Milliseconds between experiments. 14400000 (4 Hours) Available prefixes Prefix Description aws.ec2 AWS EC2 experiments aws.rds AWS RDS experiments cf Cloud Foundry experiments kubernetes Kubernetes experiments","title":"Experiment Modules"},{"location":"Experiment_Modules/#experiment-modules","text":"Each Experiment Module is responsible for interacting with the API endpoint of its appropriate Cloud Platform or Orchestration tool. The Experiment Modules discover nodes that can be experimented upon, ensuring experiments keep a minimum blast radius, and performing the API calls necessary to create an experiment.","title":"Experiment Modules"},{"location":"Experiment_Modules/#common-configuration","text":"Some properties are common to all Experiment Modules. These are all configured with the same prefix as the main experiment component variables (i.e., aws.ec2.averageMillisPerExperiment ). Key Name Description Default ${prefix}.averageMillisPerExperiment The average number of Milliseconds between experiments. 14400000 (4 Hours)","title":"Common Configuration"},{"location":"Experiment_Modules/#available-prefixes","text":"Prefix Description aws.ec2 AWS EC2 experiments aws.rds AWS RDS experiments cf Cloud Foundry experiments kubernetes Kubernetes experiments","title":"Available prefixes"},{"location":"Experiment_Modules/aws_ec2_experiments/","text":"AWS EC2 Experiments SDK The AWS Module makes use of the Official AWS SDK for Java from Amazon. Version The current SDK Version in use is 1.11.357. Configuration Key Name Description Default aws.ec2 The presence of this key enables the EC2 Module N/A aws.accessKeyId The Access Key ID for an AWS API Key None aws.secretAccessKey The Access Key Secret for the given Access Key ID None aws.region The AWS Region to run experiments on. us-east-2 aws.ec2.groupingTags A comma separated list of tag names to use as a shared resource identifier. See Autoscaling Support for more information. None aws.ec2.filter.<key> Each key/value pair will be used to restrict the scope of Experiment targets. See Filtering for more information. None aws.ec2.sshprivatekeys.<key> Each key/value pair will be used to associate an EC2 SSH Key and confirm SSH access. See SSH Based Experiments for more information. None aws.ec2.imageIdToUsernameMap.<image-id> Associate EC2 AMI Image IDs to the username used for SSH Access ec2-user aws.ec2.routableCidrBlocks A comma separated list of CIDR Blocks that should be considered Routable for SSH Access. None Note The aws.accessKeyId , aws.secretAccessKey , and aws.region keys are shared with other AWS Modules. Node Discovery The AWS EC2 Platform automatically discovers EC2 containers that the provided keys are able to access. If any filters are configured, those filter values are set. It will also filter out it's own instance if found to be running in EC2 (see Self Awareness ). Filtering Each key/value pair under the aws.ec2.filter.<key> map will be used for filtering AWS EC2 Instances to find the scope of Chaos. The specific criteria are the same as the AWS CLI . However, since environment variables cannot contain dashes (-), functionality was added to parse camelCaps into dash-separated-words . For example, to use the filter criteria root-device-name , set the property aws.ec2.filter .rootDeviceName . Tags To filter on a tag, use the subkey tag.tag_name . For example, to filter on instances where the tag service_name is mongo , use the property aws.ec2.filter .tag.service_name=mongo . The same camelCaps conversion does not apply. Self Awareness The Chaos Engine will detect if it is running in an EC2 instance, and will avoid itself as a potential target. The mechanism calls the magic AWS EC2 URI http://169.254.169.254/latest/meta-data/instance-id . If running in EC2, this will return the Instance ID of the machine. If not running in EC2, this IP Address should not respond, as it is an RFC 3927 address. Autoscaling Support The EC2 Module makes use of AWS Autoscaling Groups in three capacities. * As a mechanism to ensure that not all identical services are experimented on simultaneously * As a mechanism of determining if an EC2 instance has returned to a full healthy state, including potential replacement containers * As a mechanism to initiate self healing on an EC2 Instance. Discovery Mechanism The property aws.ec2.groupingTags is used to identify which tags should be used for considering grouped instances. By default this is just aws:autoscaling:groupName , but if you are using other technologies that operate in a similar mechanism, you can provide a full comma separated list of all tags in the order of preferred use. Experiment Roster Generation At the beginning of every EC2 based experiment set, for each group, if at least 2 instances exist in that group, one will be flagged as a designated survivor. That instance will be immune from direct experimentation during this time. If a scaling group tag is not found for an instance, or if only one instance exists for a scaling group, then there is no guaranteed protection from experiments. In a real world scenario, these individual instances are just as likely to fail as any single redundant instance, and so are considered eligible for experimentation. Health Check Mechanism Autoscaled EC2 instances also handle their experiment results differently. An EC2 instance as part of an autoscaling group can also complete any experiment by entering a Terminated state. This state indicates that the mechanism for managing these instances has recognized an issue and is recreating the instance. If the instance uses AWS Autoscaling as described above, the system also verifies that the autoscaling group has a number of healthy instances equal to its number of desired instances. The experiment is not considered finished until the autoscaling group is back until desired capacity. API: https://docs.aws.amazon.com/autoscaling/ec2/APIReference/API_DescribeAutoScalingGroups.html Self Healing Mechanism EC2 instances managed by AWS Autoscaling Groups will use an alternative method for their FIRST execution of self healing. Instead of using whatever self healing mechanism there is, the engine will use the AWS Autoscaling Group API and mark the instance as Unhealthy. The Autoscaling Group should pick this up and recreate the instance. API: API SetInstanceHealth SSH Experiment Support EC2 Instances will be considered for SSH Based Experiments based on two criteria. The first criteria is that the property aws.ec2.sshprivatekeys. must be set, where keyname is the name of the SSH Keypair in AWS of the instance. That value should contain the SSH Private Key used for the connection. The second criteria is that the EC2 instance must be routable. This is handled by checking if either the instance has a Public IP Address, or if its private IP Address is contained in a CIDR Block configured via the aws.ec2.routableCidrBlocks variable. When connecting, a routable private IP Address is preferred over a public IP Address. Connections are made using the username in the configurable map of aws.ec2.imageIdToUsernameMap , where each expected subkey is an ImageID, and its value is a username for that AMI. Any AMI that is not present in the map will use the username ec2-user . See SSH Based Experiments for details on what experiments may be run. Experiment Methods Instance Stopped Stopping an EC2 instance tests both the ability of the PaaS solution to move instances over to other hosts, and also tests that the PaaS solution has not overloaded a critical amount of a microservice instances into one host. Mechanism API: API StopInstances The StopInstances API is called with Force = True. This overrides the setting to prevent accidental stops of EC2 instances. Health Check API: API DescribeInstances The DescribeInstances API is called and parsed for the InstanceState of the specific InstanceID. A healthy instance is in a running state (Code 16). Self Healing API: API StartInstances The StartInstances API is called when the experiment has reached it's conclusion. This should result in the container being in a running state, allowing the Health Check to pass. Instance Rebooted Rebooting an EC2 instance tests that the PaaS solution effectively moves instances over to other hosts, but also tests that the PaaS solution moves the instances back after the reboot has finalized, allowing it to take benefit of all available resources. Mechanism API: API RebootInstances The RebootInstances API is called. This API causes a Ctrl+Alt+Del to be triggered in the instance. Ctrl+Alt+Del vs Hard Reboot Some AMI's have Ctrl+Alt+Del disabled. In these cases, a Hard Reboot is initiated by AWS 4 minutes after the Reboot Request was made. Health Check API: API DescribeInstances An internal timer is started when the Reboot command is run. After 4 minutes, the DescribeInstances API is used to verify that the instance is running. This validates that, either a graceful reboot occurred under 4 minutes, or after 4 minutes, if a hard reboot was performed, the instance came back in a running state. Self Healing API: API StartInstances The StartInstances API is called when the experiment has reached it's conclusion. This should result in the container being in a running state, allowing the Health Check to pass. Security Groups Changed Changing the security groups of an EC2 instance will remove it from network accessibility. This tests that the PaaS system is able to properly balance work on nodes that are still connected to the network, and that the disconnected nodes do not cause consistency errors when they are able to reconnect. Mechanism API: API DescribeInstanceAttribute , API ModifyInstanceAttribute The original security groups for an instance are recorded using the DescribeInstanceAttribute API. The security group is then replaced with an automatically created Chaos Security Group using the ModifyInstanceAttribute API. Health Check API: API DescribeInstanceAttribute The security groups returned in the DescribeInstanceAttribute API are compared to the list preserved during the experiment initiation. If the two lists are exclusively equal, the system is deemed healthy. Self Healing API: API ModifyInstanceAttribute The original security groups are put back in place using the ModifyInstanceAttribute API. Automatic Security Group Creation Mechanism API: API DescribeSecurityGroups , API CreateSecurityGroup , API DescribeVpcs The Default VPC is looked up using the DescribeVpcs API, and cached. The Security Groups are looked up using the DescribeSecurityGroup API, and parsed for a security group named ChaosEngine Security Group. If one is found, it is cached and used. If it is not found, it is created using the CreateSecurityGroup API, and cached for use. Terminate Instance in Auto Scaling Group Terminating an instance in an Auto Scaling Group tests the overall resolution time of an error, ensuring the overall customer experience is not impacted. Additionally, it verifies if the product returns back to it's steady state when the requested amount of instances is restored. Mechanism API: API TerminateInstances The TerminateInstances API is called with the InstanceID under test as parameter. Health Check API: API DescribeAutoScalingGroups The health check proves if the actual capacity is back to the desired capacity inside the given Auto Scaling Group. Self Healing As a terminated instance cannot be restored, no self healing mechanism has been implemented. Therefore, an outcome of this experiment can be that the steady state cannot be restored","title":"AWS EC2 Experiments"},{"location":"Experiment_Modules/aws_ec2_experiments/#aws-ec2-experiments","text":"","title":"AWS EC2 Experiments"},{"location":"Experiment_Modules/aws_ec2_experiments/#sdk","text":"The AWS Module makes use of the Official AWS SDK for Java from Amazon.","title":"SDK"},{"location":"Experiment_Modules/aws_ec2_experiments/#version","text":"The current SDK Version in use is 1.11.357.","title":"Version"},{"location":"Experiment_Modules/aws_ec2_experiments/#configuration","text":"Key Name Description Default aws.ec2 The presence of this key enables the EC2 Module N/A aws.accessKeyId The Access Key ID for an AWS API Key None aws.secretAccessKey The Access Key Secret for the given Access Key ID None aws.region The AWS Region to run experiments on. us-east-2 aws.ec2.groupingTags A comma separated list of tag names to use as a shared resource identifier. See Autoscaling Support for more information. None aws.ec2.filter.<key> Each key/value pair will be used to restrict the scope of Experiment targets. See Filtering for more information. None aws.ec2.sshprivatekeys.<key> Each key/value pair will be used to associate an EC2 SSH Key and confirm SSH access. See SSH Based Experiments for more information. None aws.ec2.imageIdToUsernameMap.<image-id> Associate EC2 AMI Image IDs to the username used for SSH Access ec2-user aws.ec2.routableCidrBlocks A comma separated list of CIDR Blocks that should be considered Routable for SSH Access. None Note The aws.accessKeyId , aws.secretAccessKey , and aws.region keys are shared with other AWS Modules.","title":"Configuration"},{"location":"Experiment_Modules/aws_ec2_experiments/#node-discovery","text":"The AWS EC2 Platform automatically discovers EC2 containers that the provided keys are able to access. If any filters are configured, those filter values are set. It will also filter out it's own instance if found to be running in EC2 (see Self Awareness ).","title":"Node Discovery"},{"location":"Experiment_Modules/aws_ec2_experiments/#filtering","text":"Each key/value pair under the aws.ec2.filter.<key> map will be used for filtering AWS EC2 Instances to find the scope of Chaos. The specific criteria are the same as the AWS CLI . However, since environment variables cannot contain dashes (-), functionality was added to parse camelCaps into dash-separated-words . For example, to use the filter criteria root-device-name , set the property aws.ec2.filter .rootDeviceName .","title":"Filtering"},{"location":"Experiment_Modules/aws_ec2_experiments/#tags","text":"To filter on a tag, use the subkey tag.tag_name . For example, to filter on instances where the tag service_name is mongo , use the property aws.ec2.filter .tag.service_name=mongo . The same camelCaps conversion does not apply.","title":"Tags"},{"location":"Experiment_Modules/aws_ec2_experiments/#self-awareness","text":"The Chaos Engine will detect if it is running in an EC2 instance, and will avoid itself as a potential target. The mechanism calls the magic AWS EC2 URI http://169.254.169.254/latest/meta-data/instance-id . If running in EC2, this will return the Instance ID of the machine. If not running in EC2, this IP Address should not respond, as it is an RFC 3927 address.","title":"Self Awareness"},{"location":"Experiment_Modules/aws_ec2_experiments/#autoscaling-support","text":"The EC2 Module makes use of AWS Autoscaling Groups in three capacities. * As a mechanism to ensure that not all identical services are experimented on simultaneously * As a mechanism of determining if an EC2 instance has returned to a full healthy state, including potential replacement containers * As a mechanism to initiate self healing on an EC2 Instance.","title":"Autoscaling Support"},{"location":"Experiment_Modules/aws_ec2_experiments/#discovery-mechanism","text":"The property aws.ec2.groupingTags is used to identify which tags should be used for considering grouped instances. By default this is just aws:autoscaling:groupName , but if you are using other technologies that operate in a similar mechanism, you can provide a full comma separated list of all tags in the order of preferred use.","title":"Discovery Mechanism"},{"location":"Experiment_Modules/aws_ec2_experiments/#experiment-roster-generation","text":"At the beginning of every EC2 based experiment set, for each group, if at least 2 instances exist in that group, one will be flagged as a designated survivor. That instance will be immune from direct experimentation during this time. If a scaling group tag is not found for an instance, or if only one instance exists for a scaling group, then there is no guaranteed protection from experiments. In a real world scenario, these individual instances are just as likely to fail as any single redundant instance, and so are considered eligible for experimentation.","title":"Experiment Roster Generation"},{"location":"Experiment_Modules/aws_ec2_experiments/#health-check-mechanism","text":"Autoscaled EC2 instances also handle their experiment results differently. An EC2 instance as part of an autoscaling group can also complete any experiment by entering a Terminated state. This state indicates that the mechanism for managing these instances has recognized an issue and is recreating the instance. If the instance uses AWS Autoscaling as described above, the system also verifies that the autoscaling group has a number of healthy instances equal to its number of desired instances. The experiment is not considered finished until the autoscaling group is back until desired capacity. API: https://docs.aws.amazon.com/autoscaling/ec2/APIReference/API_DescribeAutoScalingGroups.html","title":"Health Check Mechanism"},{"location":"Experiment_Modules/aws_ec2_experiments/#self-healing-mechanism","text":"EC2 instances managed by AWS Autoscaling Groups will use an alternative method for their FIRST execution of self healing. Instead of using whatever self healing mechanism there is, the engine will use the AWS Autoscaling Group API and mark the instance as Unhealthy. The Autoscaling Group should pick this up and recreate the instance. API: API SetInstanceHealth","title":"Self Healing Mechanism"},{"location":"Experiment_Modules/aws_ec2_experiments/#ssh-experiment-support","text":"EC2 Instances will be considered for SSH Based Experiments based on two criteria. The first criteria is that the property aws.ec2.sshprivatekeys. must be set, where keyname is the name of the SSH Keypair in AWS of the instance. That value should contain the SSH Private Key used for the connection. The second criteria is that the EC2 instance must be routable. This is handled by checking if either the instance has a Public IP Address, or if its private IP Address is contained in a CIDR Block configured via the aws.ec2.routableCidrBlocks variable. When connecting, a routable private IP Address is preferred over a public IP Address. Connections are made using the username in the configurable map of aws.ec2.imageIdToUsernameMap , where each expected subkey is an ImageID, and its value is a username for that AMI. Any AMI that is not present in the map will use the username ec2-user . See SSH Based Experiments for details on what experiments may be run.","title":"SSH Experiment Support"},{"location":"Experiment_Modules/aws_ec2_experiments/#experiment-methods","text":"","title":"Experiment Methods"},{"location":"Experiment_Modules/aws_ec2_experiments/#instance-stopped","text":"Stopping an EC2 instance tests both the ability of the PaaS solution to move instances over to other hosts, and also tests that the PaaS solution has not overloaded a critical amount of a microservice instances into one host.","title":"Instance Stopped"},{"location":"Experiment_Modules/aws_ec2_experiments/#mechanism","text":"API: API StopInstances The StopInstances API is called with Force = True. This overrides the setting to prevent accidental stops of EC2 instances.","title":"Mechanism"},{"location":"Experiment_Modules/aws_ec2_experiments/#health-check","text":"API: API DescribeInstances The DescribeInstances API is called and parsed for the InstanceState of the specific InstanceID. A healthy instance is in a running state (Code 16).","title":"Health Check"},{"location":"Experiment_Modules/aws_ec2_experiments/#self-healing","text":"API: API StartInstances The StartInstances API is called when the experiment has reached it's conclusion. This should result in the container being in a running state, allowing the Health Check to pass.","title":"Self Healing"},{"location":"Experiment_Modules/aws_ec2_experiments/#instance-rebooted","text":"Rebooting an EC2 instance tests that the PaaS solution effectively moves instances over to other hosts, but also tests that the PaaS solution moves the instances back after the reboot has finalized, allowing it to take benefit of all available resources.","title":"Instance Rebooted"},{"location":"Experiment_Modules/aws_ec2_experiments/#mechanism_1","text":"API: API RebootInstances The RebootInstances API is called. This API causes a Ctrl+Alt+Del to be triggered in the instance. Ctrl+Alt+Del vs Hard Reboot Some AMI's have Ctrl+Alt+Del disabled. In these cases, a Hard Reboot is initiated by AWS 4 minutes after the Reboot Request was made.","title":"Mechanism"},{"location":"Experiment_Modules/aws_ec2_experiments/#health-check_1","text":"API: API DescribeInstances An internal timer is started when the Reboot command is run. After 4 minutes, the DescribeInstances API is used to verify that the instance is running. This validates that, either a graceful reboot occurred under 4 minutes, or after 4 minutes, if a hard reboot was performed, the instance came back in a running state.","title":"Health Check"},{"location":"Experiment_Modules/aws_ec2_experiments/#self-healing_1","text":"API: API StartInstances The StartInstances API is called when the experiment has reached it's conclusion. This should result in the container being in a running state, allowing the Health Check to pass.","title":"Self Healing"},{"location":"Experiment_Modules/aws_ec2_experiments/#security-groups-changed","text":"Changing the security groups of an EC2 instance will remove it from network accessibility. This tests that the PaaS system is able to properly balance work on nodes that are still connected to the network, and that the disconnected nodes do not cause consistency errors when they are able to reconnect.","title":"Security Groups Changed"},{"location":"Experiment_Modules/aws_ec2_experiments/#mechanism_2","text":"API: API DescribeInstanceAttribute , API ModifyInstanceAttribute The original security groups for an instance are recorded using the DescribeInstanceAttribute API. The security group is then replaced with an automatically created Chaos Security Group using the ModifyInstanceAttribute API.","title":"Mechanism"},{"location":"Experiment_Modules/aws_ec2_experiments/#health-check_2","text":"API: API DescribeInstanceAttribute The security groups returned in the DescribeInstanceAttribute API are compared to the list preserved during the experiment initiation. If the two lists are exclusively equal, the system is deemed healthy.","title":"Health Check"},{"location":"Experiment_Modules/aws_ec2_experiments/#self-healing_2","text":"API: API ModifyInstanceAttribute The original security groups are put back in place using the ModifyInstanceAttribute API.","title":"Self Healing"},{"location":"Experiment_Modules/aws_ec2_experiments/#automatic-security-group-creation-mechanism","text":"API: API DescribeSecurityGroups , API CreateSecurityGroup , API DescribeVpcs The Default VPC is looked up using the DescribeVpcs API, and cached. The Security Groups are looked up using the DescribeSecurityGroup API, and parsed for a security group named ChaosEngine Security Group. If one is found, it is cached and used. If it is not found, it is created using the CreateSecurityGroup API, and cached for use.","title":"Automatic Security Group Creation Mechanism"},{"location":"Experiment_Modules/aws_ec2_experiments/#terminate-instance-in-auto-scaling-group","text":"Terminating an instance in an Auto Scaling Group tests the overall resolution time of an error, ensuring the overall customer experience is not impacted. Additionally, it verifies if the product returns back to it's steady state when the requested amount of instances is restored.","title":"Terminate Instance in Auto Scaling Group"},{"location":"Experiment_Modules/aws_ec2_experiments/#mechanism_3","text":"API: API TerminateInstances The TerminateInstances API is called with the InstanceID under test as parameter.","title":"Mechanism"},{"location":"Experiment_Modules/aws_ec2_experiments/#health-check_3","text":"API: API DescribeAutoScalingGroups The health check proves if the actual capacity is back to the desired capacity inside the given Auto Scaling Group.","title":"Health Check"},{"location":"Experiment_Modules/aws_ec2_experiments/#self-healing_3","text":"As a terminated instance cannot be restored, no self healing mechanism has been implemented. Therefore, an outcome of this experiment can be that the steady state cannot be restored","title":"Self Healing"},{"location":"Experiment_Modules/aws_rds_experiments/","text":"AWS RDS Experiments SDK The AWS Module makes use of the AWS SDK from Amazon. https://aws.amazon.com/sdk-for-java/ Version: 1.11.357 Configuration Several environment variables control how AWS EC2 is instantiated. The presence of Some Variables control whether or not the module is loaded. Key Name Description Default aws.accessKeyId The Access Key ID for an AWS API key. This key is shared with other AWS Modules. None aws.secretAccessKey The Access Key Secret associated to the same AWS API Key. This key is shared with other AWS Modules. None aws.region The AWS Region to run experiments in. Formatted like: eu-central-1 . This key is shared with other AWS Modules. us-east-2 aws.rds The presence of this key controls if this module is loaded. N/A aws.rds.filter.<tag_name> Each key will be used for filtering nodes. See Node Discovery for more information. e.g. aws.rds.filter.ChaosVictim = true N/A Node Discovery The AWS RDS Module discovers two sets of Containers. The first set represents AWS DB Clusters, and the second represents AWS DB Instances not part of the original clusters. Mechanism API: API DescribeDBInstances , API DescribeDBClusters The results of both API calls are parsed into their appropriate objects. DB Instances that are flagged as being a cluster member are discarded. When needed, a property of the Clusters, the DBClusterMembers, are parsed to find instances that belong to that cluster. Filtering API: API ListTagsForResource The sub-properties of aws.rds.filter are used for creating required tag filters for Instances and Clusters. For example, if the environment variable aws.rds.filter.exampleTag was set to true , all instances and clusters that do not contain the tag exampleTag with value true will be excluded. The ListTagsForResource API is used to get the tags of instances and clusters, and is only invoked if there are tags set. Experimentation Roster Because RDS is treated as a SAAS solution, and a certain amount of redundancy is expected to be handled by the platform, we intentionally do not experiment on all replicas of given containers, as it is unlikely for such a scenario to occur without a complete region outage. To keep our experiments fair with what real world scenarios we can reasonably expect, when an experiment is ran against RDS, a single Availability Zone that is in use is selected, and experiments are isolated to that AZ. Cluster Experiments Instance Rebooted Rebooting a number of instances in an RDS Cluster should be completely seemless, as there are still a number of instances available to handle requests. This may cause some delays in processing a larger amount of requests, which may cause issues upstream from the request (i.e., low CPU usage on application servers as they wait for DB response triggering autoscaling). Because of the nature of clusters, the experiment will choose a random number of instances in a cluster to reboot, such that at least 1 will be rebooted, but at most N-1 will be rebooted. Mechanism API: API RebootDBInstance The RebootDBInstance API is called once for every DB Instance to be rebooted in the cluster. Health Check API: API DescribeDBInstances The results of DescribeDBInstances are parsed for all instances in the experiment. If all instance statuses are \"available\", the experiment is finished. Self Healing The system is completely capable of self healing from the AWS Perspective. This experiment only tests stability of systems that rely on the databases, and not the database system itself. Failover Initiated A failover may occur in an RDS Cluster at any time. That failover may be part of a larger system failure. By initiating the failover independently, we can be sure that the failover reconciliation does not cause an issue downstream if and when it occurs naturally. Mechanism API: API FailoverDBCluster The FailoverDBCluster API is called for the DB Cluster Identifier of the container. Health Check API: API DescribeDBInstances The results of DescribeDBInstances are parsed for all instances in the cluster. If all instance statuses are \"available\", the experiment is finished. Self Healing The system is completely capable of self healing from the AWS Perspective. This experiment only tests stability of systems that rely on the databases, and not the database system itself. Backup In Progress Initiating a backup of a database may cause extra latency, database locks, or other odd behaviour. Normally backups should be scheduled to occur during a low usage maintenance period, but there are several scenarios where they may occur during normal operations. This includes manually initiated backups prior to a Change Request being fulfilled, or services that don't actually have periods of complete downtime. Mechanism API: API CreateDBClusterSnapshot The CreateDBClusterSnapshot API is called for the specific DB Cluster. A snapshot identifier is generated using the DB Cluster Identifier, such that it will be named chapsSnapshot-{DBClusterIdentifier}-yyyy-MM-ddtHH-mm-ss[-sss]Z. The name will automatically trim characters from the Instance Identifier to fit the 255 character limit, and will replace all disallowed characters with - , and trim all repeated - 's. In the event that a snapshot is failed to create due to Snapshot Quota being exceeded, the DeleteDBClusterSnapshot and DeleteDBSnapshot API's are called for any old Chaos snapshots that were not automatically cleaned up after their experiment. The experiment will be retried once. Health Check API: API DescribeDBClusters A DescribeDBClusters request is run against the Cluster ID being experimented on. If the state is \"backing-up\", then the experiment is still in progress. Self Healing API: API DeleteDBClusterSnapshot If the experiment takes too long, the DeleteDBClusterSnapshot API is called. Deleting a snapshot in progress causes the backup to be cancelled. Finalization API: API DeleteDBClusterSnapshot Once the experiment is over, the DeleteDBClusterSnapshot API is called to delete the snapshot. Instance Experiments Instance Rebooted Rebooting an instance outside of a cluster tests a random failure of the instance. AWS is expected to bring the resource back up, but we need to ensure that applications dependent on the database are not negatively impacted during the downtime. Mechanism API: API RebootDBInstance The Reboot DB Instance API is called for the individual instance Identifier. Health Check API: API DescribeDBInstances The Describe DB Instance API is called for the individual instance identifier. If the instance status is \"available\", the experiment is finished. Self Healing The system is completely capable of self healing from the AWS Perspective. This experiment only tests stability of systems that rely on the databases, and not the database system itself. Security Groups Changed Changing the Security Groups on a database tests what happens in applications that depend on the database when a network failure occurs while connecting. Unlike the Instance Rebooted test, this does not necessarily have a graceful disconnect from all existing resources. In addition, triggers in the database may still occur, and transactions may be held open. Mechanism API: API DescribeDBInstances , API ModifyDBInstance The DescribeDBInstances API is called to pull an existing list of Security Groups associated to a database instance. After looking up (and creating, if necessary) a security group to move the database into, the change is done using the modifyDBInstance API Call. Health Check API: API DescribeDBInstances The DescribeDBInstances API call is used to get the list of actual Security Groups applied to instances. This list is compared to the list recorded at the start of the experiment, and if they are exactly equal, the experiment is finished. Self Healing API: API ModifyDBInstance The ModifyDBInstance API call is used to revert the Security Groups of the DB Instance back to the original values. Automatic Security Group Creation Mechanism API: API DescribeSecurityGroups , API CreateSecurityGroup , API DescribeVpcs The Chaos Security Group creation requires the AWS EC2 module to load. The Default VPC is looked up using the DescribeVpcs API, and cached. The Security Groups are looked up using the DescribeSecurityGroup API, and parsed for a security group named ChaosEngine Security Group . If one is found, it is cached and used. If it is not found, it is created using the CreateSecurityGroup API, and cached for use. Backup In Progress Initiating a backup of a database may cause extra latency, database locks, or other odd behaviour. Normally backups should be scheduled to occur during a low usage maintenance period, but there are several scenarios where they may occur during normal operations. This includes manually initiated backups prior to a Change Request being fulfilled, or services that don't actually have periods of complete downtime. Mechanism API: API CreateDBSnapshot The CreateDBSnapshot API is called for the specific DB Instance. A snapshot identifier is generated using the DB Instance Identifier, such that it will be named chapsSnapshot-{DBInstanceIdentifier}-yyyy-MM-ddtHH-mm-ss[-sss]Z . The name will automatically trim characters from the Instance Identifier to fit the 255 character limit, and will replace all disallowed characters with - , and trim all repeated - 's to a single one. In the event that a snapshot is failed to create due to Snapshot Quota being exceeded, the DeleteDBClusterSnapshot and DeleteDBSnapshot API's are called for any old Chaos snapshots that were not automatically cleaned up after their experiment. Health Check API: API DescribeDBInstances A DescribeDBInstances request is run against the instance ID being experimented on. If the state is \"backing-up\", then the experiment is still in progress. Self Healing API: API DeleteDBSnapshot If the experiment takes too long, the DeleteDBSnapshot API is called. Deleting a snapshot in progress causes the backup to be cancelled. Finalization API: API DeleteDBSnapshot Once the experiment is over, the DeleteDBSnapshot API is called to delete the snapshot. Health Check When the Health Check endpoint of the Chaos Engine is polled, it will test if the AWS RDS API is returning without errors. It does so by running a DescribeDBInstances API and DescribeDBClusters, and discarding the results. If any exceptions are thrown, the API Status returns as failed, allowing any Health Check mechanisms for the Chaos Engine to run. Snapshot Cleanup API: API DescribeDBSnapshots , API DeleteDBSnapshot , API DescribeDBClusterSnapshots , API DeleteDBClusterSnapshot Snapshots created by Chaos Engine are supposed to be cleaned up as part of their experiments. In the event that an experiment fails to do so, an automated mechanism will try and delete any snapshots that are named with the Chaos Engine's snapshot naming convention (ChaosSnapshot-<Instance Nane>-<Date/Time>). The snapshots are listed using the DescribeDBSnapshots and DescribeDBClusterSnapshots API's, and any that are more than 60 minutes old are deleted using the DeleteDBSnapshot and DeleteDBClusterSnapshot APIs.","title":"AWS RDS Experiments"},{"location":"Experiment_Modules/aws_rds_experiments/#aws-rds-experiments","text":"","title":"AWS RDS Experiments"},{"location":"Experiment_Modules/aws_rds_experiments/#sdk","text":"The AWS Module makes use of the AWS SDK from Amazon. https://aws.amazon.com/sdk-for-java/ Version: 1.11.357","title":"SDK"},{"location":"Experiment_Modules/aws_rds_experiments/#configuration","text":"Several environment variables control how AWS EC2 is instantiated. The presence of Some Variables control whether or not the module is loaded. Key Name Description Default aws.accessKeyId The Access Key ID for an AWS API key. This key is shared with other AWS Modules. None aws.secretAccessKey The Access Key Secret associated to the same AWS API Key. This key is shared with other AWS Modules. None aws.region The AWS Region to run experiments in. Formatted like: eu-central-1 . This key is shared with other AWS Modules. us-east-2 aws.rds The presence of this key controls if this module is loaded. N/A aws.rds.filter.<tag_name> Each key will be used for filtering nodes. See Node Discovery for more information. e.g. aws.rds.filter.ChaosVictim = true N/A","title":"Configuration"},{"location":"Experiment_Modules/aws_rds_experiments/#node-discovery","text":"The AWS RDS Module discovers two sets of Containers. The first set represents AWS DB Clusters, and the second represents AWS DB Instances not part of the original clusters.","title":"Node Discovery"},{"location":"Experiment_Modules/aws_rds_experiments/#mechanism","text":"API: API DescribeDBInstances , API DescribeDBClusters The results of both API calls are parsed into their appropriate objects. DB Instances that are flagged as being a cluster member are discarded. When needed, a property of the Clusters, the DBClusterMembers, are parsed to find instances that belong to that cluster.","title":"Mechanism"},{"location":"Experiment_Modules/aws_rds_experiments/#filtering","text":"API: API ListTagsForResource The sub-properties of aws.rds.filter are used for creating required tag filters for Instances and Clusters. For example, if the environment variable aws.rds.filter.exampleTag was set to true , all instances and clusters that do not contain the tag exampleTag with value true will be excluded. The ListTagsForResource API is used to get the tags of instances and clusters, and is only invoked if there are tags set.","title":"Filtering"},{"location":"Experiment_Modules/aws_rds_experiments/#experimentation-roster","text":"Because RDS is treated as a SAAS solution, and a certain amount of redundancy is expected to be handled by the platform, we intentionally do not experiment on all replicas of given containers, as it is unlikely for such a scenario to occur without a complete region outage. To keep our experiments fair with what real world scenarios we can reasonably expect, when an experiment is ran against RDS, a single Availability Zone that is in use is selected, and experiments are isolated to that AZ.","title":"Experimentation Roster"},{"location":"Experiment_Modules/aws_rds_experiments/#cluster-experiments","text":"","title":"Cluster Experiments"},{"location":"Experiment_Modules/aws_rds_experiments/#instance-rebooted","text":"Rebooting a number of instances in an RDS Cluster should be completely seemless, as there are still a number of instances available to handle requests. This may cause some delays in processing a larger amount of requests, which may cause issues upstream from the request (i.e., low CPU usage on application servers as they wait for DB response triggering autoscaling). Because of the nature of clusters, the experiment will choose a random number of instances in a cluster to reboot, such that at least 1 will be rebooted, but at most N-1 will be rebooted.","title":"Instance Rebooted"},{"location":"Experiment_Modules/aws_rds_experiments/#mechanism_1","text":"API: API RebootDBInstance The RebootDBInstance API is called once for every DB Instance to be rebooted in the cluster.","title":"Mechanism"},{"location":"Experiment_Modules/aws_rds_experiments/#health-check","text":"API: API DescribeDBInstances The results of DescribeDBInstances are parsed for all instances in the experiment. If all instance statuses are \"available\", the experiment is finished.","title":"Health Check"},{"location":"Experiment_Modules/aws_rds_experiments/#self-healing","text":"The system is completely capable of self healing from the AWS Perspective. This experiment only tests stability of systems that rely on the databases, and not the database system itself.","title":"Self Healing"},{"location":"Experiment_Modules/aws_rds_experiments/#failover-initiated","text":"A failover may occur in an RDS Cluster at any time. That failover may be part of a larger system failure. By initiating the failover independently, we can be sure that the failover reconciliation does not cause an issue downstream if and when it occurs naturally.","title":"Failover Initiated"},{"location":"Experiment_Modules/aws_rds_experiments/#mechanism_2","text":"API: API FailoverDBCluster The FailoverDBCluster API is called for the DB Cluster Identifier of the container.","title":"Mechanism"},{"location":"Experiment_Modules/aws_rds_experiments/#health-check_1","text":"API: API DescribeDBInstances The results of DescribeDBInstances are parsed for all instances in the cluster. If all instance statuses are \"available\", the experiment is finished.","title":"Health Check"},{"location":"Experiment_Modules/aws_rds_experiments/#self-healing_1","text":"The system is completely capable of self healing from the AWS Perspective. This experiment only tests stability of systems that rely on the databases, and not the database system itself.","title":"Self Healing"},{"location":"Experiment_Modules/aws_rds_experiments/#backup-in-progress","text":"Initiating a backup of a database may cause extra latency, database locks, or other odd behaviour. Normally backups should be scheduled to occur during a low usage maintenance period, but there are several scenarios where they may occur during normal operations. This includes manually initiated backups prior to a Change Request being fulfilled, or services that don't actually have periods of complete downtime.","title":"Backup In Progress"},{"location":"Experiment_Modules/aws_rds_experiments/#mechanism_3","text":"API: API CreateDBClusterSnapshot The CreateDBClusterSnapshot API is called for the specific DB Cluster. A snapshot identifier is generated using the DB Cluster Identifier, such that it will be named chapsSnapshot-{DBClusterIdentifier}-yyyy-MM-ddtHH-mm-ss[-sss]Z. The name will automatically trim characters from the Instance Identifier to fit the 255 character limit, and will replace all disallowed characters with - , and trim all repeated - 's. In the event that a snapshot is failed to create due to Snapshot Quota being exceeded, the DeleteDBClusterSnapshot and DeleteDBSnapshot API's are called for any old Chaos snapshots that were not automatically cleaned up after their experiment. The experiment will be retried once.","title":"Mechanism"},{"location":"Experiment_Modules/aws_rds_experiments/#health-check_2","text":"API: API DescribeDBClusters A DescribeDBClusters request is run against the Cluster ID being experimented on. If the state is \"backing-up\", then the experiment is still in progress.","title":"Health Check"},{"location":"Experiment_Modules/aws_rds_experiments/#self-healing_2","text":"API: API DeleteDBClusterSnapshot If the experiment takes too long, the DeleteDBClusterSnapshot API is called. Deleting a snapshot in progress causes the backup to be cancelled.","title":"Self Healing"},{"location":"Experiment_Modules/aws_rds_experiments/#finalization","text":"API: API DeleteDBClusterSnapshot Once the experiment is over, the DeleteDBClusterSnapshot API is called to delete the snapshot.","title":"Finalization"},{"location":"Experiment_Modules/aws_rds_experiments/#instance-experiments","text":"","title":"Instance Experiments"},{"location":"Experiment_Modules/aws_rds_experiments/#instance-rebooted_1","text":"Rebooting an instance outside of a cluster tests a random failure of the instance. AWS is expected to bring the resource back up, but we need to ensure that applications dependent on the database are not negatively impacted during the downtime.","title":"Instance Rebooted"},{"location":"Experiment_Modules/aws_rds_experiments/#mechanism_4","text":"API: API RebootDBInstance The Reboot DB Instance API is called for the individual instance Identifier.","title":"Mechanism"},{"location":"Experiment_Modules/aws_rds_experiments/#health-check_3","text":"API: API DescribeDBInstances The Describe DB Instance API is called for the individual instance identifier. If the instance status is \"available\", the experiment is finished.","title":"Health Check"},{"location":"Experiment_Modules/aws_rds_experiments/#self-healing_3","text":"The system is completely capable of self healing from the AWS Perspective. This experiment only tests stability of systems that rely on the databases, and not the database system itself.","title":"Self Healing"},{"location":"Experiment_Modules/aws_rds_experiments/#security-groups-changed","text":"Changing the Security Groups on a database tests what happens in applications that depend on the database when a network failure occurs while connecting. Unlike the Instance Rebooted test, this does not necessarily have a graceful disconnect from all existing resources. In addition, triggers in the database may still occur, and transactions may be held open.","title":"Security Groups Changed"},{"location":"Experiment_Modules/aws_rds_experiments/#mechanism_5","text":"API: API DescribeDBInstances , API ModifyDBInstance The DescribeDBInstances API is called to pull an existing list of Security Groups associated to a database instance. After looking up (and creating, if necessary) a security group to move the database into, the change is done using the modifyDBInstance API Call.","title":"Mechanism"},{"location":"Experiment_Modules/aws_rds_experiments/#health-check_4","text":"API: API DescribeDBInstances The DescribeDBInstances API call is used to get the list of actual Security Groups applied to instances. This list is compared to the list recorded at the start of the experiment, and if they are exactly equal, the experiment is finished.","title":"Health Check"},{"location":"Experiment_Modules/aws_rds_experiments/#self-healing_4","text":"API: API ModifyDBInstance The ModifyDBInstance API call is used to revert the Security Groups of the DB Instance back to the original values.","title":"Self Healing"},{"location":"Experiment_Modules/aws_rds_experiments/#automatic-security-group-creation-mechanism","text":"API: API DescribeSecurityGroups , API CreateSecurityGroup , API DescribeVpcs The Chaos Security Group creation requires the AWS EC2 module to load. The Default VPC is looked up using the DescribeVpcs API, and cached. The Security Groups are looked up using the DescribeSecurityGroup API, and parsed for a security group named ChaosEngine Security Group . If one is found, it is cached and used. If it is not found, it is created using the CreateSecurityGroup API, and cached for use.","title":"Automatic Security Group Creation Mechanism"},{"location":"Experiment_Modules/aws_rds_experiments/#backup-in-progress_1","text":"Initiating a backup of a database may cause extra latency, database locks, or other odd behaviour. Normally backups should be scheduled to occur during a low usage maintenance period, but there are several scenarios where they may occur during normal operations. This includes manually initiated backups prior to a Change Request being fulfilled, or services that don't actually have periods of complete downtime.","title":"Backup In Progress"},{"location":"Experiment_Modules/aws_rds_experiments/#mechanism_6","text":"API: API CreateDBSnapshot The CreateDBSnapshot API is called for the specific DB Instance. A snapshot identifier is generated using the DB Instance Identifier, such that it will be named chapsSnapshot-{DBInstanceIdentifier}-yyyy-MM-ddtHH-mm-ss[-sss]Z . The name will automatically trim characters from the Instance Identifier to fit the 255 character limit, and will replace all disallowed characters with - , and trim all repeated - 's to a single one. In the event that a snapshot is failed to create due to Snapshot Quota being exceeded, the DeleteDBClusterSnapshot and DeleteDBSnapshot API's are called for any old Chaos snapshots that were not automatically cleaned up after their experiment.","title":"Mechanism"},{"location":"Experiment_Modules/aws_rds_experiments/#health-check_5","text":"API: API DescribeDBInstances A DescribeDBInstances request is run against the instance ID being experimented on. If the state is \"backing-up\", then the experiment is still in progress.","title":"Health Check"},{"location":"Experiment_Modules/aws_rds_experiments/#self-healing_5","text":"API: API DeleteDBSnapshot If the experiment takes too long, the DeleteDBSnapshot API is called. Deleting a snapshot in progress causes the backup to be cancelled.","title":"Self Healing"},{"location":"Experiment_Modules/aws_rds_experiments/#finalization_1","text":"API: API DeleteDBSnapshot Once the experiment is over, the DeleteDBSnapshot API is called to delete the snapshot.","title":"Finalization"},{"location":"Experiment_Modules/aws_rds_experiments/#health-check_6","text":"When the Health Check endpoint of the Chaos Engine is polled, it will test if the AWS RDS API is returning without errors. It does so by running a DescribeDBInstances API and DescribeDBClusters, and discarding the results. If any exceptions are thrown, the API Status returns as failed, allowing any Health Check mechanisms for the Chaos Engine to run.","title":"Health Check"},{"location":"Experiment_Modules/aws_rds_experiments/#snapshot-cleanup","text":"API: API DescribeDBSnapshots , API DeleteDBSnapshot , API DescribeDBClusterSnapshots , API DeleteDBClusterSnapshot Snapshots created by Chaos Engine are supposed to be cleaned up as part of their experiments. In the event that an experiment fails to do so, an automated mechanism will try and delete any snapshots that are named with the Chaos Engine's snapshot naming convention (ChaosSnapshot-<Instance Nane>-<Date/Time>). The snapshots are listed using the DescribeDBSnapshots and DescribeDBClusterSnapshots API's, and any that are more than 60 minutes old are deleted using the DeleteDBSnapshot and DeleteDBClusterSnapshot APIs.","title":"Snapshot Cleanup"},{"location":"Experiment_Modules/cattle_pet_distinction/","text":"Cattle/Pet Distinction Description Some experiments may be destructive in nature. These experiments should only ever be applied to Cattle, and not to Pets. Both Containers and Experiments need to be appropriately tagged. Defining Containers as Cattle The base Container class has a method called isCattle() . The default value for this is false. This can be Overriden in any Container class to be evaluated according to the nature of the specific container. For example, an EC2 container may evaluate if it is part of an Autoscaling Group, while a Kubernetes Pod Container may evaluate if it is backed by a Replica Set. Defining Experiments as Cattle-only Experiments are assumed to be Pet-Friendly unless otherwise specified. There are two ways of denoting experiments to be cattle-only. Method-Based Experiments Experiments defined in the Container classes can be annotated with the annotation @CattleExperiment to denote that they are reserved for Cattle. Script-Based Experiments Experiments from Scripts require a Header block that defines them as Cattle. See Chaos Engine Script Header for more information.","title":"Cattle/Pet Distinction"},{"location":"Experiment_Modules/cattle_pet_distinction/#cattlepet-distinction","text":"","title":"Cattle/Pet Distinction"},{"location":"Experiment_Modules/cattle_pet_distinction/#description","text":"Some experiments may be destructive in nature. These experiments should only ever be applied to Cattle, and not to Pets. Both Containers and Experiments need to be appropriately tagged.","title":"Description"},{"location":"Experiment_Modules/cattle_pet_distinction/#defining-containers-as-cattle","text":"The base Container class has a method called isCattle() . The default value for this is false. This can be Overriden in any Container class to be evaluated according to the nature of the specific container. For example, an EC2 container may evaluate if it is part of an Autoscaling Group, while a Kubernetes Pod Container may evaluate if it is backed by a Replica Set.","title":"Defining Containers as Cattle"},{"location":"Experiment_Modules/cattle_pet_distinction/#defining-experiments-as-cattle-only","text":"Experiments are assumed to be Pet-Friendly unless otherwise specified. There are two ways of denoting experiments to be cattle-only.","title":"Defining Experiments as Cattle-only"},{"location":"Experiment_Modules/cattle_pet_distinction/#method-based-experiments","text":"Experiments defined in the Container classes can be annotated with the annotation @CattleExperiment to denote that they are reserved for Cattle.","title":"Method-Based Experiments"},{"location":"Experiment_Modules/cattle_pet_distinction/#script-based-experiments","text":"Experiments from Scripts require a Header block that defines them as Cattle. See Chaos Engine Script Header for more information.","title":"Script-Based Experiments"},{"location":"Experiment_Modules/kubernetes_experiments/","text":"Kubernetes Module The Chaos Engine Kubernetes Module is able to connect to a Kubernetes cluster and interact with deployed PODs. Supported Versions Chaos Engine supports minimum Kubernetes version 1.9 and currently supports up to version 1.15. Support is driven by the Kubernetes Client SDK version compatibility. SDK The official Kubernetes Java Client is used to interact with the cluster. Resource https://github.com/kubernetes-client/java Version 7.0.0 Maven Repositories https://mvnrepository.com/artifact/io.kubernetes/client-java Configuration Environment variables that control how the Chaos Engine interacts with Kubernetes. Key Name Description Default Value kubernetes The presence of this key enables Kubernetes module. N/A kubernetes.url Kubernetes server API url e.g. None kubernetes.token JWT token assigned to service account. You can get the value by running kubectl describe secret name_of_your_secret None kubernetes.namespace K8S namespace where experiments should be performed default kubernetes.debug Enables debug log of Kubernetes java client false kubernetes.validateSSL Enables validation of sever side certificates false Required Kubernetes Cluster Configuration A service account with a role binding needs to be created in order to access the specific API endpoints required for Kubernetes Experiments Please replace the {{namespace}} fillers with the appropriate values and apply to your cluster. You can retrieve the token by running kubectl describe secret chaos-engine -n {{namespace}} chaos-engine-service-account.yaml apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : chaos-engine-role namespace : {{ namespace }} rules : - apiGroups : - apps resources : - daemonsets - daemonsets/status - deployments - deployments/status - replicasets - replicasets/status - statefulsets - statefulsets/status verbs : - get - list - apiGroups : - \"\" resources : - pods verbs : - delete - apiGroups : - \"\" resources : - pods - pods/status - replicationcontrollers/status verbs : - get - list - apiGroups : - \"\" resources : - pods/exec verbs : - create - get --- apiVersion : v1 kind : ServiceAccount metadata : name : chaos-engine-serviceaccount namespace : {{ namespace }} --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : chaos-engine-rolebinding namespace : {{ namespace }} roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : chaos-engine-role subjects : - kind : ServiceAccount name : chaos-engine-serviceaccount namespace : {{ namespace }} Node Discovery Mechanism API: listNamespacedPod The Kubernetes Platform generates list of available containers by calling listNamespacedPod API. Filtering is done according to namespace name provided as a platform configuration parameter. Only one namespace can be targeted right now. Note Job and Cron Job controllers are not supported by the Engine. Containers managed by those controllers are considered unhealthy and they are automatically skipped by scheduler. Self Awareness Not yet implemented Experiments Delete Pod Mechanism The Engine invokes deleteNamespacedPod API call with zero graceful period. That leads to immediate POD termination. In theory all containers should be backed by a controller so the deleted container should be replaced by brand new container instance created by the controller. API: deleteNamespacedPod Health Check Experiment is finished when originally targeted container is no more present on the platform and the controller is back to desired number of replicas. API Check pod: listNamespacedPod API Controllers: readNamespacedReplicationControllerStatus readNamespacedReplicaSetStatus readNamespacedStatefulSetStatus readNamespacedDaemonSetStatus readNamespacedDeploymentStatus Self Healing None Shell based experiments Mechanism Shell base experiments is a suite of shell scripts that are randomly selected and transferred to the targeted Kubernetes container and executed. API: connectGetNamespacedPodExec Health Check Same as Delete Pod experiment, actual replicas count and container existence is checked. Self Healing Target container is deleted with no graceful period. API: listNamespacedPod List of experiments See Included Script Experiments for a list of experiments included with the engine.","title":"Kubernetes Module"},{"location":"Experiment_Modules/kubernetes_experiments/#kubernetes-module","text":"The Chaos Engine Kubernetes Module is able to connect to a Kubernetes cluster and interact with deployed PODs.","title":"Kubernetes Module"},{"location":"Experiment_Modules/kubernetes_experiments/#supported-versions","text":"Chaos Engine supports minimum Kubernetes version 1.9 and currently supports up to version 1.15. Support is driven by the Kubernetes Client SDK version compatibility.","title":"Supported Versions"},{"location":"Experiment_Modules/kubernetes_experiments/#sdk","text":"The official Kubernetes Java Client is used to interact with the cluster. Resource https://github.com/kubernetes-client/java Version 7.0.0 Maven Repositories https://mvnrepository.com/artifact/io.kubernetes/client-java","title":"SDK"},{"location":"Experiment_Modules/kubernetes_experiments/#configuration","text":"Environment variables that control how the Chaos Engine interacts with Kubernetes. Key Name Description Default Value kubernetes The presence of this key enables Kubernetes module. N/A kubernetes.url Kubernetes server API url e.g. None kubernetes.token JWT token assigned to service account. You can get the value by running kubectl describe secret name_of_your_secret None kubernetes.namespace K8S namespace where experiments should be performed default kubernetes.debug Enables debug log of Kubernetes java client false kubernetes.validateSSL Enables validation of sever side certificates false","title":"Configuration"},{"location":"Experiment_Modules/kubernetes_experiments/#required-kubernetes-cluster-configuration","text":"A service account with a role binding needs to be created in order to access the specific API endpoints required for Kubernetes Experiments Please replace the {{namespace}} fillers with the appropriate values and apply to your cluster. You can retrieve the token by running kubectl describe secret chaos-engine -n {{namespace}} chaos-engine-service-account.yaml apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : chaos-engine-role namespace : {{ namespace }} rules : - apiGroups : - apps resources : - daemonsets - daemonsets/status - deployments - deployments/status - replicasets - replicasets/status - statefulsets - statefulsets/status verbs : - get - list - apiGroups : - \"\" resources : - pods verbs : - delete - apiGroups : - \"\" resources : - pods - pods/status - replicationcontrollers/status verbs : - get - list - apiGroups : - \"\" resources : - pods/exec verbs : - create - get --- apiVersion : v1 kind : ServiceAccount metadata : name : chaos-engine-serviceaccount namespace : {{ namespace }} --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : chaos-engine-rolebinding namespace : {{ namespace }} roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : chaos-engine-role subjects : - kind : ServiceAccount name : chaos-engine-serviceaccount namespace : {{ namespace }}","title":"Required Kubernetes Cluster Configuration"},{"location":"Experiment_Modules/kubernetes_experiments/#node-discovery","text":"","title":"Node Discovery"},{"location":"Experiment_Modules/kubernetes_experiments/#mechanism","text":"API: listNamespacedPod The Kubernetes Platform generates list of available containers by calling listNamespacedPod API. Filtering is done according to namespace name provided as a platform configuration parameter. Only one namespace can be targeted right now. Note Job and Cron Job controllers are not supported by the Engine. Containers managed by those controllers are considered unhealthy and they are automatically skipped by scheduler.","title":"Mechanism"},{"location":"Experiment_Modules/kubernetes_experiments/#self-awareness","text":"Not yet implemented","title":"Self Awareness"},{"location":"Experiment_Modules/kubernetes_experiments/#experiments","text":"","title":"Experiments"},{"location":"Experiment_Modules/kubernetes_experiments/#delete-pod","text":"","title":"Delete Pod"},{"location":"Experiment_Modules/kubernetes_experiments/#mechanism_1","text":"The Engine invokes deleteNamespacedPod API call with zero graceful period. That leads to immediate POD termination. In theory all containers should be backed by a controller so the deleted container should be replaced by brand new container instance created by the controller. API: deleteNamespacedPod","title":"Mechanism"},{"location":"Experiment_Modules/kubernetes_experiments/#health-check","text":"Experiment is finished when originally targeted container is no more present on the platform and the controller is back to desired number of replicas. API Check pod: listNamespacedPod API Controllers: readNamespacedReplicationControllerStatus readNamespacedReplicaSetStatus readNamespacedStatefulSetStatus readNamespacedDaemonSetStatus readNamespacedDeploymentStatus","title":"Health Check"},{"location":"Experiment_Modules/kubernetes_experiments/#self-healing","text":"None","title":"Self Healing"},{"location":"Experiment_Modules/kubernetes_experiments/#shell-based-experiments","text":"","title":"Shell based experiments"},{"location":"Experiment_Modules/kubernetes_experiments/#mechanism_2","text":"Shell base experiments is a suite of shell scripts that are randomly selected and transferred to the targeted Kubernetes container and executed. API: connectGetNamespacedPodExec","title":"Mechanism"},{"location":"Experiment_Modules/kubernetes_experiments/#health-check_1","text":"Same as Delete Pod experiment, actual replicas count and container existence is checked.","title":"Health Check"},{"location":"Experiment_Modules/kubernetes_experiments/#self-healing_1","text":"Target container is deleted with no graceful period. API: listNamespacedPod","title":"Self Healing"},{"location":"Experiment_Modules/kubernetes_experiments/#list-of-experiments","text":"See Included Script Experiments for a list of experiments included with the engine.","title":"List of experiments"},{"location":"Experiment_Modules/pcf_experiments/","text":"PCF Experiments The Chaos Engine Cloud Foundry Module is able to connect to a Cloud Foundry organization and interact Applications deployed in Cloud Foundry. It can interact with entire applications or individual application instances. The Cloud Foundry module is split into two platforms that operate independently from one another. One Platform is designed to run experiments that affect entire applications as resources, while the other interacts with individual application instances. SDK The official Pivotal Cloud Foundry Java Client is used to interact with Cloud Foundry. Resource: https://github.com/cloudfoundry/cf-java-client Version: 3.13.0.RELEASE Maven Repositories: https://mvnrepository.com/artifact/org.cloudfoundry/cloudfoundry-operations https://mvnrepository.com/artifact/org.cloudfoundry/cloudfoundry-client-reactor Configuration Several environment variables control how the Chaos Engine creates a connection to the Cloud Foundry manager. Key Name Description Default Value cf.apiHost The FQDN of the API Host to connect to. None cf.port The port used to communicate with the API Host 443 cf.username The username to connect to the API Host with. None cf.password The password for the associated username. None cf.organization The Organization within Cloud Foundry to connect to. None cf.space The Space within the Organization to connect to. default cf.applicationChaos The presence of this key enables the application level experiments. Please note that at least one of cf.applicationChaos or cf.containerChaos must be set N/A cf.containerChaos The presence of this key enables the container level experiments. N/A Node Discovery Node Discovery for Cloud Foundry results in two rosters. The first roster represents every application, while the second represents every instance of every application. This allows experiments to operate at multiple layers in Cloud Foundry. Mechanism API: https://apidocs.cloudfoundry.org/3.5.0/apps/list_all_apps.html The module calls the Applications List API from Cloud Foundry via the Java SDK. Multiple Container objects are created as the result of this call. One set of Containers represents each Application. Another set of Containers is created by iterating for each of the requested number of instances per application, and represents every individual instance. Self Awareness The Chaos Engine is able to determine if it is running in Cloud Foundry. It does so by looking for a pair of environment variables that Cloud Foundry sets in the container where the application runs. These variables are vcap.application.name and CF_INSTANCE_ID . These values are compared to the nodes during discovery, and if there is a match, the node is dropped from the roster. Resource: https://docs.run.pivotal.io/devguide/deploy-apps/environment-variable.html Experiments Container level experiments Application Instance Stopped Stopping a Cloud Foundry instance validates a few things. Any chain of calls that pass through the instance should seamlessly be handled by other available instances. The instance should be able to resume operations and start taking requests once BOSH realizes a container is terminated, and software in the container should be capable of starting up without issue. Mechanism API: https://apidocs.cloudfoundry.org/3.5.0/apps/terminate_the_running_app_instance_at_the_given_index.html The Application Instance Terminate is called through the Java SDK. This will stop the instance of an application. The BOSH manager is supposed to restart instances at this point. Health Check Mechanism API: https://apidocs.cloudfoundry.org/3.5.0/apps/get_the_instance_information_for_a_started_app.html The Application Instance Summary information queried via the Java SDK. The specific InstanceID is filtered from the results, and the State of the container is checked. Self Healing API: https://apidocs.cloudfoundry.org/3.5.0/apps/restage_an_app.html The Application is restaged via the Restage Application API via the Java SDK. SSH Based Experiments Cloud Foundry Containers support SSH Based experiments that can be used to simulate specific failure scenarios like high CPU or MEM consumption. See Script Experiments for more details. Mechanism API: https://apidocs.cloudfoundry.org/3.5.0/info/get_info.html Resource: https://docs.cloudfoundry.org/devguide/deploy-apps/ssh-apps.html A combination of information is used to create an SSH Connection. The Info API provides an SSH Endpoint Address and port. A separate API call provides a one time password to be used for the SSH connection. A connection for the individual instance is created by using the application id and instance id as a username (Formatted as cf: ApplicationID /*InstanceID *. This connection is passed to the SSH Manager to run SSH based experiments. Entire process can be described by following manual steps: Run cf app MY-AWESOME-APP --guid and record the GUID of your target app. $ cf app MY-AWESOME-APP --guid abcdefab-1234-5678-abcd-1234abcd1234 Query the /v2/info endpoint of the Cloud Controller in your deployment. Record the domain name and port of the app_ssh_endpoint field. $ cf curl /v2/info { ... \"app_ssh_endpoint\" : \"ssh.MY-DOMAIN.com:2222\" , \"app_ssh_host_key_fingerprint\" : \"a6:14:c0:ea:42:07:b2:f7:53:2c:0b:60:e0:00:21:6c\" , ... } Run cf ssh-code to obtain a one-time authorization code that substitutes for an SSH password. $ cf ssh-code E1x89n Run your ssh or other command to connect to the app instance. For the username, use a string of the form cf:APP-GUID/APP-INSTANCE-INDEX@SSH-ENDPOINT , where APP-GUID and SSH-ENDPOINT come from the previous steps. For the port number, use the SSH-PORT recorded above. APP-INSTANCE-INDEX is the index of the instance you want to access. With the above example, you ssh into the container hosting the first instance of your app by running the following command: $ ssh -p 2222 cf:abcdefab-1234-5678-abcd-1234abcd1234/0@ssh.MY-DOMAIN.com Health Check Mechanism API: https://apidocs.cloudfoundry.org/3.5.0/apps/get_the_instance_information_for_a_started_app.html SSH base experiments use two phase health check mechanism: 1) The application instance summary information queried via the Java SDK. The specific InstanceID is filtered from the results, and the State of the container is checked. 2) (If defined) When step 1 confirms the container is healthy, ssh connection to the container is made and specific health check command is executed, if expected status code is return the healthy state is returned 3) Container is considered healthy when both previous steps return healthy state Application level experiments Application Scaling Application Scaling tests verify that any autoscaling that is configured does not cause random issues as applications are scaled up or down. For instance, too many instances of one application may cause thread pool exhaustion on a dependent service. Mechanism API: https://apidocs.cloudfoundry.org/3.5.0/apps/updating_an_app.html The Application Update API is called via the Cloud Foundry SDK Applications Scale interface. Containers are appropriately created or destroyed automatically. Health Check Mechanism API: https://apidocs.cloudfoundry.org/3.5.0/apps/list_all_apps.html All Applications are listed and parsed for running state. This ensures that any dependency chains are still healthy. Self Healing & Experiment Finalization API: https://apidocs.cloudfoundry.org/3.5.0/apps/updating_an_app.html The application is rescaled back to its original value through the same API by which the experiment was launched. Application Restart Application Restart tests how resilient the platform is to a short downtime of an entire application. This can happen while an application is being updated with a hotfix, or if a bug in an application causes all instances to simultaneously crash. Restarting your application stops your application and restarts it with the already compiled droplet. A droplet is a tarball that includes: stack buildpack application source code Mechanism API: https://apidocs.cloudfoundry.org/4.2.0/apps/creating_an_app.html A Restart Application request is initiated through the Cloud Foundry SDK. Health Check Mechanism API: https://apidocs.cloudfoundry.org/3.5.0/apps/list_all_apps.html All Applications are listed and parsed for running state. This ensures that any dependency chains are still healthy. Application Restage Application Restage tests how resilient the platform is to a short downtime of an entire application. This can happen while an application is being updated with a hotfix, or if a bug in an application causes all instances to simultaneously crash. Restaging your application stops your application and restages it, by compiling a new droplet and starting it. Mechanism API: https://apidocs.cloudfoundry.org/3.5.0/apps/restage_an_app.html A Restage Application request is initiated through the Cloud Foundry SDK. Health Check Mechanism API: https://apidocs.cloudfoundry.org/3.5.0/apps/list_all_apps.html All Applications are listed and parsed for running state. This ensures that any dependency chains are still healthy. Unmap Application Route Unmap application routes tests how the applications running in Cloud Foundry containers are resilient to short unavailability of resources accessible over network. Routes and Domains concept description Mechanism API: https://apidocs.cloudfoundry.org/3.5.0/routes_mapping/mapping_an_app_and_a_route.html A unmap route request is initiated through the Cloud Foundry SDK. When an application has multiple routes one of them is selected randomly and unmaped. When an application has no routes the experiment is skipped. Health Check Mechanism API: https://apidocs.cloudfoundry.org/3.5.0/apps/list_all_apps.html All Applications are listed and parsed for running state. This ensures that any dependency chains are still healthy. Self Healing & Experiment Finalization API: https://apidocs.cloudfoundry.org/3.5.0/routes_mapping/delete_a_particular_route_mapping.html In the end of the experiment the previously unmaped route is assigned back to the application","title":"PCF Experiments"},{"location":"Experiment_Modules/pcf_experiments/#pcf-experiments","text":"The Chaos Engine Cloud Foundry Module is able to connect to a Cloud Foundry organization and interact Applications deployed in Cloud Foundry. It can interact with entire applications or individual application instances. The Cloud Foundry module is split into two platforms that operate independently from one another. One Platform is designed to run experiments that affect entire applications as resources, while the other interacts with individual application instances.","title":"PCF Experiments"},{"location":"Experiment_Modules/pcf_experiments/#sdk","text":"The official Pivotal Cloud Foundry Java Client is used to interact with Cloud Foundry. Resource: https://github.com/cloudfoundry/cf-java-client Version: 3.13.0.RELEASE Maven Repositories: https://mvnrepository.com/artifact/org.cloudfoundry/cloudfoundry-operations https://mvnrepository.com/artifact/org.cloudfoundry/cloudfoundry-client-reactor","title":"SDK"},{"location":"Experiment_Modules/pcf_experiments/#configuration","text":"Several environment variables control how the Chaos Engine creates a connection to the Cloud Foundry manager. Key Name Description Default Value cf.apiHost The FQDN of the API Host to connect to. None cf.port The port used to communicate with the API Host 443 cf.username The username to connect to the API Host with. None cf.password The password for the associated username. None cf.organization The Organization within Cloud Foundry to connect to. None cf.space The Space within the Organization to connect to. default cf.applicationChaos The presence of this key enables the application level experiments. Please note that at least one of cf.applicationChaos or cf.containerChaos must be set N/A cf.containerChaos The presence of this key enables the container level experiments. N/A","title":"Configuration"},{"location":"Experiment_Modules/pcf_experiments/#node-discovery","text":"Node Discovery for Cloud Foundry results in two rosters. The first roster represents every application, while the second represents every instance of every application. This allows experiments to operate at multiple layers in Cloud Foundry.","title":"Node Discovery"},{"location":"Experiment_Modules/pcf_experiments/#mechanism","text":"API: https://apidocs.cloudfoundry.org/3.5.0/apps/list_all_apps.html The module calls the Applications List API from Cloud Foundry via the Java SDK. Multiple Container objects are created as the result of this call. One set of Containers represents each Application. Another set of Containers is created by iterating for each of the requested number of instances per application, and represents every individual instance.","title":"Mechanism"},{"location":"Experiment_Modules/pcf_experiments/#self-awareness","text":"The Chaos Engine is able to determine if it is running in Cloud Foundry. It does so by looking for a pair of environment variables that Cloud Foundry sets in the container where the application runs. These variables are vcap.application.name and CF_INSTANCE_ID . These values are compared to the nodes during discovery, and if there is a match, the node is dropped from the roster. Resource: https://docs.run.pivotal.io/devguide/deploy-apps/environment-variable.html","title":"Self Awareness"},{"location":"Experiment_Modules/pcf_experiments/#experiments","text":"","title":"Experiments"},{"location":"Experiment_Modules/pcf_experiments/#container-level-experiments","text":"","title":"Container level experiments"},{"location":"Experiment_Modules/pcf_experiments/#application-instance-stopped","text":"Stopping a Cloud Foundry instance validates a few things. Any chain of calls that pass through the instance should seamlessly be handled by other available instances. The instance should be able to resume operations and start taking requests once BOSH realizes a container is terminated, and software in the container should be capable of starting up without issue.","title":"Application Instance Stopped"},{"location":"Experiment_Modules/pcf_experiments/#mechanism_1","text":"API: https://apidocs.cloudfoundry.org/3.5.0/apps/terminate_the_running_app_instance_at_the_given_index.html The Application Instance Terminate is called through the Java SDK. This will stop the instance of an application. The BOSH manager is supposed to restart instances at this point.","title":"Mechanism"},{"location":"Experiment_Modules/pcf_experiments/#health-check-mechanism","text":"API: https://apidocs.cloudfoundry.org/3.5.0/apps/get_the_instance_information_for_a_started_app.html The Application Instance Summary information queried via the Java SDK. The specific InstanceID is filtered from the results, and the State of the container is checked.","title":"Health Check Mechanism"},{"location":"Experiment_Modules/pcf_experiments/#self-healing","text":"API: https://apidocs.cloudfoundry.org/3.5.0/apps/restage_an_app.html The Application is restaged via the Restage Application API via the Java SDK.","title":"Self Healing"},{"location":"Experiment_Modules/pcf_experiments/#ssh-based-experiments","text":"Cloud Foundry Containers support SSH Based experiments that can be used to simulate specific failure scenarios like high CPU or MEM consumption. See Script Experiments for more details.","title":"SSH Based Experiments"},{"location":"Experiment_Modules/pcf_experiments/#mechanism_2","text":"API: https://apidocs.cloudfoundry.org/3.5.0/info/get_info.html Resource: https://docs.cloudfoundry.org/devguide/deploy-apps/ssh-apps.html A combination of information is used to create an SSH Connection. The Info API provides an SSH Endpoint Address and port. A separate API call provides a one time password to be used for the SSH connection. A connection for the individual instance is created by using the application id and instance id as a username (Formatted as cf: ApplicationID /*InstanceID *. This connection is passed to the SSH Manager to run SSH based experiments. Entire process can be described by following manual steps: Run cf app MY-AWESOME-APP --guid and record the GUID of your target app. $ cf app MY-AWESOME-APP --guid abcdefab-1234-5678-abcd-1234abcd1234 Query the /v2/info endpoint of the Cloud Controller in your deployment. Record the domain name and port of the app_ssh_endpoint field. $ cf curl /v2/info { ... \"app_ssh_endpoint\" : \"ssh.MY-DOMAIN.com:2222\" , \"app_ssh_host_key_fingerprint\" : \"a6:14:c0:ea:42:07:b2:f7:53:2c:0b:60:e0:00:21:6c\" , ... } Run cf ssh-code to obtain a one-time authorization code that substitutes for an SSH password. $ cf ssh-code E1x89n Run your ssh or other command to connect to the app instance. For the username, use a string of the form cf:APP-GUID/APP-INSTANCE-INDEX@SSH-ENDPOINT , where APP-GUID and SSH-ENDPOINT come from the previous steps. For the port number, use the SSH-PORT recorded above. APP-INSTANCE-INDEX is the index of the instance you want to access. With the above example, you ssh into the container hosting the first instance of your app by running the following command: $ ssh -p 2222 cf:abcdefab-1234-5678-abcd-1234abcd1234/0@ssh.MY-DOMAIN.com","title":"Mechanism"},{"location":"Experiment_Modules/pcf_experiments/#health-check-mechanism_1","text":"API: https://apidocs.cloudfoundry.org/3.5.0/apps/get_the_instance_information_for_a_started_app.html SSH base experiments use two phase health check mechanism: 1) The application instance summary information queried via the Java SDK. The specific InstanceID is filtered from the results, and the State of the container is checked. 2) (If defined) When step 1 confirms the container is healthy, ssh connection to the container is made and specific health check command is executed, if expected status code is return the healthy state is returned 3) Container is considered healthy when both previous steps return healthy state","title":"Health Check Mechanism"},{"location":"Experiment_Modules/pcf_experiments/#application-level-experiments","text":"","title":"Application level experiments"},{"location":"Experiment_Modules/pcf_experiments/#application-scaling","text":"Application Scaling tests verify that any autoscaling that is configured does not cause random issues as applications are scaled up or down. For instance, too many instances of one application may cause thread pool exhaustion on a dependent service.","title":"Application Scaling"},{"location":"Experiment_Modules/pcf_experiments/#mechanism_3","text":"API: https://apidocs.cloudfoundry.org/3.5.0/apps/updating_an_app.html The Application Update API is called via the Cloud Foundry SDK Applications Scale interface. Containers are appropriately created or destroyed automatically.","title":"Mechanism"},{"location":"Experiment_Modules/pcf_experiments/#health-check-mechanism_2","text":"API: https://apidocs.cloudfoundry.org/3.5.0/apps/list_all_apps.html All Applications are listed and parsed for running state. This ensures that any dependency chains are still healthy.","title":"Health Check Mechanism"},{"location":"Experiment_Modules/pcf_experiments/#self-healing-experiment-finalization","text":"API: https://apidocs.cloudfoundry.org/3.5.0/apps/updating_an_app.html The application is rescaled back to its original value through the same API by which the experiment was launched.","title":"Self Healing &amp; Experiment Finalization"},{"location":"Experiment_Modules/pcf_experiments/#application-restart","text":"Application Restart tests how resilient the platform is to a short downtime of an entire application. This can happen while an application is being updated with a hotfix, or if a bug in an application causes all instances to simultaneously crash. Restarting your application stops your application and restarts it with the already compiled droplet. A droplet is a tarball that includes: stack buildpack application source code","title":"Application Restart"},{"location":"Experiment_Modules/pcf_experiments/#mechanism_4","text":"API: https://apidocs.cloudfoundry.org/4.2.0/apps/creating_an_app.html A Restart Application request is initiated through the Cloud Foundry SDK.","title":"Mechanism"},{"location":"Experiment_Modules/pcf_experiments/#health-check-mechanism_3","text":"API: https://apidocs.cloudfoundry.org/3.5.0/apps/list_all_apps.html All Applications are listed and parsed for running state. This ensures that any dependency chains are still healthy.","title":"Health Check Mechanism"},{"location":"Experiment_Modules/pcf_experiments/#application-restage","text":"Application Restage tests how resilient the platform is to a short downtime of an entire application. This can happen while an application is being updated with a hotfix, or if a bug in an application causes all instances to simultaneously crash. Restaging your application stops your application and restages it, by compiling a new droplet and starting it.","title":"Application Restage"},{"location":"Experiment_Modules/pcf_experiments/#mechanism_5","text":"API: https://apidocs.cloudfoundry.org/3.5.0/apps/restage_an_app.html A Restage Application request is initiated through the Cloud Foundry SDK.","title":"Mechanism"},{"location":"Experiment_Modules/pcf_experiments/#health-check-mechanism_4","text":"API: https://apidocs.cloudfoundry.org/3.5.0/apps/list_all_apps.html All Applications are listed and parsed for running state. This ensures that any dependency chains are still healthy.","title":"Health Check Mechanism"},{"location":"Experiment_Modules/pcf_experiments/#unmap-application-route","text":"Unmap application routes tests how the applications running in Cloud Foundry containers are resilient to short unavailability of resources accessible over network. Routes and Domains concept description","title":"Unmap Application Route"},{"location":"Experiment_Modules/pcf_experiments/#mechanism_6","text":"API: https://apidocs.cloudfoundry.org/3.5.0/routes_mapping/mapping_an_app_and_a_route.html A unmap route request is initiated through the Cloud Foundry SDK. When an application has multiple routes one of them is selected randomly and unmaped. When an application has no routes the experiment is skipped.","title":"Mechanism"},{"location":"Experiment_Modules/pcf_experiments/#health-check-mechanism_5","text":"API: https://apidocs.cloudfoundry.org/3.5.0/apps/list_all_apps.html All Applications are listed and parsed for running state. This ensures that any dependency chains are still healthy.","title":"Health Check Mechanism"},{"location":"Experiment_Modules/pcf_experiments/#self-healing-experiment-finalization_1","text":"API: https://apidocs.cloudfoundry.org/3.5.0/routes_mapping/delete_a_particular_route_mapping.html In the end of the experiment the previously unmaped route is assigned back to the application","title":"Self Healing &amp; Experiment Finalization"},{"location":"Experiment_Modules/ssh_based_experiments/","text":"SSH Based Experiments The SSH module can interact with remote SSH servers and perform specific set of experiments on the targeted platform. Implementation Details The Chaos Engine SSH Module uses the SSHJ Library for SSH Session management, including all ciphers and key exchange algorithms in the library. Each Platform has their own mechanism for determining the SSH Endpoint and Authentication Mechanism. Info The SSH Module uses PromiscuousVerifier so it accepts any platform fingerprint. Custom Script-Based Experiments Chaos Engine automatically loads any scripts in the classpath under ssh/experiments/ . Included Script-Based Experiments","title":"SSH Based Experiments"},{"location":"Experiment_Modules/ssh_based_experiments/#ssh-based-experiments","text":"The SSH module can interact with remote SSH servers and perform specific set of experiments on the targeted platform.","title":"SSH Based Experiments"},{"location":"Experiment_Modules/ssh_based_experiments/#implementation-details","text":"The Chaos Engine SSH Module uses the SSHJ Library for SSH Session management, including all ciphers and key exchange algorithms in the library. Each Platform has their own mechanism for determining the SSH Endpoint and Authentication Mechanism. Info The SSH Module uses PromiscuousVerifier so it accepts any platform fingerprint.","title":"Implementation Details"},{"location":"Experiment_Modules/ssh_based_experiments/#custom-script-based-experiments","text":"Chaos Engine automatically loads any scripts in the classpath under ssh/experiments/ .","title":"Custom Script-Based Experiments"},{"location":"Experiment_Modules/ssh_based_experiments/#included-script-based-experiments","text":"","title":"Included Script-Based Experiments"},{"location":"Experiment_Modules/Script_Experiments/included_script_experiments/","text":"Included Script Experiments Following scripts are packaged inside Chaos Engine core jar archive. Each script must follow Chaos Engine Scripting Requirements Script Name Description Requires cattle burnIO.sh Utilize system disk to maximum Yes cpuBurn.sh Simulates high CPU usage on all available processing units Yes dnsBlock.sh Removes all DNS servers from system configuration Yes fillDisk.sh Creates large file on the system root partition Yes forkBomb Runs endless recursion that corrupts system memory Yes memoryConsumer.sh Consumes all free memory Yes nullRoute.sh Adds an IP table rule that will forward traffic from specific subnet to black hole Yes starveRandomGenerator.sh Simulates low entropy Yes terminateProcess.sh Sends a SIGINT to PID 1 in a Container, or SIGKILL to many processes based on keyword names if not a container (i.e., python, java, node, etc.) Yes Custom Scripts Custom Scripts can be loaded in from the file system by placing them in a specific path and configuring one property. Property Description Default allowScriptsFromFilesystem Allows scripts to be loaded from the file system in the Script Manager false The scripts must be nested under the classpath folder, under the subfolder path ssh/experiments/ . In the default Docker image, this would require mounting a path under /chaosengine/lib/ssh/experiments/ and including custom scripts there. Note that the User Defined Experiment functionality depends on the Experiment name, which is derived from the filename. If a duplicate filename exists, it will choose which one to run based on which is loaded by the file system first. Choose a distinct filename to ensure proper control.","title":"Included Script Experiments"},{"location":"Experiment_Modules/Script_Experiments/included_script_experiments/#included-script-experiments","text":"Following scripts are packaged inside Chaos Engine core jar archive. Each script must follow Chaos Engine Scripting Requirements Script Name Description Requires cattle burnIO.sh Utilize system disk to maximum Yes cpuBurn.sh Simulates high CPU usage on all available processing units Yes dnsBlock.sh Removes all DNS servers from system configuration Yes fillDisk.sh Creates large file on the system root partition Yes forkBomb Runs endless recursion that corrupts system memory Yes memoryConsumer.sh Consumes all free memory Yes nullRoute.sh Adds an IP table rule that will forward traffic from specific subnet to black hole Yes starveRandomGenerator.sh Simulates low entropy Yes terminateProcess.sh Sends a SIGINT to PID 1 in a Container, or SIGKILL to many processes based on keyword names if not a container (i.e., python, java, node, etc.) Yes","title":"Included Script Experiments"},{"location":"Experiment_Modules/Script_Experiments/included_script_experiments/#custom-scripts","text":"Custom Scripts can be loaded in from the file system by placing them in a specific path and configuring one property. Property Description Default allowScriptsFromFilesystem Allows scripts to be loaded from the file system in the Script Manager false The scripts must be nested under the classpath folder, under the subfolder path ssh/experiments/ . In the default Docker image, this would require mounting a path under /chaosengine/lib/ssh/experiments/ and including custom scripts there. Note that the User Defined Experiment functionality depends on the Experiment name, which is derived from the filename. If a duplicate filename exists, it will choose which one to run based on which is loaded by the file system first. Choose a distinct filename to ensure proper control.","title":"Custom Scripts"},{"location":"Experiment_Modules/Script_Experiments/script_header/","text":"Script Header The header block of a Script loaded in the Chaos Engine Script Manager is parsed for several values that will be used to consider how the script is used. Example Header Script for Cattle #!/bin/sh # Dependencies: dd # Description: This will continuously read from /dev/random and drain entropy on the server. while [[ true ]] ; do dd if = /dev/random of = /dev/null bs = 1 count = 1024 ; done Script for Pets #!/bin/sh # Dependencies: whoami, echo, init # Description: This will continuously read from /dev/random and drain entropy on the server. # Health check: echo 'Hello, world' # Self healing: init 6 whoami # This is admittedly a bad script for an experiment The parsed fields require exact matches, including the space after the #, as well as the colon immediately after the phrase. Property Description Dependencies A comma separated list of binaries that need to be present on the server in order for the script to succeed. These will be evaluated using which , type , and command -v before the script is executed. Containers will persist a memory of which of these dependencies do not exist, and filter out future execution of scripts that have these missing dependencies. Health check A command to be run to evaluate the health of the server during the experiment runtime. This command should produce a non-zero exit code if the experiment hasn't corrected itself. This is not used for cattle scripts, as they instead evaluate if the container was recycled. Self healing A command to be run if an experiment has gone over duration. This command should have an effect which causes the Health Check command to return successfully. This is not used for cattle scripts, as they instead recycled the container as a healing mechanism. Finalize command A command to be run after the experiment has finished, perhaps to correct something that we don't expect automation to fix, even though it appears health. This is not used for cattle scripts, as they expected the container to be recycled, and thus, need no correction. Description No functional use, but leaves room to be exposed by APIs. Cattle-Only Status A script may be used by either Cattle and Pets, or just Cattle. This is determined by the presence of the Health check and Self healing commands. If both commands are present, then the script is flagged as being pet-capable. It is expected that the Self healing and Finalize scripts will result in the system coming back to normal operation. If neither of the commands are present, the script is flagged as cattle-only. It will only be run on systems that are backed by some form of controller that is capable of rebuilding the object (for example, an Autoscaling Group EC2 instance, or a Kubernetes Pod as part of a ReplicaSet). If exactly one of the commands is present, an Exception is thrown during creation. This should also cause unit tests to fail, which should prevent the system from being built.","title":"Script Header"},{"location":"Experiment_Modules/Script_Experiments/script_header/#script-header","text":"The header block of a Script loaded in the Chaos Engine Script Manager is parsed for several values that will be used to consider how the script is used.","title":"Script Header"},{"location":"Experiment_Modules/Script_Experiments/script_header/#example-header","text":"Script for Cattle #!/bin/sh # Dependencies: dd # Description: This will continuously read from /dev/random and drain entropy on the server. while [[ true ]] ; do dd if = /dev/random of = /dev/null bs = 1 count = 1024 ; done Script for Pets #!/bin/sh # Dependencies: whoami, echo, init # Description: This will continuously read from /dev/random and drain entropy on the server. # Health check: echo 'Hello, world' # Self healing: init 6 whoami # This is admittedly a bad script for an experiment The parsed fields require exact matches, including the space after the #, as well as the colon immediately after the phrase. Property Description Dependencies A comma separated list of binaries that need to be present on the server in order for the script to succeed. These will be evaluated using which , type , and command -v before the script is executed. Containers will persist a memory of which of these dependencies do not exist, and filter out future execution of scripts that have these missing dependencies. Health check A command to be run to evaluate the health of the server during the experiment runtime. This command should produce a non-zero exit code if the experiment hasn't corrected itself. This is not used for cattle scripts, as they instead evaluate if the container was recycled. Self healing A command to be run if an experiment has gone over duration. This command should have an effect which causes the Health Check command to return successfully. This is not used for cattle scripts, as they instead recycled the container as a healing mechanism. Finalize command A command to be run after the experiment has finished, perhaps to correct something that we don't expect automation to fix, even though it appears health. This is not used for cattle scripts, as they expected the container to be recycled, and thus, need no correction. Description No functional use, but leaves room to be exposed by APIs.","title":"Example Header"},{"location":"Experiment_Modules/Script_Experiments/script_header/#cattle-only-status","text":"A script may be used by either Cattle and Pets, or just Cattle. This is determined by the presence of the Health check and Self healing commands. If both commands are present, then the script is flagged as being pet-capable. It is expected that the Self healing and Finalize scripts will result in the system coming back to normal operation. If neither of the commands are present, the script is flagged as cattle-only. It will only be run on systems that are backed by some form of controller that is capable of rebuilding the object (for example, an Autoscaling Group EC2 instance, or a Kubernetes Pod as part of a ReplicaSet). If exactly one of the commands is present, an Exception is thrown during creation. This should also cause unit tests to fail, which should prevent the system from being built.","title":"Cattle-Only Status"},{"location":"Logging_and_Reporting/Notifications/datadog_events/","text":"Datatog Events Description The Chaos Engine can feed all notifications to DataDog. All the events can be analyzed and post-process in DataDog's event stream . The event stream and information in Chaos Engine logs can be correlated with platform metrics, raised alerts, app logs and it helps to build overall picture of experiment execution as well as platform health. Datatog Event There are two types of events which differs event title: Chaos Experiment Event - generated during experiment lifecycle Chaos Message - general notifications Every Event consists of title (Chaos Event prefix + experiment method or Chaos Message depending on type), message (detailed description of an event, see Chaos Event Types) and set of tags. Following tags are assigned to each event: Chaos Experiment Event Structure Note Chaos Experiment Events are grouped in DataDog stream by experiment ID Field Description Example Value Title Title composed of Chaos Message prefix + detail suffix Chaos Message - Engine Started Message General Notice Chaos Engine has been started Tag Description Example Value notificationlevel Event severity ERROR, WARN, GOOD experimentid UUID of an experiment 505ec298-d4c6-4be0-b209-7f4aae78b359 experimenttype Type of an experiment state, resource, network experimentmethod Experiment implementation nullRoute.sh container.aggregationidentifier Resource group identifier nginx (k8s deployment name) container.name Container simple name ngnix-hm5d2 (k8s container name) host hostname or IP of the Chaos Engine instance which produced the message example.com service Static identifier of Chaos Engine generated events chaosengine Chaos Experiment Events Example Chaos Message Structure Field Description Example Value Title Static title Chaos Experiment Event Message General Notice Starting new experiment Tag Description Example Value notificationlevel GOOD host hostname or IP of the Chaos Engine instance which produced the message example.com service Static identifier of Chaos Engine generated events chaosengine Chaos Message Examples Severity Message GOOD WARN Configuration All DataDog related configuration variables have datadog prefix. DataDog notification channel can be enabled by adding enableEvents property into your environment variables. Variable Description Value Default datadog.enableEvents Environment variable controlling DD notification module true / false false datadog.statsdHost Datadog agent host datadog datadog.statsdPort Datadog agent StatsD Port 8125","title":"Datatog Events"},{"location":"Logging_and_Reporting/Notifications/datadog_events/#datatog-events","text":"","title":"Datatog Events"},{"location":"Logging_and_Reporting/Notifications/datadog_events/#description","text":"The Chaos Engine can feed all notifications to DataDog. All the events can be analyzed and post-process in DataDog's event stream . The event stream and information in Chaos Engine logs can be correlated with platform metrics, raised alerts, app logs and it helps to build overall picture of experiment execution as well as platform health.","title":"Description"},{"location":"Logging_and_Reporting/Notifications/datadog_events/#datatog-event","text":"There are two types of events which differs event title: Chaos Experiment Event - generated during experiment lifecycle Chaos Message - general notifications Every Event consists of title (Chaos Event prefix + experiment method or Chaos Message depending on type), message (detailed description of an event, see Chaos Event Types) and set of tags. Following tags are assigned to each event:","title":"Datatog Event"},{"location":"Logging_and_Reporting/Notifications/datadog_events/#chaos-experiment-event-structure","text":"Note Chaos Experiment Events are grouped in DataDog stream by experiment ID Field Description Example Value Title Title composed of Chaos Message prefix + detail suffix Chaos Message - Engine Started Message General Notice Chaos Engine has been started Tag Description Example Value notificationlevel Event severity ERROR, WARN, GOOD experimentid UUID of an experiment 505ec298-d4c6-4be0-b209-7f4aae78b359 experimenttype Type of an experiment state, resource, network experimentmethod Experiment implementation nullRoute.sh container.aggregationidentifier Resource group identifier nginx (k8s deployment name) container.name Container simple name ngnix-hm5d2 (k8s container name) host hostname or IP of the Chaos Engine instance which produced the message example.com service Static identifier of Chaos Engine generated events chaosengine","title":"Chaos Experiment Event Structure"},{"location":"Logging_and_Reporting/Notifications/datadog_events/#chaos-experiment-events-example","text":"","title":"Chaos Experiment Events Example"},{"location":"Logging_and_Reporting/Notifications/datadog_events/#chaos-message-structure","text":"Field Description Example Value Title Static title Chaos Experiment Event Message General Notice Starting new experiment Tag Description Example Value notificationlevel GOOD host hostname or IP of the Chaos Engine instance which produced the message example.com service Static identifier of Chaos Engine generated events chaosengine","title":"Chaos Message Structure"},{"location":"Logging_and_Reporting/Notifications/datadog_events/#chaos-message-examples","text":"Severity Message GOOD WARN","title":"Chaos Message Examples"},{"location":"Logging_and_Reporting/Notifications/datadog_events/#configuration","text":"All DataDog related configuration variables have datadog prefix. DataDog notification channel can be enabled by adding enableEvents property into your environment variables. Variable Description Value Default datadog.enableEvents Environment variable controlling DD notification module true / false false datadog.statsdHost Datadog agent host datadog datadog.statsdPort Datadog agent StatsD Port 8125","title":"Configuration"},{"location":"Logging_and_Reporting/Notifications/slack/","text":"Slack Notifications Description Notify team members of running experiments. The Chaos Engine generates two types of notifications: Chaos Experiment Event - generated during experiment lifecycle Chaos Message - general notifications Note Slack notifications are buffered. The buffer is flushed every minute or when buffer size reaches 20 items. Chaos Experiment Event Structure Field Description Example Value Header Event Prefix Chaos Experiment Event Notification Level Event severity ERROR, WARN, GOOD Message Event description Starting new experiment Experiment ID UUID of an experiment 505ec298-d4c6-4be0-b209-7f4aae78b359 Experiment Method Experiment implementation nullRoute.sh Experiment Type Type of an experiment state, resource, network Aggregation Identifier Resource group identifier nginx (k8s deployment name) Container Type Type of targeted container CloudFoundryContainer, KubernetesPodContainer,.. Simple Name Container simple name ngnix-hm5d2 (k8s container name) Chaos Experiment Event Examples ERROR WARN GOOD Chaos Message Structure Field Description Example Value Header Event prefix Chaos Message Message General notice Chaos Engine has been started Chaos Message Examples GOOD WARN Configuration Configure a Slack Webhook and provide the URI as an argument. Variable Description Default slack_webhookuri Web Hook URI for pushing to Slack. None For more details see webhook creation manual Development notes Slack API Documentation Message Formatting Message Attachments Message Builder","title":"Slack Notifications"},{"location":"Logging_and_Reporting/Notifications/slack/#slack-notifications","text":"","title":"Slack Notifications"},{"location":"Logging_and_Reporting/Notifications/slack/#description","text":"Notify team members of running experiments. The Chaos Engine generates two types of notifications: Chaos Experiment Event - generated during experiment lifecycle Chaos Message - general notifications Note Slack notifications are buffered. The buffer is flushed every minute or when buffer size reaches 20 items.","title":"Description"},{"location":"Logging_and_Reporting/Notifications/slack/#chaos-experiment-event-structure","text":"Field Description Example Value Header Event Prefix Chaos Experiment Event Notification Level Event severity ERROR, WARN, GOOD Message Event description Starting new experiment Experiment ID UUID of an experiment 505ec298-d4c6-4be0-b209-7f4aae78b359 Experiment Method Experiment implementation nullRoute.sh Experiment Type Type of an experiment state, resource, network Aggregation Identifier Resource group identifier nginx (k8s deployment name) Container Type Type of targeted container CloudFoundryContainer, KubernetesPodContainer,.. Simple Name Container simple name ngnix-hm5d2 (k8s container name)","title":"Chaos Experiment Event Structure"},{"location":"Logging_and_Reporting/Notifications/slack/#chaos-experiment-event-examples","text":"ERROR WARN GOOD","title":"Chaos Experiment Event Examples"},{"location":"Logging_and_Reporting/Notifications/slack/#chaos-message-structure","text":"Field Description Example Value Header Event prefix Chaos Message Message General notice Chaos Engine has been started","title":"Chaos Message Structure"},{"location":"Logging_and_Reporting/Notifications/slack/#chaos-message-examples","text":"GOOD WARN","title":"Chaos Message Examples"},{"location":"Logging_and_Reporting/Notifications/slack/#configuration","text":"Configure a Slack Webhook and provide the URI as an argument. Variable Description Default slack_webhookuri Web Hook URI for pushing to Slack. None For more details see webhook creation manual","title":"Configuration"},{"location":"Logging_and_Reporting/Notifications/slack/#development-notes","text":"","title":"Development notes"},{"location":"Logging_and_Reporting/Notifications/slack/#slack-api-documentation","text":"Message Formatting Message Attachments Message Builder","title":"Slack API Documentation"},{"location":"Quick_Start_Guide/","text":"Quick Start Guide Step 1: Install Docker Follow the Docker instalation instructions here Verify Docker Installation To verify Docker installation run docker ps . Expected output is: user@host:~$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES If the output looks similar to following example open new terminal session or simply logout and login again. user@host:~$ docker ps Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.40/containers/json: dial unix /var/run/docker.sock: connect: permission denied Step 2: Install Docker Compose For detailed instructions on how to install docker compose see official Docker documentation. Verify Docker Compose Installation user@host:~$ docker-compose -v docker-compose version 1 .25.0, build 0a186604 If the command docker-compose fails after installation, check your path. You can also create a symbolic link to /usr/bin or any other directory in your path . Step 3: Download Latest Chaos Engine Sources user@host:~$ git clone https://github.com/gemalto/chaos-engine Cloning into 'chaos-engine' ... ... Checking connectivity... done . Step 4: Pull Chaos Engine Image Pull latest Chaos Engine image from DockerHub. user@host:~$ cd chaos-engine/ user@host:~/chaos-engine$ docker pull thalesgroup/chaos-engine:latest Step 5: Configure Basic Framework Setup user@host:~/chaos-engine$ echo \"holidays=DUM VAULT_TOKEN=00000000-0000-0000-0000-000000000000 VAULT_SCHEME=http VAULT_HOST=vault VAULT_PORT=8200 VAULT_10=true spring_profiles_active=PRODUCTION automatedMode=false CHAOS_SECURITY_ENABLED=false\" > .env Configure Experiment Modules Let's say you want to execute experiments targeting a Kubernetes cluster. First check experiment modules documentation to see what config options are available then add all configuration properties into vault-secrets.json file located in ~/developer-tools/vault-loader . Example content of vault-secrets.json { \"kubernetes\" : \"\" , \"kubernetes.url\" : \"https://127.127.127.127\" , \"kubernetes.token\" : \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c\" , \"kubernetes.averageMillisPerExperiment\" : \"30000\" } Configure DataDog Integration Enable DataDog If you are going to ship data to DataDog run following command where $YOUR_API_KEY will be replaced by your real DataDog API key. user@host:~/chaos-engine$ echo \"DD_API_KEY= $YOUR_API_KEY \" > .datadog_api_key Disable DataDog If you don't need DataDog integration keep .datadog_api_key empty user@host:~/chaos-engine$ touch .datadog_api_key Step 6: Run Start the Engine by running docker-compose up user@host:~/chaos-engine$ docker-compose up When you see Experiments total count: 0 your Chaos Engine instance is up and ready chaosengine_ 1 | { \"@timestamp\" : \"2019-11-28T18:07:36.491Z\" , \"@version\" : \"1\" , \"message\" : \"Experiments total count: 0\" , \"logger_name\" : \"com.thales.chaos.experiment.ExperimentManager\" , \"thread_name\" : \"chaos-10\" , \"level\" : \"INFO\" , \"level_value\" : 20000 , \"count\" : 0 , \"env\" : \"DEVELOPMENT\" , \"chaos-host\" : \"b4bd5f0829d6@172.18.0.4\" } Step 7: Enjoy Manualy trigger experiment execution by running user@host:~/chaos-engine$ curl -X POST \"http://localhost:8080/experiment/start\" -H \"accept: */*\"","title":"Quick Start Guide"},{"location":"Quick_Start_Guide/#quick-start-guide","text":"","title":"Quick Start Guide"},{"location":"Quick_Start_Guide/#step-1-install-docker","text":"Follow the Docker instalation instructions here","title":"Step 1: Install Docker"},{"location":"Quick_Start_Guide/#verify-docker-installation","text":"To verify Docker installation run docker ps . Expected output is: user@host:~$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES If the output looks similar to following example open new terminal session or simply logout and login again. user@host:~$ docker ps Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.40/containers/json: dial unix /var/run/docker.sock: connect: permission denied","title":"Verify Docker Installation"},{"location":"Quick_Start_Guide/#step-2-install-docker-compose","text":"For detailed instructions on how to install docker compose see official Docker documentation.","title":"Step 2: Install Docker Compose"},{"location":"Quick_Start_Guide/#verify-docker-compose-installation","text":"user@host:~$ docker-compose -v docker-compose version 1 .25.0, build 0a186604 If the command docker-compose fails after installation, check your path. You can also create a symbolic link to /usr/bin or any other directory in your path .","title":"Verify Docker Compose Installation"},{"location":"Quick_Start_Guide/#step-3-download-latest-chaos-engine-sources","text":"user@host:~$ git clone https://github.com/gemalto/chaos-engine Cloning into 'chaos-engine' ... ... Checking connectivity... done .","title":"Step 3: Download Latest Chaos Engine Sources"},{"location":"Quick_Start_Guide/#step-4-pull-chaos-engine-image","text":"Pull latest Chaos Engine image from DockerHub. user@host:~$ cd chaos-engine/ user@host:~/chaos-engine$ docker pull thalesgroup/chaos-engine:latest","title":"Step 4: Pull Chaos Engine Image"},{"location":"Quick_Start_Guide/#step-5-configure","text":"","title":"Step 5: Configure"},{"location":"Quick_Start_Guide/#basic-framework-setup","text":"user@host:~/chaos-engine$ echo \"holidays=DUM VAULT_TOKEN=00000000-0000-0000-0000-000000000000 VAULT_SCHEME=http VAULT_HOST=vault VAULT_PORT=8200 VAULT_10=true spring_profiles_active=PRODUCTION automatedMode=false CHAOS_SECURITY_ENABLED=false\" > .env","title":"Basic Framework Setup"},{"location":"Quick_Start_Guide/#configure-experiment-modules","text":"Let's say you want to execute experiments targeting a Kubernetes cluster. First check experiment modules documentation to see what config options are available then add all configuration properties into vault-secrets.json file located in ~/developer-tools/vault-loader . Example content of vault-secrets.json { \"kubernetes\" : \"\" , \"kubernetes.url\" : \"https://127.127.127.127\" , \"kubernetes.token\" : \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c\" , \"kubernetes.averageMillisPerExperiment\" : \"30000\" }","title":"Configure Experiment Modules"},{"location":"Quick_Start_Guide/#configure-datadog-integration","text":"","title":"Configure DataDog Integration"},{"location":"Quick_Start_Guide/#enable-datadog","text":"If you are going to ship data to DataDog run following command where $YOUR_API_KEY will be replaced by your real DataDog API key. user@host:~/chaos-engine$ echo \"DD_API_KEY= $YOUR_API_KEY \" > .datadog_api_key","title":"Enable DataDog"},{"location":"Quick_Start_Guide/#disable-datadog","text":"If you don't need DataDog integration keep .datadog_api_key empty user@host:~/chaos-engine$ touch .datadog_api_key","title":"Disable DataDog"},{"location":"Quick_Start_Guide/#step-6-run","text":"Start the Engine by running docker-compose up user@host:~/chaos-engine$ docker-compose up When you see Experiments total count: 0 your Chaos Engine instance is up and ready chaosengine_ 1 | { \"@timestamp\" : \"2019-11-28T18:07:36.491Z\" , \"@version\" : \"1\" , \"message\" : \"Experiments total count: 0\" , \"logger_name\" : \"com.thales.chaos.experiment.ExperimentManager\" , \"thread_name\" : \"chaos-10\" , \"level\" : \"INFO\" , \"level_value\" : 20000 , \"count\" : 0 , \"env\" : \"DEVELOPMENT\" , \"chaos-host\" : \"b4bd5f0829d6@172.18.0.4\" }","title":"Step 6: Run"},{"location":"Quick_Start_Guide/#step-7-enjoy","text":"Manualy trigger experiment execution by running user@host:~/chaos-engine$ curl -X POST \"http://localhost:8080/experiment/start\" -H \"accept: */*\"","title":"Step 7: Enjoy"},{"location":"Quick_Start_Guide/config_examples/","text":"Configuration Examples Enable EC2 Module This example will enable EC2 module targeting all machines in eu-west-1 region with ChaosVictim tag set to true. Note sshprivatekeys is needed only in case you would like to perform shell based experiments from Chaos Engine build in suite. Vault { \"aws.ec2\" : \"true\" , \"aws.accessKeyId\" : \"ABCDEFGHIJKLMNOPQRST\" , \"aws.secretAccessKey\" : \"AbCDeFGHI+Jklmnop12345678789+123456789AB\" , \"aws.region\" : \"eu-west-1\" , \"aws.ec2.filter.tag.ChaosVictim\" : \"true\" , \"aws.ec2.averageMillisPerExperiment\" : \"30000\" , \"aws.ec2.sshprivatekeys.your-key-name\" : \"MIIEowIBABC ... EFQ=\" } ENV Vars AWS_EC2 = \"true\" AWS_ACCESSKEYID = \"ABCDEFGHIJKLMNOPQRST\" AWS_SECRETACCESSKEY = \"AbCDeFGHI+Jklmnop12345678789+123456789AB\" AWS_REGION = \"eu-west-1\" AWS_EC2_FILTER_TAG_CHAOSVICTIM = \"true\" AWS_EC2_AVERAGEMILLISPEREXPERIMENT = 30000 AWS_EC2_SSHPRIVATEKEYS_YOUR_KEY_NAME = \"MIIEowIBABC ... EFQ=\" Enable RDS Module Following example enables RDS module. Experiments will be executed in eu-west-3 region on top all DB instances with ChaosVictim tag set to true. Vault { \"aws.rds\" : \"true\" , \"aws.rds.filter.ChaosVictim\" : \"true\" , \"aws.rds.averageMillisPerExperiment\" : \"300000\" , \"aws.accessKeyId\" : \"ABCDEFGHIJKLMNOPQRST\" , \"aws.secretAccessKey\" : \"AbCDeFGHI+Jklmnop12345678789+123456789AB\" , \"aws.region\" : \"eu-west-3\" , } ENV Vars AWS_RDS = \"true\" AWS_RDS_FILTER_CHAOSVICTIM = \"true\" AWS_RDS_AVERAGEMILLISPEREXPERIMENT = \"300000\" AWS_ACCESSKEYID = \"ABCDEFGHIJKLMNOPQRST\" AWS_SECRETACCESSKEY = \"AbCDeFGHI+Jklmnop12345678789+123456789AB\" AWS_REGION = \"eu-west-3\" Enable PCF Module When you use following example the Chaos Engine will run experiments on both possible PCF levels application and container . If you need just one of them simply remove cf.containerChaos or cf.applicationChaos from your configuration. Vault { \"cf.apihost\" : \"api.my.pcf.com\" , \"cf.organization\" : \"chaos\" , \"cf.space\" : \"chaos\" , \"cf.password\" : \"pa$$Word-ABCDefgAbc\" , \"cf.port\" : \"443\" , \"cf.username\" : \"admin\" , \"cf.averageMillisPerExperiment\" : \"300000\" , \"cf.containerChaos\" : \"true\" , \"cf.applicationChaos\" : \"true\" } ENV Vars CF_APIHOST = \"api.my.pcf.com\" CF_ORGANIZATION = \"chaos\" CF_SPACE = \"chaos\" CF_PASSWORD = \"pa $$ Word-ABCDefgAbc\" CF_PORT = \"443\" CF_USERNAME = \"admin\" CF_AVERAGEMILLISPEREXPERIMENT = \"300000\" CF_CONTAINERCHAOS = \"true\" CF_APPLICATIONCHAOS = \"true\" Enable Kubernetes Module Running Kubernetes experiments requires configuration on the server side please check Kubernetes module manual Vault { \"kubernetes\" : \"true\" , \"kubernetes.url\" : \"https://77.77.77.77\" , \"kubernetes.token\" : \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c\" , \"kubernetes.averageMillisPerExperiment\" : \"30000\" } ENV Vars KUBERNETES = \"TRUE\" KUBERNETES_URL = \"HTTPS://77.77.77.77\" KUBERNETES_TOKEN = \"EYJHBGCIOIJIUZI1NIISINR5CCI6IKPXVCJ9.EYJZDWIIOIIXMJM0NTY3ODKWIIWIBMFTZSI6IKPVAG4GRG9LIIWIAWF0IJOXNTE2MJM5MDIYFQ.SFLKXWRJSMEKKF2QT4FWPMEJF36POK6YJV_ADQSSW5C\" KUBERNETES_AVERAGEMILLISPEREXPERIMENT = \"30000\" Enable Chaos Engine REST Security This way you can enable Chaos Engine REST security. Following example will provision two users admin and user . Vault { \"chaos.security.enabled\" : \"true\" , \"chaos.security.users[0].username\" : \"admin\" , \"chaos.security.users[0].password\" : \"admin_P@ssw0rd\" , \"chaos.security.users[0].roles\" : \"ADMIN\" , \"chaos.security.users[1].username\" : \"user\" , \"chaos.security.users[1].password\" : \"user_P@ssw0rd\" , \"chaos.security.users[1].roles\" : \"USER\" } ENV Vars CHAOS_SECURITY_USERS_0_USERNAME = admin CHAOS_SECURITY_USERS_0_PASSWORD = admin_P@ssw0rd CHAOS_SECURITY_USERS_0_ROLES = ADMIN CHAOS_SECURITY_USERS_1_USERNAME = user CHAOS_SECURITY_USERS_1_PASSWORD = user_P@ssw0rd CHAOS_SECURITY_USERS_1_ROLES = USER","title":"Configuration Examples"},{"location":"Quick_Start_Guide/config_examples/#configuration-examples","text":"","title":"Configuration Examples"},{"location":"Quick_Start_Guide/config_examples/#enable-ec2-module","text":"This example will enable EC2 module targeting all machines in eu-west-1 region with ChaosVictim tag set to true. Note sshprivatekeys is needed only in case you would like to perform shell based experiments from Chaos Engine build in suite. Vault { \"aws.ec2\" : \"true\" , \"aws.accessKeyId\" : \"ABCDEFGHIJKLMNOPQRST\" , \"aws.secretAccessKey\" : \"AbCDeFGHI+Jklmnop12345678789+123456789AB\" , \"aws.region\" : \"eu-west-1\" , \"aws.ec2.filter.tag.ChaosVictim\" : \"true\" , \"aws.ec2.averageMillisPerExperiment\" : \"30000\" , \"aws.ec2.sshprivatekeys.your-key-name\" : \"MIIEowIBABC ... EFQ=\" } ENV Vars AWS_EC2 = \"true\" AWS_ACCESSKEYID = \"ABCDEFGHIJKLMNOPQRST\" AWS_SECRETACCESSKEY = \"AbCDeFGHI+Jklmnop12345678789+123456789AB\" AWS_REGION = \"eu-west-1\" AWS_EC2_FILTER_TAG_CHAOSVICTIM = \"true\" AWS_EC2_AVERAGEMILLISPEREXPERIMENT = 30000 AWS_EC2_SSHPRIVATEKEYS_YOUR_KEY_NAME = \"MIIEowIBABC ... EFQ=\"","title":"Enable EC2 Module"},{"location":"Quick_Start_Guide/config_examples/#enable-rds-module","text":"Following example enables RDS module. Experiments will be executed in eu-west-3 region on top all DB instances with ChaosVictim tag set to true. Vault { \"aws.rds\" : \"true\" , \"aws.rds.filter.ChaosVictim\" : \"true\" , \"aws.rds.averageMillisPerExperiment\" : \"300000\" , \"aws.accessKeyId\" : \"ABCDEFGHIJKLMNOPQRST\" , \"aws.secretAccessKey\" : \"AbCDeFGHI+Jklmnop12345678789+123456789AB\" , \"aws.region\" : \"eu-west-3\" , } ENV Vars AWS_RDS = \"true\" AWS_RDS_FILTER_CHAOSVICTIM = \"true\" AWS_RDS_AVERAGEMILLISPEREXPERIMENT = \"300000\" AWS_ACCESSKEYID = \"ABCDEFGHIJKLMNOPQRST\" AWS_SECRETACCESSKEY = \"AbCDeFGHI+Jklmnop12345678789+123456789AB\" AWS_REGION = \"eu-west-3\"","title":"Enable RDS Module"},{"location":"Quick_Start_Guide/config_examples/#enable-pcf-module","text":"When you use following example the Chaos Engine will run experiments on both possible PCF levels application and container . If you need just one of them simply remove cf.containerChaos or cf.applicationChaos from your configuration. Vault { \"cf.apihost\" : \"api.my.pcf.com\" , \"cf.organization\" : \"chaos\" , \"cf.space\" : \"chaos\" , \"cf.password\" : \"pa$$Word-ABCDefgAbc\" , \"cf.port\" : \"443\" , \"cf.username\" : \"admin\" , \"cf.averageMillisPerExperiment\" : \"300000\" , \"cf.containerChaos\" : \"true\" , \"cf.applicationChaos\" : \"true\" } ENV Vars CF_APIHOST = \"api.my.pcf.com\" CF_ORGANIZATION = \"chaos\" CF_SPACE = \"chaos\" CF_PASSWORD = \"pa $$ Word-ABCDefgAbc\" CF_PORT = \"443\" CF_USERNAME = \"admin\" CF_AVERAGEMILLISPEREXPERIMENT = \"300000\" CF_CONTAINERCHAOS = \"true\" CF_APPLICATIONCHAOS = \"true\"","title":"Enable PCF Module"},{"location":"Quick_Start_Guide/config_examples/#enable-kubernetes-module","text":"Running Kubernetes experiments requires configuration on the server side please check Kubernetes module manual Vault { \"kubernetes\" : \"true\" , \"kubernetes.url\" : \"https://77.77.77.77\" , \"kubernetes.token\" : \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c\" , \"kubernetes.averageMillisPerExperiment\" : \"30000\" } ENV Vars KUBERNETES = \"TRUE\" KUBERNETES_URL = \"HTTPS://77.77.77.77\" KUBERNETES_TOKEN = \"EYJHBGCIOIJIUZI1NIISINR5CCI6IKPXVCJ9.EYJZDWIIOIIXMJM0NTY3ODKWIIWIBMFTZSI6IKPVAG4GRG9LIIWIAWF0IJOXNTE2MJM5MDIYFQ.SFLKXWRJSMEKKF2QT4FWPMEJF36POK6YJV_ADQSSW5C\" KUBERNETES_AVERAGEMILLISPEREXPERIMENT = \"30000\"","title":"Enable Kubernetes Module"},{"location":"Quick_Start_Guide/config_examples/#enable-chaos-engine-rest-security","text":"This way you can enable Chaos Engine REST security. Following example will provision two users admin and user . Vault { \"chaos.security.enabled\" : \"true\" , \"chaos.security.users[0].username\" : \"admin\" , \"chaos.security.users[0].password\" : \"admin_P@ssw0rd\" , \"chaos.security.users[0].roles\" : \"ADMIN\" , \"chaos.security.users[1].username\" : \"user\" , \"chaos.security.users[1].password\" : \"user_P@ssw0rd\" , \"chaos.security.users[1].roles\" : \"USER\" } ENV Vars CHAOS_SECURITY_USERS_0_USERNAME = admin CHAOS_SECURITY_USERS_0_PASSWORD = admin_P@ssw0rd CHAOS_SECURITY_USERS_0_ROLES = ADMIN CHAOS_SECURITY_USERS_1_USERNAME = user CHAOS_SECURITY_USERS_1_PASSWORD = user_P@ssw0rd CHAOS_SECURITY_USERS_1_ROLES = USER","title":"Enable Chaos Engine REST Security"},{"location":"Quick_Start_Guide/kubernetes/","text":"Run Chaos Engine On Kubernetes This chapter describes how to deploy and run Chaos Engine as a service on Kubernetes. If you search for a manual describing how to run Kubernetes experiments please continue to Experiment Modules You can apply all steps described below by running example Chaos Engine Deployment YAML template. Just call kubectl apply -f chaos_engine_example_deployment.yml (don't forget to provision Vault). The script will deploy: Chaos Engine, Vault containers Vault Service Chaos Engine load balancer Step 1: Vault deployment We need a secure storage for our secrets and configuration variables. Let's use HashiCorp Vault . Create Vault Secret Start with creation of Vault secret token. This secret will be used by Chaos Engine to invoke Vault API and for UI access. In this example we use 00000000-0000-0000-0000-000000000000 . Replace this dummy token with your secret keyphrase. cat <<EOF | kubectl apply -f - apiVersion : v1 kind : Secret metadata : name : chaos-engine-secrets labels : app : \"chaosengine\" type : Opaque stringData : vault-token : \"00000000-0000-0000-0000-000000000000\" EOF Deploy Vault Deploy latest version of Vault by running using following template cat <<EOF | kubectl apply -f - apiVersion : apps/v1 kind : Deployment metadata : name : vault spec : replicas : 1 selector : matchLabels : app : vault strategy : type : RollingUpdate template : metadata : labels : app : vault spec : containers : - name : vault image : vault:latest env : - name : VAULT_DEV_ROOT_TOKEN_ID valueFrom : secretKeyRef : name : chaos-engine-secrets key : vault-token EOF Create Vault Service Expose the Vault container instance to rest of the containers as a cluster service. cat <<EOF | kubectl apply -f - apiVersion : v1 kind : Service metadata : name : vault labels : app : vault-chaos spec : ports : - port : 8200 protocol : TCP targetPort : 8200 selector : app : vault EOF Provision Vault Create a config file Create a config file called vault-secrets.json and add your required variables. Expected input format is JSON. You can find config files examples in Configuration Examples . Feeding Vault with data Run folowing sequence of commands to feed data to Vault CONTAINER = $( kubectl get pods -o json | jq -r ' .items[] | select(.kind == \"Pod\") | select(.metadata.name|startswith(\"vault\")) | .metadata.name' ) kubectl cp vault-secrets.json $CONTAINER :/tmp/ cat <<EOF | kubectl exec -it $CONTAINER /bin/sh - export VAULT_ADDR='http://127.0.0.1:8200'; vault login 00000000-0000-0000-0000-000000000000 ; vault kv put secret/chaosengine - < /tmp/vault-secrets.json rm /tmp/vault-secrets.json EOF Step 2: Chaos Engine Deployment Deploy Chaos Engine Download and deploy latest Chaos Engine cat <<EOF | kubectl apply -f - apiVersion : apps/v1 kind : Deployment metadata : name : chaosengine spec : replicas : 1 selector : matchLabels : app : chaosengine strategy : type : RollingUpdate template : metadata : labels : app : chaosengine spec : containers : - name : chaosengine image : thalesgroup/chaos-engine:latest env : - name : VAULT_TOKEN valueFrom : secretKeyRef : name : chaos-engine-secrets key : vault-token - name : VAULT_SCHEME value : http - name : VAULT_HOST value : vault - name : VAULT_PORT value : \"8200\" - name : VAULT_10 value : \"true\" - name : CHAOS_SECURITY_ENABLED value : \"false\" - name : automatedMode value : \"false\" EOF Create Load Balancer Expose Chaos Engine REST api. cat <<EOF | kubectl apply -f - apiVersion : v1 kind : Service metadata : name : chaosengine-lb labels : app : chaosengine-lb spec : ports : - port : 8080 targetPort : 8080 selector : app : chaosengine type : LoadBalancer EOF Check The Chaos Engine Health Get the IP of the chaosengine-lb . To do so run following command. kubectl describe services chaosengine-lb | grep 'LoadBalancer\\ Ingress' | awk '{print $3}' Go to http://$CHAOS_ENGINE_LB_IP:/health and if you see \"OK\" your Chaos Engine instance is ready to run your experiments.","title":"Run Chaos Engine On Kubernetes"},{"location":"Quick_Start_Guide/kubernetes/#run-chaos-engine-on-kubernetes","text":"This chapter describes how to deploy and run Chaos Engine as a service on Kubernetes. If you search for a manual describing how to run Kubernetes experiments please continue to Experiment Modules You can apply all steps described below by running example Chaos Engine Deployment YAML template. Just call kubectl apply -f chaos_engine_example_deployment.yml (don't forget to provision Vault). The script will deploy: Chaos Engine, Vault containers Vault Service Chaos Engine load balancer","title":"Run Chaos Engine On Kubernetes"},{"location":"Quick_Start_Guide/kubernetes/#step-1-vault-deployment","text":"We need a secure storage for our secrets and configuration variables. Let's use HashiCorp Vault .","title":"Step 1: Vault deployment"},{"location":"Quick_Start_Guide/kubernetes/#create-vault-secret","text":"Start with creation of Vault secret token. This secret will be used by Chaos Engine to invoke Vault API and for UI access. In this example we use 00000000-0000-0000-0000-000000000000 . Replace this dummy token with your secret keyphrase. cat <<EOF | kubectl apply -f - apiVersion : v1 kind : Secret metadata : name : chaos-engine-secrets labels : app : \"chaosengine\" type : Opaque stringData : vault-token : \"00000000-0000-0000-0000-000000000000\" EOF","title":"Create Vault Secret"},{"location":"Quick_Start_Guide/kubernetes/#deploy-vault","text":"Deploy latest version of Vault by running using following template cat <<EOF | kubectl apply -f - apiVersion : apps/v1 kind : Deployment metadata : name : vault spec : replicas : 1 selector : matchLabels : app : vault strategy : type : RollingUpdate template : metadata : labels : app : vault spec : containers : - name : vault image : vault:latest env : - name : VAULT_DEV_ROOT_TOKEN_ID valueFrom : secretKeyRef : name : chaos-engine-secrets key : vault-token EOF","title":"Deploy Vault"},{"location":"Quick_Start_Guide/kubernetes/#create-vault-service","text":"Expose the Vault container instance to rest of the containers as a cluster service. cat <<EOF | kubectl apply -f - apiVersion : v1 kind : Service metadata : name : vault labels : app : vault-chaos spec : ports : - port : 8200 protocol : TCP targetPort : 8200 selector : app : vault EOF","title":"Create Vault Service"},{"location":"Quick_Start_Guide/kubernetes/#provision-vault","text":"","title":"Provision Vault"},{"location":"Quick_Start_Guide/kubernetes/#create-a-config-file","text":"Create a config file called vault-secrets.json and add your required variables. Expected input format is JSON. You can find config files examples in Configuration Examples .","title":"Create a config file"},{"location":"Quick_Start_Guide/kubernetes/#feeding-vault-with-data","text":"Run folowing sequence of commands to feed data to Vault CONTAINER = $( kubectl get pods -o json | jq -r ' .items[] | select(.kind == \"Pod\") | select(.metadata.name|startswith(\"vault\")) | .metadata.name' ) kubectl cp vault-secrets.json $CONTAINER :/tmp/ cat <<EOF | kubectl exec -it $CONTAINER /bin/sh - export VAULT_ADDR='http://127.0.0.1:8200'; vault login 00000000-0000-0000-0000-000000000000 ; vault kv put secret/chaosengine - < /tmp/vault-secrets.json rm /tmp/vault-secrets.json EOF","title":"Feeding Vault with data"},{"location":"Quick_Start_Guide/kubernetes/#step-2-chaos-engine-deployment","text":"","title":"Step 2: Chaos Engine Deployment"},{"location":"Quick_Start_Guide/kubernetes/#deploy-chaos-engine","text":"Download and deploy latest Chaos Engine cat <<EOF | kubectl apply -f - apiVersion : apps/v1 kind : Deployment metadata : name : chaosengine spec : replicas : 1 selector : matchLabels : app : chaosengine strategy : type : RollingUpdate template : metadata : labels : app : chaosengine spec : containers : - name : chaosengine image : thalesgroup/chaos-engine:latest env : - name : VAULT_TOKEN valueFrom : secretKeyRef : name : chaos-engine-secrets key : vault-token - name : VAULT_SCHEME value : http - name : VAULT_HOST value : vault - name : VAULT_PORT value : \"8200\" - name : VAULT_10 value : \"true\" - name : CHAOS_SECURITY_ENABLED value : \"false\" - name : automatedMode value : \"false\" EOF","title":"Deploy Chaos Engine"},{"location":"Quick_Start_Guide/kubernetes/#create-load-balancer","text":"Expose Chaos Engine REST api. cat <<EOF | kubectl apply -f - apiVersion : v1 kind : Service metadata : name : chaosengine-lb labels : app : chaosengine-lb spec : ports : - port : 8080 targetPort : 8080 selector : app : chaosengine type : LoadBalancer EOF","title":"Create Load Balancer"},{"location":"Quick_Start_Guide/kubernetes/#check-the-chaos-engine-health","text":"Get the IP of the chaosengine-lb . To do so run following command. kubectl describe services chaosengine-lb | grep 'LoadBalancer\\ Ingress' | awk '{print $3}' Go to http://$CHAOS_ENGINE_LB_IP:/health and if you see \"OK\" your Chaos Engine instance is ready to run your experiments.","title":"Check The Chaos Engine Health"},{"location":"_Developer_Documentation/","text":"Developer Documentation This documentation is provided to help ensure that new contributions to the Chaos Engine project can easily ramp up, and follow similar standards to avoid Version Control merging issues. Required Tools Tool Description Download Git Version Control Software https://git-scm.com/downloads OpenJDK Java Development Kit https://openjdk.java.net/install/ Maven Java dependency manager https://maven.apache.org/install.html IntelliJ IDEA Development Environment https://www.jetbrains.com/idea/ Docker Local deployment and testing in containers https://docs.docker.com/install/ Docker-Compose Local deployment for testing container with microservice dependencies https://docs.docker.com/compose/install/ Version Control Git Chaos Engine is managed in the Gemalto GitHub repository. Protected Branches The master and develop branches should not have direct pushes. All work should be done in sub-branches and managed through Pull Requests. Pull requests should be reviewed by a person in the Thales Chaos Engine team. IDE IntelliJ IDEA IntelliJ IDEA is the IDE of choice for Chaos Engine development. The IDE can be downloaded from here . Code Style Code Style formatting is included for IntelliJ as part of the Git Repository. You should ensure that before committing code, IntelliJ performs the Reformat Code , Rearrange Code , Optimize Imports , and Cleanup operations. These are all available in the Code Commit dialog. These operations ensure that code formatting is consistent for everyone using the IDE, reducing the complexity of Merge operations. Coding Conventions See Chaos Engine Coding Conventions Alternatives Other IDE's may be used. However, care should be taken to ensure that the Code Style used in IntelliJ is mapped identically to other IDE's, to prevent potential merge issues. Local Testing Docker A Dockerfile has been made for building the Chaos Engine in a consistent manner. This Dockerfile ensures that the version run on your local machine should be equivalent to one built by pipelines. Docker-Compose A docker-compose.yml file has been included in the root of the project for easy testing with dependencies. Any additional Dockerfiles will be in the developer-tools folder and should have an associated README.","title":"Developer Documentation"},{"location":"_Developer_Documentation/#developer-documentation","text":"This documentation is provided to help ensure that new contributions to the Chaos Engine project can easily ramp up, and follow similar standards to avoid Version Control merging issues.","title":"Developer Documentation"},{"location":"_Developer_Documentation/#required-tools","text":"Tool Description Download Git Version Control Software https://git-scm.com/downloads OpenJDK Java Development Kit https://openjdk.java.net/install/ Maven Java dependency manager https://maven.apache.org/install.html IntelliJ IDEA Development Environment https://www.jetbrains.com/idea/ Docker Local deployment and testing in containers https://docs.docker.com/install/ Docker-Compose Local deployment for testing container with microservice dependencies https://docs.docker.com/compose/install/","title":"Required Tools"},{"location":"_Developer_Documentation/#version-control","text":"","title":"Version Control"},{"location":"_Developer_Documentation/#git","text":"Chaos Engine is managed in the Gemalto GitHub repository.","title":"Git"},{"location":"_Developer_Documentation/#protected-branches","text":"The master and develop branches should not have direct pushes. All work should be done in sub-branches and managed through Pull Requests. Pull requests should be reviewed by a person in the Thales Chaos Engine team.","title":"Protected Branches"},{"location":"_Developer_Documentation/#ide","text":"","title":"IDE"},{"location":"_Developer_Documentation/#intellij-idea","text":"IntelliJ IDEA is the IDE of choice for Chaos Engine development. The IDE can be downloaded from here .","title":"IntelliJ IDEA"},{"location":"_Developer_Documentation/#code-style","text":"Code Style formatting is included for IntelliJ as part of the Git Repository. You should ensure that before committing code, IntelliJ performs the Reformat Code , Rearrange Code , Optimize Imports , and Cleanup operations. These are all available in the Code Commit dialog. These operations ensure that code formatting is consistent for everyone using the IDE, reducing the complexity of Merge operations.","title":"Code Style"},{"location":"_Developer_Documentation/#coding-conventions","text":"See Chaos Engine Coding Conventions","title":"Coding Conventions"},{"location":"_Developer_Documentation/#alternatives","text":"Other IDE's may be used. However, care should be taken to ensure that the Code Style used in IntelliJ is mapped identically to other IDE's, to prevent potential merge issues.","title":"Alternatives"},{"location":"_Developer_Documentation/#local-testing","text":"","title":"Local Testing"},{"location":"_Developer_Documentation/#docker","text":"A Dockerfile has been made for building the Chaos Engine in a consistent manner. This Dockerfile ensures that the version run on your local machine should be equivalent to one built by pipelines.","title":"Docker"},{"location":"_Developer_Documentation/#docker-compose","text":"A docker-compose.yml file has been included in the root of the project for easy testing with dependencies. Any additional Dockerfiles will be in the developer-tools folder and should have an associated README.","title":"Docker-Compose"},{"location":"_Developer_Documentation/API/","text":"OpenAPI Specification OpenAPI 3.0 Specifications are automatically generated at runtime by the server. They can be accessed at OpenAPI JSON and OpenAPI YAML . These specifications can be imported into tools such as Postman for interacting with Chaos Engine APIs. Keep the REST Security Configuration in mind when using the API. On Chaos Engine instances, you can also access the OpenAPI specification dynamically by accessing /swagger-ui.html","title":"OpenAPI Specification"},{"location":"_Developer_Documentation/API/#openapi-specification","text":"OpenAPI 3.0 Specifications are automatically generated at runtime by the server. They can be accessed at OpenAPI JSON and OpenAPI YAML . These specifications can be imported into tools such as Postman for interacting with Chaos Engine APIs. Keep the REST Security Configuration in mind when using the API. On Chaos Engine instances, you can also access the OpenAPI specification dynamically by accessing /swagger-ui.html","title":"OpenAPI Specification"},{"location":"_Developer_Documentation/annotations/","text":"Chaos Engine Annotations Custom annotations may be used to control the flow of other parts of the Chaos Engine application. This is a rundown of the annotations, and where they are used. Identifier Annotation Package com.thales.chaos.container Target Fields Purpose Used in toString() and equals() implementations Container objects contain many fields that are needed for comparing equality. These fields are used as part of the object checksum calculation for equality, and for converting the object to a string. The Identifier contains a order modifier. This integer value configures the sorting order for the fields to ensure consistency. ChaosExperiment Annotation Package com.thales.chaos.container Target Methods Purpose Flagging methods to be used as experiments Container objects contain many methods to be used as the source of an experiment. These methods should be tagged with the ChaosExperiment annotation. Java Reflections are used to scan these objects to find the tagged methods. The methods need to take a single parameter, an Experiment object. Annotation Parameters Name Type Description Default experimentType ExperimentType The Type of experiment, based on what kind of core issue the application instance will experience, whether a state failure, resource failure, or network failure. STATE minimumDurationInSeconds int The minimum amount of time before an experiment can be evaluated for health. This time should consider how long it may take before a request for an action, such as stopping an instance, may actually take visible action. 30 maximumDurationInSeconds int The maximum amount of time before an experiment should proceed to self healing. This time should consider if an experiment may take a longer time to affect performance of a system. 300 cattleOnly boolean Flags whether or not this experiment should only affect cattle-like containers (i.e., containers that can be disposed of and recreated as needed). Use this if you cannot naturally recover your experiment via self-healing. false","title":"Chaos Engine Annotations"},{"location":"_Developer_Documentation/annotations/#chaos-engine-annotations","text":"Custom annotations may be used to control the flow of other parts of the Chaos Engine application. This is a rundown of the annotations, and where they are used.","title":"Chaos Engine Annotations"},{"location":"_Developer_Documentation/annotations/#identifier-annotation","text":"Package com.thales.chaos.container Target Fields Purpose Used in toString() and equals() implementations Container objects contain many fields that are needed for comparing equality. These fields are used as part of the object checksum calculation for equality, and for converting the object to a string. The Identifier contains a order modifier. This integer value configures the sorting order for the fields to ensure consistency.","title":"Identifier Annotation"},{"location":"_Developer_Documentation/annotations/#chaosexperiment-annotation","text":"Package com.thales.chaos.container Target Methods Purpose Flagging methods to be used as experiments Container objects contain many methods to be used as the source of an experiment. These methods should be tagged with the ChaosExperiment annotation. Java Reflections are used to scan these objects to find the tagged methods. The methods need to take a single parameter, an Experiment object.","title":"ChaosExperiment Annotation"},{"location":"_Developer_Documentation/annotations/#annotation-parameters","text":"Name Type Description Default experimentType ExperimentType The Type of experiment, based on what kind of core issue the application instance will experience, whether a state failure, resource failure, or network failure. STATE minimumDurationInSeconds int The minimum amount of time before an experiment can be evaluated for health. This time should consider how long it may take before a request for an action, such as stopping an instance, may actually take visible action. 30 maximumDurationInSeconds int The maximum amount of time before an experiment should proceed to self healing. This time should consider if an experiment may take a longer time to affect performance of a system. 300 cattleOnly boolean Flags whether or not this experiment should only affect cattle-like containers (i.e., containers that can be disposed of and recreated as needed). Use this if you cannot naturally recover your experiment via self-healing. false","title":"Annotation Parameters"},{"location":"_Developer_Documentation/coding_conventions/","text":"Coding Conventions These coding conventions ensure a consistent style across the Chaos Engine, and avoid potential issues with the large number of Third Party Libraries that are used for interacting with each Cloud Technology. Project Structure The core project structure is divided into multiple packages. The Services package should define Beans to be used by the Platform layer. The platform layer should act as a wrapper for those services. The Container layer is created by the platform layer, and should not interact directly with the Service layer. As another way of saying that, the Container layer should be agnostic to the specific libraries used for communicating with the actual platform. A change in libraries should only require changes in the Platform layer. Chain Calls In General, the Law of Demeter should apply to all packages in Chaos Engine. Chain calls that go through multiple packages can create a problem understanding where information comes from, and where it should be filtered or acted upon. A chain call should never pass through multiple packages. When considering Third Party Libraries, chain calls are unavoidable. However, in order to prevent Null Pointer Exceptions from occurring due to potential faults in the libraries, Java Optionals should be used, with chained map calls to go through the object layers. This ensures that potential Null values are parsed within and can be handled by using .orElse* methods of the Optional class. Bad Chain Call public ContainerHealth checkHealth ( KubernetesPodContainer container ) { V1Pod pod = getPodFromContainer ( container ); return pod . getStatus () . getContainerStatuses () . stream () . anyMatch ( s -> ! s . isReady ()) ? ContainerHealth . RUNNING_EXPERIMENT : ContainerHealth . NORMAL ) } /* In this example, there are multiple opportunities for a Null entry to cause a Null Pointer Exception. */ Good Chain Call public ContainerHealth checkHealth ( KubernetesPodContainer kubernetesPodContainer ) { V1Pod result = getPodFromContainer ( container ); return Optional . ofNullable ( result ) . map ( V1Pod :: getStatus ) . map ( V1PodStatus :: getContainerStatuses ) . map ( Collection :: stream ) . map ( s -> s . anyMatch ( ContainerStatus :: isReady )) . map ( aBoolean -> aBoolean ? ContainerHealth . NORMAL : ContainerHealth . RUNNING_EXPERIMENT ) . orElse ( ContainerHealth . DOES_NOT_EXIST ); } /* In this example, the Optional class wraps the method calls, and uses an orElse value to provide a third potential output from this method (essentially mapping True, False, and Null) */","title":"Coding Conventions"},{"location":"_Developer_Documentation/coding_conventions/#coding-conventions","text":"These coding conventions ensure a consistent style across the Chaos Engine, and avoid potential issues with the large number of Third Party Libraries that are used for interacting with each Cloud Technology.","title":"Coding Conventions"},{"location":"_Developer_Documentation/coding_conventions/#project-structure","text":"The core project structure is divided into multiple packages. The Services package should define Beans to be used by the Platform layer. The platform layer should act as a wrapper for those services. The Container layer is created by the platform layer, and should not interact directly with the Service layer. As another way of saying that, the Container layer should be agnostic to the specific libraries used for communicating with the actual platform. A change in libraries should only require changes in the Platform layer.","title":"Project Structure"},{"location":"_Developer_Documentation/coding_conventions/#chain-calls","text":"In General, the Law of Demeter should apply to all packages in Chaos Engine. Chain calls that go through multiple packages can create a problem understanding where information comes from, and where it should be filtered or acted upon. A chain call should never pass through multiple packages. When considering Third Party Libraries, chain calls are unavoidable. However, in order to prevent Null Pointer Exceptions from occurring due to potential faults in the libraries, Java Optionals should be used, with chained map calls to go through the object layers. This ensures that potential Null values are parsed within and can be handled by using .orElse* methods of the Optional class. Bad Chain Call public ContainerHealth checkHealth ( KubernetesPodContainer container ) { V1Pod pod = getPodFromContainer ( container ); return pod . getStatus () . getContainerStatuses () . stream () . anyMatch ( s -> ! s . isReady ()) ? ContainerHealth . RUNNING_EXPERIMENT : ContainerHealth . NORMAL ) } /* In this example, there are multiple opportunities for a Null entry to cause a Null Pointer Exception. */ Good Chain Call public ContainerHealth checkHealth ( KubernetesPodContainer kubernetesPodContainer ) { V1Pod result = getPodFromContainer ( container ); return Optional . ofNullable ( result ) . map ( V1Pod :: getStatus ) . map ( V1PodStatus :: getContainerStatuses ) . map ( Collection :: stream ) . map ( s -> s . anyMatch ( ContainerStatus :: isReady )) . map ( aBoolean -> aBoolean ? ContainerHealth . NORMAL : ContainerHealth . RUNNING_EXPERIMENT ) . orElse ( ContainerHealth . DOES_NOT_EXIST ); } /* In this example, the Optional class wraps the method calls, and uses an orElse value to provide a third potential output from this method (essentially mapping True, False, and Null) */","title":"Chain Calls"},{"location":"_Developer_Documentation/multi_module/","text":"Multi Module Structure This page is intended to be a guide on how to create, maintain, and organize modules for components of the Chaos Engine. Module Structure `--pom.xml (Parent) `-Module-A+-src | `-pom.xml (Module A) `-Module-B+-src `-pom.xml (Module B) The parent project directory contains a single pom.xml file that manages the entire project. The packaging field of that file will be pom. Each module is listed as a module under a modules section. Parent POM <?xml version=\"1.0\" encoding=\"UTF-8\"?> <project xmlns= \"http://maven.apache.org/POM/4.0.0\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation= \"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\" > ... <modules> <module> Module-A </module> <module> Module-B </module> </modules> <packaging> pom </packaging> ... </project> Each module exists as a directory, exactly as named in the module field. That directory needs to contain a pom.xml in its root. Those pom.xml files may reference the project pom as their parent. Example Module <?xml version=\"1.0\" encoding=\"UTF-8\"?> <project xmlns= \"http://maven.apache.org/POM/4.0.0\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation= \"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\" > <parent> <artifactId> parent-artifact </artifactId> <groupId> parent.group </groupId> <version> 1.0.0-SNAPSHOT </version> </parent> <modelVersion> 4.0.0 </modelVersion> <artifactId> Module-B </artifactId> <packaging> jar </packaging> <dependencies> <dependency> <groupId> parent.group </groupId> <artifactId> Module-A </artifactId> <version> ${project.version} </version> <scope> provided </scope> </dependency> </dependencies> </project> The child pom may reference other modules as dependencies. If this is done, they should be set with scope of provided. This ensures that the build process does not include the dependent JAR inside the child JAR. Nested Modules To keep the root level clean, similar module types should be grouped together with a second level module. This prevents the need to regularly add updates to the project pom. This second level module will be of type packaging = pom , and be declared as a module of the parent. chaosengine |-- chaosengine-core | |-- pom.xml | `-- src | |-- main | `-- test |-- chaosengine-experiments | |-- chaosengine-aws-ec2 | | |-- pom.xml | | `-- src | |-- chaosengine-aws-rds | | |-- pom.xml | | `-- src | |-- chaosengine-kubernetes | | |-- pom.xml | | `-- src | |-- chaosengine-pcf | | |-- pom.xml | | `-- src | `-- pom.xml |-- chaosengine-launcher | |-- pom.xml | `-- src | |-- main | `-- test |-- chaosengine-notifications | |-- chaosengine-notif-datadog | | |-- pom.xml | | `-- src | |-- chaosengine-notif-slack | | |-- pom.xml | | `-- src | `-- pom.xml `-- chaosengine-schedule |-- canada | |-- pom.xml | `-- src |-- czech-republic | |-- pom.xml | `-- src |-- france | |-- pom.xml | `-- src `-- pom.xml Building Modules A successful local build of the entire project will require running the command mvn install . This will trigger a few things. First, the Maven Reactor will determine the necessary order for building modules. The initial order will be the order of the modules in the project pom, but any dependent modules are pushed until after their dependencies are resolved. Some dependencies may not be obvious, so the modules should be maintained in a logical build order if possible. Second, the project pom has been configured so that the clean goal is automatically run when the builder initializes for each step. This ensures that you cannot possibly work with cached builds of the project, as a cached version of a dependent layer may either cause or hide a bug in a further layer. Third, each module will go through its build process, one at a time, including unit tests, and installing to the Maven Repository. This last step is necessary so that dependencies labelled as scope = provided will have their dependencies fulfilled. If any module fails at any step, the remainder of steps will be skipped. For instance, a unit test failure in the Core module prevents the Experiment modules from being compiled. A final output will show the results and build times of each module the reactor built, in the order they were built, with their corresponding exit status and build time. Final Build Output [ INFO ] ------------------------------------------------------------------------ [ INFO ] Reactor Summary for chaos - engine 1.1.0 - SNAPSHOT : [ INFO ] [ INFO ] chaos - engine ....................................... SUCCESS [ 3.103 s ] [ INFO ] chaosengine - launcher ............................... SUCCESS [ 5.987 s ] [ INFO ] chaosengine - core ................................... SUCCESS [ 58.850 s ] [ INFO ] chaosengine - kubernetes ............................. SUCCESS [ 16.017 s ] [ INFO ] chaosengine - aws - ec2 ................................ SUCCESS [ 9.143 s ] [ INFO ] chaosengine - aws - rds ................................ SUCCESS [ 8.887 s ] [ INFO ] chaosengine - pcf .................................... SUCCESS [ 10.948 s ] [ INFO ] ------------------------------------------------------------------------ [ INFO ] BUILD SUCCESS [ INFO ] ------------------------------------------------------------------------ [ INFO ] Total time : 01 : 54 min [ INFO ] Finished at : 2019 - 05 - 01 T13 : 48 : 34 - 04 : 00 [ INFO ] ------------------------------------------------------------------------ Packaging and Assembly Launcher The Launcher module requires special packaging, since it has the Main method in it. This is accomplished with the spring-boot-maven-plugin repackage. This is run by default as part of inheriting the spring-boot-starter-parent , but causes excessive and unnecessary file sizes in other modules. As such, in the project pom file, the plugin is set with configuration.skip=true , and then explicitly configured in the Launcher module pom. The launcher module is configured to create two copies of its Jar, as the repackaging will cause it to fail being consumed as a dependency in further layers. The original version of the launcher Jar is unchanged. The version run through the spring-boot-maven-plugin is placed in the target/entrypoint directory. This is used to identify it in the Dockerfile and choose the starting Jar for the image to run. Experiment Modules The experiment modules have dependencies for SDKs used explicitly in their module. To keep the ability to load these as needed, the modules use the maven-assembly-plugin configured in the project pom\\'s pluginManagement section. This plugin requires that the module calling it have a file called src/assembly/bin.xml in their module folder. The assembler instructions should use an id of assembler , so that the folder structure is common, and a format of dir . Then, in the dependencySets , include the specific groupId:artifactId of any packages imported in the pom.xml, and set useTransitiveDependencies and useTransitiveFiltering to true . This will cause all required JARs, both directly and indirectly, to be assembled in a subdirectory of the assembled JAR file. This is used in the Dockerfile to side load those JARs in the ClassPath, keeping the size of the modules to a minimum. Docker Build The Dockerfile has been assembled to account for the build instructions of the engine. A .dockerignore file automatically prunes out **/target/ folders, as well as non-code files that are created by IDE\\'s, such as .iml files. All module folders are copied into the Docker image, preserving their directory structure. The launcher module is run through a mvn install before the remainder of the modules are copied in. This will cache the majority of the dependencies, making future builds faster. In the final image, all Jar files that end up in the target directories are copied into the lib folder, as well as all Jar files that are in target/*-assembler directories. It is expected to find exactly one Jar in chaosengine-*/target/entrypoint/*.jar from the Launcher module, and places that file as chaosengine.jar in the working directory. That Jar is called as the entrypoint. The Docker image entrypoint is configured to launch the chaosengine.jar , while including all Jar files in the lib folder in its classpath. Those libraries are also imported into Spring via the loader.path property to scan for Spring Bean instantiation. Common Test Resources In order to share common test resources (such as the application.properties or logback-test.xml configuration files), a module titled chaosengine-test-utilities exists. This module does NOT use the project module as a parent, and is instead independent. It is imported in the project pom with the scope of test . This maps all code and resources in the test utilities src folder into the classpath when running unit tests. This allows us to hold a single logback-test.xml file, plus any other files we will need to share between modules in testing.","title":"Multi Module Structure"},{"location":"_Developer_Documentation/multi_module/#multi-module-structure","text":"This page is intended to be a guide on how to create, maintain, and organize modules for components of the Chaos Engine.","title":"Multi Module Structure"},{"location":"_Developer_Documentation/multi_module/#module-structure","text":"`--pom.xml (Parent) `-Module-A+-src | `-pom.xml (Module A) `-Module-B+-src `-pom.xml (Module B) The parent project directory contains a single pom.xml file that manages the entire project. The packaging field of that file will be pom. Each module is listed as a module under a modules section. Parent POM <?xml version=\"1.0\" encoding=\"UTF-8\"?> <project xmlns= \"http://maven.apache.org/POM/4.0.0\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation= \"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\" > ... <modules> <module> Module-A </module> <module> Module-B </module> </modules> <packaging> pom </packaging> ... </project> Each module exists as a directory, exactly as named in the module field. That directory needs to contain a pom.xml in its root. Those pom.xml files may reference the project pom as their parent. Example Module <?xml version=\"1.0\" encoding=\"UTF-8\"?> <project xmlns= \"http://maven.apache.org/POM/4.0.0\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation= \"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\" > <parent> <artifactId> parent-artifact </artifactId> <groupId> parent.group </groupId> <version> 1.0.0-SNAPSHOT </version> </parent> <modelVersion> 4.0.0 </modelVersion> <artifactId> Module-B </artifactId> <packaging> jar </packaging> <dependencies> <dependency> <groupId> parent.group </groupId> <artifactId> Module-A </artifactId> <version> ${project.version} </version> <scope> provided </scope> </dependency> </dependencies> </project> The child pom may reference other modules as dependencies. If this is done, they should be set with scope of provided. This ensures that the build process does not include the dependent JAR inside the child JAR.","title":"Module Structure"},{"location":"_Developer_Documentation/multi_module/#nested-modules","text":"To keep the root level clean, similar module types should be grouped together with a second level module. This prevents the need to regularly add updates to the project pom. This second level module will be of type packaging = pom , and be declared as a module of the parent. chaosengine |-- chaosengine-core | |-- pom.xml | `-- src | |-- main | `-- test |-- chaosengine-experiments | |-- chaosengine-aws-ec2 | | |-- pom.xml | | `-- src | |-- chaosengine-aws-rds | | |-- pom.xml | | `-- src | |-- chaosengine-kubernetes | | |-- pom.xml | | `-- src | |-- chaosengine-pcf | | |-- pom.xml | | `-- src | `-- pom.xml |-- chaosengine-launcher | |-- pom.xml | `-- src | |-- main | `-- test |-- chaosengine-notifications | |-- chaosengine-notif-datadog | | |-- pom.xml | | `-- src | |-- chaosengine-notif-slack | | |-- pom.xml | | `-- src | `-- pom.xml `-- chaosengine-schedule |-- canada | |-- pom.xml | `-- src |-- czech-republic | |-- pom.xml | `-- src |-- france | |-- pom.xml | `-- src `-- pom.xml","title":"Nested Modules"},{"location":"_Developer_Documentation/multi_module/#building-modules","text":"A successful local build of the entire project will require running the command mvn install . This will trigger a few things. First, the Maven Reactor will determine the necessary order for building modules. The initial order will be the order of the modules in the project pom, but any dependent modules are pushed until after their dependencies are resolved. Some dependencies may not be obvious, so the modules should be maintained in a logical build order if possible. Second, the project pom has been configured so that the clean goal is automatically run when the builder initializes for each step. This ensures that you cannot possibly work with cached builds of the project, as a cached version of a dependent layer may either cause or hide a bug in a further layer. Third, each module will go through its build process, one at a time, including unit tests, and installing to the Maven Repository. This last step is necessary so that dependencies labelled as scope = provided will have their dependencies fulfilled. If any module fails at any step, the remainder of steps will be skipped. For instance, a unit test failure in the Core module prevents the Experiment modules from being compiled. A final output will show the results and build times of each module the reactor built, in the order they were built, with their corresponding exit status and build time. Final Build Output [ INFO ] ------------------------------------------------------------------------ [ INFO ] Reactor Summary for chaos - engine 1.1.0 - SNAPSHOT : [ INFO ] [ INFO ] chaos - engine ....................................... SUCCESS [ 3.103 s ] [ INFO ] chaosengine - launcher ............................... SUCCESS [ 5.987 s ] [ INFO ] chaosengine - core ................................... SUCCESS [ 58.850 s ] [ INFO ] chaosengine - kubernetes ............................. SUCCESS [ 16.017 s ] [ INFO ] chaosengine - aws - ec2 ................................ SUCCESS [ 9.143 s ] [ INFO ] chaosengine - aws - rds ................................ SUCCESS [ 8.887 s ] [ INFO ] chaosengine - pcf .................................... SUCCESS [ 10.948 s ] [ INFO ] ------------------------------------------------------------------------ [ INFO ] BUILD SUCCESS [ INFO ] ------------------------------------------------------------------------ [ INFO ] Total time : 01 : 54 min [ INFO ] Finished at : 2019 - 05 - 01 T13 : 48 : 34 - 04 : 00 [ INFO ] ------------------------------------------------------------------------","title":"Building Modules"},{"location":"_Developer_Documentation/multi_module/#packaging-and-assembly","text":"","title":"Packaging and Assembly"},{"location":"_Developer_Documentation/multi_module/#launcher","text":"The Launcher module requires special packaging, since it has the Main method in it. This is accomplished with the spring-boot-maven-plugin repackage. This is run by default as part of inheriting the spring-boot-starter-parent , but causes excessive and unnecessary file sizes in other modules. As such, in the project pom file, the plugin is set with configuration.skip=true , and then explicitly configured in the Launcher module pom. The launcher module is configured to create two copies of its Jar, as the repackaging will cause it to fail being consumed as a dependency in further layers. The original version of the launcher Jar is unchanged. The version run through the spring-boot-maven-plugin is placed in the target/entrypoint directory. This is used to identify it in the Dockerfile and choose the starting Jar for the image to run.","title":"Launcher"},{"location":"_Developer_Documentation/multi_module/#experiment-modules","text":"The experiment modules have dependencies for SDKs used explicitly in their module. To keep the ability to load these as needed, the modules use the maven-assembly-plugin configured in the project pom\\'s pluginManagement section. This plugin requires that the module calling it have a file called src/assembly/bin.xml in their module folder. The assembler instructions should use an id of assembler , so that the folder structure is common, and a format of dir . Then, in the dependencySets , include the specific groupId:artifactId of any packages imported in the pom.xml, and set useTransitiveDependencies and useTransitiveFiltering to true . This will cause all required JARs, both directly and indirectly, to be assembled in a subdirectory of the assembled JAR file. This is used in the Dockerfile to side load those JARs in the ClassPath, keeping the size of the modules to a minimum.","title":"Experiment Modules"},{"location":"_Developer_Documentation/multi_module/#docker-build","text":"The Dockerfile has been assembled to account for the build instructions of the engine. A .dockerignore file automatically prunes out **/target/ folders, as well as non-code files that are created by IDE\\'s, such as .iml files. All module folders are copied into the Docker image, preserving their directory structure. The launcher module is run through a mvn install before the remainder of the modules are copied in. This will cache the majority of the dependencies, making future builds faster. In the final image, all Jar files that end up in the target directories are copied into the lib folder, as well as all Jar files that are in target/*-assembler directories. It is expected to find exactly one Jar in chaosengine-*/target/entrypoint/*.jar from the Launcher module, and places that file as chaosengine.jar in the working directory. That Jar is called as the entrypoint. The Docker image entrypoint is configured to launch the chaosengine.jar , while including all Jar files in the lib folder in its classpath. Those libraries are also imported into Spring via the loader.path property to scan for Spring Bean instantiation.","title":"Docker Build"},{"location":"_Developer_Documentation/multi_module/#common-test-resources","text":"In order to share common test resources (such as the application.properties or logback-test.xml configuration files), a module titled chaosengine-test-utilities exists. This module does NOT use the project module as a parent, and is instead independent. It is imported in the project pom with the scope of test . This maps all code and resources in the test utilities src folder into the classpath when running unit tests. This allows us to hold a single logback-test.xml file, plus any other files we will need to share between modules in testing.","title":"Common Test Resources"},{"location":"_Developer_Documentation/notable_issues/","text":"Notable Issues This page is intended to be a guide to issues that we have seen while developing and running Chaos Engine, including how to identify, and how to resolve them. These may be persistent issues if not configured, or bugs that can be created from improper development/packaging. SpringBoot Illegal State Exception / Class Not Found If a Bean Instantiation attempts to load a class that is not in the classpath, then a IllegalStateException is thrown, wrapping a lower ClassNotFoundException.This can happen if a dependency is not packaged in one of the built JARs, and also is not copied into the loaded classpath folders as part of the build process. Example java . lang . IllegalStateException : Error processing condition on org . springframework . boot . autoconfigure . context . PropertyPlaceholderAutoConfiguration . propertySourcesPlaceholderConfigurer at org . springframework . boot . autoconfigure . condition . SpringBootCondition . matches ( SpringBootCondition . java : 64 ) at org . springframework . context . annotation . ConditionEvaluator . shouldSkip ( ConditionEvaluator . java : 108 ) at org . springframework . context . annotation . ConfigurationClassBeanDefinitionReader . loadBeanDefinitionsForBeanMethod ( ConfigurationClassBeanDefinitionReader . java : 181 ) at org . springframework . context . annotation . ConfigurationClassBeanDefinitionReader . loadBeanDefinitionsForConfigurationClass ( ConfigurationClassBeanDefinitionReader . java : 141 ) at org . springframework . context . annotation . ConfigurationClassBeanDefinitionReader . loadBeanDefinitions ( ConfigurationClassBeanDefinitionReader . java : 117 ) at org . springframework . context . annotation . ConfigurationClassPostProcessor . processConfigBeanDefinitions ( ConfigurationClassPostProcessor . java : 327 ) at org . springframework . context . annotation . ConfigurationClassPostProcessor . postProcessBeanDefinitionRegistry ( ConfigurationClassPostProcessor . java : 232 ) at org . springframework . context . support . PostProcessorRegistrationDelegate . invokeBeanDefinitionRegistryPostProcessors ( PostProcessorRegistrationDelegate . java : 275 ) at org . springframework . context . support . PostProcessorRegistrationDelegate . invokeBeanFactoryPostProcessors ( PostProcessorRegistrationDelegate . java : 95 ) at org . springframework . context . support . AbstractApplicationContext . invokeBeanFactoryPostProcessors ( AbstractApplicationContext . java : 705 ) at org . springframework . context . support . AbstractApplicationContext . refresh ( AbstractApplicationContext . java : 531 ) at org . springframework . boot . web . servlet . context . ServletWebServerApplicationContext . refresh ( ServletWebServerApplicationContext . java : 142 ) at org . springframework . boot . SpringApplication . refresh ( SpringApplication . java : 775 ) at org . springframework . boot . SpringApplication . refreshContext ( SpringApplication . java : 397 ) at org . springframework . boot . SpringApplication . run ( SpringApplication . java : 316 ) at com . gemalto . chaos . ChaosEngine . main ( ChaosEngine . java : 17 ) at java . base / jdk . internal . reflect . NativeMethodAccessorImpl . invoke0 ( Native Method ) at java . base / jdk . internal . reflect . NativeMethodAccessorImpl . invoke ( NativeMethodAccessorImpl . java : 62 ) at java . base / jdk . internal . reflect . DelegatingMethodAccessorImpl . invoke ( DelegatingMethodAccessorImpl . java : 43 ) at java . base / java . lang . reflect . Method . invoke ( Method . java : 566 ) at org . springframework . boot . loader . MainMethodRunner . run ( MainMethodRunner . java : 48 ) at org . springframework . boot . loader . Launcher . launch ( Launcher . java : 87 ) at org . springframework . boot . loader . Launcher . launch ( Launcher . java : 50 ) at org . springframework . boot . loader . PropertiesLauncher . main ( PropertiesLauncher . java : 593 ) Caused by : java . lang . IllegalStateException : Failed to introspect Class [ com . gemalto . chaos . notification . services . DataDogNotificationService ] from ClassLoader [ org . springframework . boot . loader . LaunchedURLClassLoader @63 c12fb0 ] at org . springframework . util . ReflectionUtils . getDeclaredMethods ( ReflectionUtils . java : 507 ) at org . springframework . util . ReflectionUtils . doWithMethods ( ReflectionUtils . java : 404 ) at org . springframework . util . ReflectionUtils . doWithMethods ( ReflectionUtils . java : 389 ) at org . springframework . util . ReflectionUtils . getUniqueDeclaredMethods ( ReflectionUtils . java : 447 ) at java . base / java . util . concurrent . ConcurrentHashMap . computeIfAbsent ( ConcurrentHashMap . java : 1705 ) at org . springframework . beans . factory . support . AbstractAutowireCapableBeanFactory . getTypeForFactoryMethod ( AbstractAutowireCapableBeanFactory . java : 738 ) at org . springframework . beans . factory . support . AbstractAutowireCapableBeanFactory . determineTargetType ( AbstractAutowireCapableBeanFactory . java : 679 ) at org . springframework . beans . factory . support . AbstractAutowireCapableBeanFactory . predictBeanType ( AbstractAutowireCapableBeanFactory . java : 647 ) at org . springframework . beans . factory . support . AbstractBeanFactory . isFactoryBean ( AbstractBeanFactory . java : 1518 ) at org . springframework . beans . factory . support . AbstractBeanFactory . isFactoryBean ( AbstractBeanFactory . java : 1023 ) at org . springframework . boot . autoconfigure . condition . BeanTypeRegistry . addBeanTypeForNonAliasDefinition ( BeanTypeRegistry . java : 195 ) at org . springframework . boot . autoconfigure . condition . BeanTypeRegistry . addBeanTypeForNonAliasDefinition ( BeanTypeRegistry . java : 159 ) at org . springframework . boot . autoconfigure . condition . BeanTypeRegistry . addBeanType ( BeanTypeRegistry . java : 152 ) at org . springframework . boot . autoconfigure . condition . BeanTypeRegistry . updateTypesIfNecessary ( BeanTypeRegistry . java : 140 ) at java . base / java . util . Iterator . forEachRemaining ( Iterator . java : 133 ) at org . springframework . boot . autoconfigure . condition . BeanTypeRegistry . updateTypesIfNecessary ( BeanTypeRegistry . java : 135 ) at org . springframework . boot . autoconfigure . condition . BeanTypeRegistry . getNamesForType ( BeanTypeRegistry . java : 97 ) at org . springframework . boot . autoconfigure . condition . OnBeanCondition . collectBeanNamesForType ( OnBeanCondition . java : 298 ) at org . springframework . boot . autoconfigure . condition . OnBeanCondition . getBeanNamesForType ( OnBeanCondition . java : 289 ) at org . springframework . boot . autoconfigure . condition . OnBeanCondition . getBeanNamesForType ( OnBeanCondition . java : 278 ) at org . springframework . boot . autoconfigure . condition . OnBeanCondition . getMatchingBeans ( OnBeanCondition . java : 189 ) at org . springframework . boot . autoconfigure . condition . OnBeanCondition . getMatchOutcome ( OnBeanCondition . java : 160 ) at org . springframework . boot . autoconfigure . condition . SpringBootCondition . matches ( SpringBootCondition . java : 47 ) ... 23 common frames omitted Caused by : java . lang . NoClassDefFoundError : com / timgroup / statsd / StatsDClient at java . base / java . lang . Class . getDeclaredMethods0 ( Native Method ) at java . base / java . lang . Class . privateGetDeclaredMethods ( Class . java : 3166 ) at java . base / java . lang . Class . getDeclaredMethods ( Class . java : 2309 ) at org . springframework . util . ReflectionUtils . getDeclaredMethods ( ReflectionUtils . java : 489 ) ... 45 common frames omitted Caused by : java . lang . ClassNotFoundException : com . timgroup . statsd . StatsDClient at java . base / java . net . URLClassLoader . findClass ( URLClassLoader . java : 471 ) at java . base / java . lang . ClassLoader . loadClass ( ClassLoader . java : 588 ) at org . springframework . boot . loader . LaunchedURLClassLoader . loadClass ( LaunchedURLClassLoader . java : 93 ) at java . base / java . lang . ClassLoader . loadClass ( ClassLoader . java : 521 ) ... 49 common frames omitted Common Resolution Review the Build instructions for the class attempting to load this class. It either needs to be configured to package dependencies with itself, or to copy dependent Jar\\'s in such a way that they are packaged in the final image. Check List: The module has build instructions to include dependencies either in itself or in a relative path. Relative path dependencies are copied into the final image","title":"Notable Issues"},{"location":"_Developer_Documentation/notable_issues/#notable-issues","text":"This page is intended to be a guide to issues that we have seen while developing and running Chaos Engine, including how to identify, and how to resolve them. These may be persistent issues if not configured, or bugs that can be created from improper development/packaging.","title":"Notable Issues"},{"location":"_Developer_Documentation/notable_issues/#springboot-illegal-state-exception-class-not-found","text":"If a Bean Instantiation attempts to load a class that is not in the classpath, then a IllegalStateException is thrown, wrapping a lower ClassNotFoundException.This can happen if a dependency is not packaged in one of the built JARs, and also is not copied into the loaded classpath folders as part of the build process.","title":"SpringBoot Illegal State Exception / Class Not Found"},{"location":"_Developer_Documentation/notable_issues/#example","text":"java . lang . IllegalStateException : Error processing condition on org . springframework . boot . autoconfigure . context . PropertyPlaceholderAutoConfiguration . propertySourcesPlaceholderConfigurer at org . springframework . boot . autoconfigure . condition . SpringBootCondition . matches ( SpringBootCondition . java : 64 ) at org . springframework . context . annotation . ConditionEvaluator . shouldSkip ( ConditionEvaluator . java : 108 ) at org . springframework . context . annotation . ConfigurationClassBeanDefinitionReader . loadBeanDefinitionsForBeanMethod ( ConfigurationClassBeanDefinitionReader . java : 181 ) at org . springframework . context . annotation . ConfigurationClassBeanDefinitionReader . loadBeanDefinitionsForConfigurationClass ( ConfigurationClassBeanDefinitionReader . java : 141 ) at org . springframework . context . annotation . ConfigurationClassBeanDefinitionReader . loadBeanDefinitions ( ConfigurationClassBeanDefinitionReader . java : 117 ) at org . springframework . context . annotation . ConfigurationClassPostProcessor . processConfigBeanDefinitions ( ConfigurationClassPostProcessor . java : 327 ) at org . springframework . context . annotation . ConfigurationClassPostProcessor . postProcessBeanDefinitionRegistry ( ConfigurationClassPostProcessor . java : 232 ) at org . springframework . context . support . PostProcessorRegistrationDelegate . invokeBeanDefinitionRegistryPostProcessors ( PostProcessorRegistrationDelegate . java : 275 ) at org . springframework . context . support . PostProcessorRegistrationDelegate . invokeBeanFactoryPostProcessors ( PostProcessorRegistrationDelegate . java : 95 ) at org . springframework . context . support . AbstractApplicationContext . invokeBeanFactoryPostProcessors ( AbstractApplicationContext . java : 705 ) at org . springframework . context . support . AbstractApplicationContext . refresh ( AbstractApplicationContext . java : 531 ) at org . springframework . boot . web . servlet . context . ServletWebServerApplicationContext . refresh ( ServletWebServerApplicationContext . java : 142 ) at org . springframework . boot . SpringApplication . refresh ( SpringApplication . java : 775 ) at org . springframework . boot . SpringApplication . refreshContext ( SpringApplication . java : 397 ) at org . springframework . boot . SpringApplication . run ( SpringApplication . java : 316 ) at com . gemalto . chaos . ChaosEngine . main ( ChaosEngine . java : 17 ) at java . base / jdk . internal . reflect . NativeMethodAccessorImpl . invoke0 ( Native Method ) at java . base / jdk . internal . reflect . NativeMethodAccessorImpl . invoke ( NativeMethodAccessorImpl . java : 62 ) at java . base / jdk . internal . reflect . DelegatingMethodAccessorImpl . invoke ( DelegatingMethodAccessorImpl . java : 43 ) at java . base / java . lang . reflect . Method . invoke ( Method . java : 566 ) at org . springframework . boot . loader . MainMethodRunner . run ( MainMethodRunner . java : 48 ) at org . springframework . boot . loader . Launcher . launch ( Launcher . java : 87 ) at org . springframework . boot . loader . Launcher . launch ( Launcher . java : 50 ) at org . springframework . boot . loader . PropertiesLauncher . main ( PropertiesLauncher . java : 593 ) Caused by : java . lang . IllegalStateException : Failed to introspect Class [ com . gemalto . chaos . notification . services . DataDogNotificationService ] from ClassLoader [ org . springframework . boot . loader . LaunchedURLClassLoader @63 c12fb0 ] at org . springframework . util . ReflectionUtils . getDeclaredMethods ( ReflectionUtils . java : 507 ) at org . springframework . util . ReflectionUtils . doWithMethods ( ReflectionUtils . java : 404 ) at org . springframework . util . ReflectionUtils . doWithMethods ( ReflectionUtils . java : 389 ) at org . springframework . util . ReflectionUtils . getUniqueDeclaredMethods ( ReflectionUtils . java : 447 ) at java . base / java . util . concurrent . ConcurrentHashMap . computeIfAbsent ( ConcurrentHashMap . java : 1705 ) at org . springframework . beans . factory . support . AbstractAutowireCapableBeanFactory . getTypeForFactoryMethod ( AbstractAutowireCapableBeanFactory . java : 738 ) at org . springframework . beans . factory . support . AbstractAutowireCapableBeanFactory . determineTargetType ( AbstractAutowireCapableBeanFactory . java : 679 ) at org . springframework . beans . factory . support . AbstractAutowireCapableBeanFactory . predictBeanType ( AbstractAutowireCapableBeanFactory . java : 647 ) at org . springframework . beans . factory . support . AbstractBeanFactory . isFactoryBean ( AbstractBeanFactory . java : 1518 ) at org . springframework . beans . factory . support . AbstractBeanFactory . isFactoryBean ( AbstractBeanFactory . java : 1023 ) at org . springframework . boot . autoconfigure . condition . BeanTypeRegistry . addBeanTypeForNonAliasDefinition ( BeanTypeRegistry . java : 195 ) at org . springframework . boot . autoconfigure . condition . BeanTypeRegistry . addBeanTypeForNonAliasDefinition ( BeanTypeRegistry . java : 159 ) at org . springframework . boot . autoconfigure . condition . BeanTypeRegistry . addBeanType ( BeanTypeRegistry . java : 152 ) at org . springframework . boot . autoconfigure . condition . BeanTypeRegistry . updateTypesIfNecessary ( BeanTypeRegistry . java : 140 ) at java . base / java . util . Iterator . forEachRemaining ( Iterator . java : 133 ) at org . springframework . boot . autoconfigure . condition . BeanTypeRegistry . updateTypesIfNecessary ( BeanTypeRegistry . java : 135 ) at org . springframework . boot . autoconfigure . condition . BeanTypeRegistry . getNamesForType ( BeanTypeRegistry . java : 97 ) at org . springframework . boot . autoconfigure . condition . OnBeanCondition . collectBeanNamesForType ( OnBeanCondition . java : 298 ) at org . springframework . boot . autoconfigure . condition . OnBeanCondition . getBeanNamesForType ( OnBeanCondition . java : 289 ) at org . springframework . boot . autoconfigure . condition . OnBeanCondition . getBeanNamesForType ( OnBeanCondition . java : 278 ) at org . springframework . boot . autoconfigure . condition . OnBeanCondition . getMatchingBeans ( OnBeanCondition . java : 189 ) at org . springframework . boot . autoconfigure . condition . OnBeanCondition . getMatchOutcome ( OnBeanCondition . java : 160 ) at org . springframework . boot . autoconfigure . condition . SpringBootCondition . matches ( SpringBootCondition . java : 47 ) ... 23 common frames omitted Caused by : java . lang . NoClassDefFoundError : com / timgroup / statsd / StatsDClient at java . base / java . lang . Class . getDeclaredMethods0 ( Native Method ) at java . base / java . lang . Class . privateGetDeclaredMethods ( Class . java : 3166 ) at java . base / java . lang . Class . getDeclaredMethods ( Class . java : 2309 ) at org . springframework . util . ReflectionUtils . getDeclaredMethods ( ReflectionUtils . java : 489 ) ... 45 common frames omitted Caused by : java . lang . ClassNotFoundException : com . timgroup . statsd . StatsDClient at java . base / java . net . URLClassLoader . findClass ( URLClassLoader . java : 471 ) at java . base / java . lang . ClassLoader . loadClass ( ClassLoader . java : 588 ) at org . springframework . boot . loader . LaunchedURLClassLoader . loadClass ( LaunchedURLClassLoader . java : 93 ) at java . base / java . lang . ClassLoader . loadClass ( ClassLoader . java : 521 ) ... 49 common frames omitted","title":"Example"},{"location":"_Developer_Documentation/notable_issues/#common-resolution","text":"Review the Build instructions for the class attempting to load this class. It either needs to be configured to package dependencies with itself, or to copy dependent Jar\\'s in such a way that they are packaged in the final image.","title":"Common Resolution"},{"location":"_Developer_Documentation/notable_issues/#check-list","text":"The module has build instructions to include dependencies either in itself or in a relative path. Relative path dependencies are copied into the final image","title":"Check List:"},{"location":"_Developer_Documentation/Design_Philosophy/pipeline_design/","text":"Pipeline Design Philosophy This page outlines the philosophy of the Chaos Engine build pipeline, and where we would like to reach with it. Desired State The pipeline should exist in 8 distinct stages that are progressive. Version - The version number to be used is calculated, to be used by all future stages as needed. Build - Compilation and unit testing should be done. Test - External static testing should be done, such as Anti Virus, SonarQube, and BlackDuck scans are initiated Packaging - Docker image is built and tagged with the COMMIT SHA only. Container Scanning - Container package is scanned for vulnerabilities. Deploy (test) - The docker image is deployed to an environment that can be used for testing. DAST - Dynamic testing of the running Docker image can be done (i.e., ZAProxy) Tear Down - The deployed environment is torn down. Container Registry Cleanup - Unnecessary images are removed. Publishable images are retagged with the appropriate Version tag. Variants The desired pipeline would have 4 distinct variants that are used for different purposes. Protected Tag based builds. These tags should be on verified stable versions with specific feature sets merged in. The full pipeline is automated, and the final step should retag images. master and develop branches. These branches should contain stable and near-stable versions of the code base. The full pipeline is automated, except for the Container Registry final step. This should be a Manual job that Allows Failure. Merge Request based builds. These builds should be temporary while awaiting approval. These should be automated up to Tear-Down, allowing a reviewer to access the environment if needed. Tear Down and Registry Cleanup should happen automatically All other branches and unprotected tags. These should be treated as developer editions. There are manual blocks at both the Package and Deploy stages. The repo cleanup stage is always run, and is delayed by a reasonable amount.","title":"Pipeline Design Philosophy"},{"location":"_Developer_Documentation/Design_Philosophy/pipeline_design/#pipeline-design-philosophy","text":"This page outlines the philosophy of the Chaos Engine build pipeline, and where we would like to reach with it.","title":"Pipeline Design Philosophy"},{"location":"_Developer_Documentation/Design_Philosophy/pipeline_design/#desired-state","text":"The pipeline should exist in 8 distinct stages that are progressive. Version - The version number to be used is calculated, to be used by all future stages as needed. Build - Compilation and unit testing should be done. Test - External static testing should be done, such as Anti Virus, SonarQube, and BlackDuck scans are initiated Packaging - Docker image is built and tagged with the COMMIT SHA only. Container Scanning - Container package is scanned for vulnerabilities. Deploy (test) - The docker image is deployed to an environment that can be used for testing. DAST - Dynamic testing of the running Docker image can be done (i.e., ZAProxy) Tear Down - The deployed environment is torn down. Container Registry Cleanup - Unnecessary images are removed. Publishable images are retagged with the appropriate Version tag.","title":"Desired State"},{"location":"_Developer_Documentation/Design_Philosophy/pipeline_design/#variants","text":"The desired pipeline would have 4 distinct variants that are used for different purposes. Protected Tag based builds. These tags should be on verified stable versions with specific feature sets merged in. The full pipeline is automated, and the final step should retag images. master and develop branches. These branches should contain stable and near-stable versions of the code base. The full pipeline is automated, except for the Container Registry final step. This should be a Manual job that Allows Failure. Merge Request based builds. These builds should be temporary while awaiting approval. These should be automated up to Tear-Down, allowing a reviewer to access the environment if needed. Tear Down and Registry Cleanup should happen automatically All other branches and unprotected tags. These should be treated as developer editions. There are manual blocks at both the Package and Deploy stages. The repo cleanup stage is always run, and is delayed by a reasonable amount.","title":"Variants"}]}